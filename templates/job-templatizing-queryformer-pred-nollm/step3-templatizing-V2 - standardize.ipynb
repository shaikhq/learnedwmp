{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import loguniform\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn import neural_network\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout \n",
    "from tensorflow.keras.layers import BatchNormalization \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint # new!\n",
    "import os # new!\n",
    "# import seaborn as sns\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout \n",
    "# from tensorflow.keras.layers import BatchNormalization \n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint # new!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "box_plot_title = 'Memory Estimation Error (MB)'\n",
    "pd.set_option('display.max_columns', None)\n",
    "cluster_set = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_success = pd.read_csv('utils/success_db2_est.csv')\n",
    "\n",
    "# Display the first few rows to verify\n",
    "df_success.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_success.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the JSON file\n",
    "val_embeddings_path = \"val_embeddings.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(val_embeddings_path, \"r\") as f:\n",
    "    val_embeddings_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "val_embeddings_df = pd.DataFrame({\n",
    "    \"file_name\": list(val_embeddings_data.keys()),\n",
    "    \"embedding\": list(val_embeddings_data.values())\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(val_embeddings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract QUERYID and EXPLAIN_TIME\n",
    "val_embeddings_df[\"QUERYID\"] = val_embeddings_df[\"file_name\"].apply(lambda x: x.split(\"_\")[1])\n",
    "val_embeddings_df[\"EXPLAIN_TIME\"] = val_embeddings_df[\"file_name\"].apply(lambda x: x.split(\"_\")[2].replace(\".pt\", \"\"))\n",
    "\n",
    "# Convert QUERYID to integer (if needed)\n",
    "val_embeddings_df[\"QUERYID\"] = val_embeddings_df[\"QUERYID\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming val_embeddings_df and df_success are already defined\n",
    "\n",
    "# Perform the join\n",
    "result_df = pd.merge(\n",
    "    val_embeddings_df,\n",
    "    df_success,\n",
    "    on=['QUERYID', 'EXPLAIN_TIME'],  # Match on QUERYID and EXPLAIN_TIME\n",
    "    how='inner'  # Inner join to keep only matching rows\n",
    ")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(result_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embeddings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = result_df.copy()[['embedding', 'SORT_SHRHEAP_TOP', 'Db2_ESTIMATE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the JSON file\n",
    "train_embeddings_path = \"train_embeddings.json\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open(train_embeddings_path, \"r\") as f:\n",
    "    train_embeddings_path_embeddings_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_embeddings_df = pd.DataFrame({\n",
    "    \"file_name\": list(train_embeddings_path_embeddings_data.keys()),\n",
    "    \"embedding\": list(train_embeddings_path_embeddings_data.values())\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "# print(train_embeddings_path_embeddings_df.head())\n",
    "\n",
    "# Extract QUERYID and EXPLAIN_TIME\n",
    "train_embeddings_df[\"QUERYID\"] = train_embeddings_df[\"file_name\"].apply(lambda x: x.split(\"_\")[1])\n",
    "train_embeddings_df[\"EXPLAIN_TIME\"] = train_embeddings_df[\"file_name\"].apply(lambda x: x.split(\"_\")[2].replace(\".pt\", \"\"))\n",
    "\n",
    "# Convert QUERYID to integer (if needed)\n",
    "train_embeddings_df[\"QUERYID\"] = train_embeddings_df[\"QUERYID\"].astype(int)\n",
    "\n",
    "# Perform the join\n",
    "result_df = pd.merge(\n",
    "    train_embeddings_df,\n",
    "    df_success,\n",
    "    on=['QUERYID', 'EXPLAIN_TIME'],  # Match on QUERYID and EXPLAIN_TIME\n",
    "    how='inner'  # Inner join to keep only matching rows\n",
    ")\n",
    "\n",
    "df_train = result_df.copy()[['embedding', 'SORT_SHRHEAP_TOP', 'Db2_ESTIMATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for df_train\n",
    "df_train.rename(columns={\n",
    "    'embedding': 'sql_embedding',\n",
    "    'SORT_SHRHEAP_TOP': 'actual',\n",
    "    'Db2_ESTIMATE': 'db2'\n",
    "}, inplace=True)\n",
    "\n",
    "df_train = df_train[['sql_embedding', 'db2', 'actual']]\n",
    "\n",
    "# Rename columns for df_test\n",
    "df_test.rename(columns={\n",
    "    'embedding': 'sql_embedding',\n",
    "    'SORT_SHRHEAP_TOP': 'actual',\n",
    "    'Db2_ESTIMATE': 'db2'\n",
    "}, inplace=True)\n",
    "\n",
    "df_test = df_test[['sql_embedding', 'db2', 'actual']]\n",
    "\n",
    "# Verify the changes\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['db2'] = df_train['db2'] * 4000 / 1000000\n",
    "df_train['actual'] = df_train['actual'] * 4000 / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['db2'] = df_test['db2'] * 4000 / 1000000\n",
    "df_test['actual'] = df_test['actual'] * 4000 / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the embeddings to 2D numpy arrays\n",
    "train_embeddings = np.array([np.array(embed) for embed in df_train['sql_embedding']])\n",
    "test_embeddings = np.array([np.array(embed) for embed in df_test['sql_embedding']])\n",
    "\n",
    "# Initialize the scaler and fit on the training embeddings\n",
    "scaler = StandardScaler()\n",
    "standardized_train_embeddings = scaler.fit_transform(train_embeddings)\n",
    "\n",
    "# Transform the test embeddings using the same scaler\n",
    "standardized_test_embeddings = scaler.transform(test_embeddings)\n",
    "\n",
    "# Overwrite the 'sql_embedding' column in both dataframes with standardized values\n",
    "df_train['sql_embedding'] = list(standardized_train_embeddings)\n",
    "df_test['sql_embedding'] = list(standardized_test_embeddings)\n",
    "\n",
    "# Save the scaler for future use if needed\n",
    "import joblib\n",
    "joblib.dump(scaler, \"embedding_scaler.pkl\")\n",
    "\n",
    "print(\"Standardization applied and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train and Evaluate a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(estimator, batch_size, X, Y):\n",
    "    predicted = estimator.predict(X)\n",
    "    Y = np.insert(Y, Y.shape[1], predicted, axis=1)\n",
    "    \n",
    "    indices = np.linspace(0, X.shape[0]-1, X.shape[0], dtype=int)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    num_batches = int(np.floor(X.shape[0] / batch_size))\n",
    "    \n",
    "    df_batches = pd.DataFrame(columns=['actual', 'db2', 'ml'])\n",
    "    \n",
    "    for ibat in range(num_batches):\n",
    "        start = (ibat * batch_size)\n",
    "        end = (ibat * batch_size + batch_size) - 1\n",
    "        \n",
    "        ibat_Y = Y[indices[start:end]]\n",
    "        \n",
    "        actual = sum(ibat_Y[:,-1])\n",
    "        db2 = sum(ibat_Y[:,-2])\n",
    "        ml = sum(ibat_Y[:,-3])\n",
    "        \n",
    "        df_batches = df_batches.append({'actual':actual,\n",
    "                                       'db2':db2,\n",
    "                                       'ml':ml},\n",
    "                                      ignore_index=True)\n",
    "        \n",
    "    return df_batches\n",
    "\n",
    "def rmse(Y):\n",
    "    cols = Y.columns.values[1:]\n",
    "    rmse_dict = {}\n",
    "    \n",
    "    for col in cols:\n",
    "        rmse = np.round(np.sqrt(mean_squared_error(Y['actual'].values, Y[col].values)))\n",
    "        rmse_dict[col] = rmse\n",
    "    \n",
    "    return rmse_dict\n",
    "    \n",
    "def calculate_residuals(Y):\n",
    "    first_col = Y.columns[0]\n",
    "    cols = Y.columns[1:]\n",
    "    df_residuals = pd.DataFrame(columns=cols)\n",
    "\n",
    "    for col in cols:\n",
    "        df_residuals[col] = Y[col] - Y[first_col]\n",
    "        \n",
    "    return df_residuals\n",
    "\n",
    "def box_plot(Y, length, height):\n",
    "    df_residuals = calculate_residuals(Y)\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "    f = plt.figure(figsize=[length,height])\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    ax = f.add_subplot(111)\n",
    "    sns.boxplot(data=df_residuals, ax=ax, showfliers = True, orient=\"h\")\n",
    "    ax.set_xlabel(xlabel=box_plot_title,fontsize=22)\n",
    "    plt.tick_params(axis='x',labeltop='on', labelbottom='on')\n",
    "    ax.xaxis.set_ticks_position('both')\n",
    "    #ax.set_yticks(yticks_new)\n",
    "#     plt.setp(ax.get_yticklabels(), rotation=90)\n",
    "    f.tight_layout()\n",
    "    plt.show()\n",
    "    ax.savefig('job_err.png')\n",
    "def residual_plot(Y):\n",
    "    Y_predicted = Y.iloc[:,1:]\n",
    "    print('Y_predicted ', Y_predicted.shape)\n",
    "    cols = Y_predicted.columns\n",
    "    markers = ['8', 'P', '*', 'h', 'X','+','^','s','o']\n",
    "#     colors = ['steelblue', 'darkorange', 'darkorchid', 'limegreen', 'fuchsia']\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(cols)))\n",
    "    \n",
    "    Y_residuals = calculate_residuals(Y)\n",
    "    print('Y_residuals ', Y_residuals.shape)\n",
    "    \n",
    "    for col in cols:\n",
    "        plot_index = Y_predicted.columns.get_loc(col)\n",
    "        plt.scatter(Y_predicted[col], Y_residuals[col], \n",
    "                   edgecolor='white', c=colors[plot_index],\n",
    "                   marker=markers[plot_index], label=col)\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.hlines(y=0, xmin=0, xmax=9000, color='black', lw=2)\n",
    "    plt.xlim([0, 9000])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def get_clusters(k, data, km):\n",
    "    X = np.vstack(data['sql_embedding'].values)  # Convert list of lists to 2D array\n",
    "    if km != None:\n",
    "        print('clustering test dataset')\n",
    "        y_kmeans = km.predict(X)\n",
    "        \n",
    "    else:\n",
    "        print('clustering train dataset')\n",
    "        km = KMeans(n_clusters = k, \n",
    "                        init='k-means++', \n",
    "                        n_init=10, \n",
    "                        max_iter=300, \n",
    "                        random_state=0)\n",
    "        km.fit(X)\n",
    "        y_kmeans = km.predict(X)\n",
    "        \n",
    "    print('Distortion: %.2f' % km.inertia_)\n",
    "    df_train = data.copy();\n",
    "    df_train['cluster'] = np.nan\n",
    "    for i,e in enumerate(y_kmeans):\n",
    "        df_train['cluster'].loc[i] = e;\n",
    "    \n",
    "    return km, df_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_workload(batch_size, data, k):\n",
    "    # Select relevant columns\n",
    "    df_data = data[['db2', 'actual', 'cluster']]\n",
    "    \n",
    "    # Create dummies for 'cluster'\n",
    "    df_data = pd.get_dummies(df_data, columns=['cluster'])\n",
    "    \n",
    "    # Add missing cluster columns in one step\n",
    "    cluster_columns = [f'cluster_{i}.0' for i in range(k)]\n",
    "    missing_columns = [col for col in cluster_columns if col not in df_data.columns]\n",
    "    if missing_columns:\n",
    "        # Create a DataFrame with missing columns set to 0\n",
    "        df_missing = pd.DataFrame(0, index=df_data.index, columns=missing_columns)\n",
    "        # Concatenate the missing columns\n",
    "        df_data = pd.concat([df_data, df_missing], axis=1)\n",
    "\n",
    "    # Sort columns to maintain a consistent order (optional)\n",
    "    df_data = df_data.reindex(columns=['db2', 'actual'] + cluster_columns)\n",
    "    \n",
    "    # Initialize batches\n",
    "    df_batches = []\n",
    "    indices = np.arange(len(df_data))\n",
    "    num_batches = len(df_data) // batch_size\n",
    "    \n",
    "    # Create batches\n",
    "    for ibat in range(num_batches):\n",
    "        batch_indices = indices[ibat * batch_size:(ibat + 1) * batch_size]\n",
    "        ibat_Y = df_data.iloc[batch_indices]\n",
    "        df_batches.append(ibat_Y.sum())\n",
    "\n",
    "    # Combine batches into a DataFrame\n",
    "    return pd.DataFrame(df_batches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the folder exists\n",
    "output_folder = \"cluster_data\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for k in cluster_set:\n",
    "    km, df_train_clusters = get_clusters(k, df_train, None)\n",
    "    km, df_test_clusters = get_clusters(k, df_test, km)\n",
    "    \n",
    "    workload_train = create_workload(batch_size, df_train_clusters, k)\n",
    "    workload_test = create_workload(batch_size, df_test_clusters, k)\n",
    "\n",
    "    file_name_train = os.path.join(output_folder, f'train_workloads_final_{k}_clusters.csv')\n",
    "    file_name_test = os.path.join(output_folder, f'test_workloads_final_{k}_clusters.csv')\n",
    "    \n",
    "    workload_train.to_csv(file_name_train, index=False)\n",
    "    workload_test.to_csv(file_name_test, index=False)\n",
    "    \n",
    "    print(f\"k = {k} is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def run_model(model, train_data, test_data, model_type, k):\n",
    "    input_cols = train_data.columns.tolist()\n",
    "    \n",
    "    # Remove target and identifier columns from input features\n",
    "    input_cols.remove('actual')\n",
    "    input_cols.remove('db2')\n",
    "    \n",
    "    # Extract input features and target variables\n",
    "    train_X = train_data[input_cols].values\n",
    "    train_y = train_data[['actual']].values.ravel()  # Flatten target to 1D\n",
    "    test_X = test_data[input_cols].values\n",
    "    test_y = test_data[['actual']].values.ravel()   # Flatten target to 1D\n",
    "    \n",
    "    # Convert to float32 for compatibility\n",
    "    train_X = np.asarray(train_X).astype('float32')\n",
    "    train_y = np.asarray(train_y).astype('float32')\n",
    "    test_X = np.asarray(test_X).astype('float32')\n",
    "    test_y = np.asarray(test_y).astype('float32')\n",
    "    \n",
    "    # Fit the model and make predictions\n",
    "    model.fit(train_X, train_y)\n",
    "    train_data[model_type] = model.predict(train_X)\n",
    "    test_data[model_type] = model.predict(test_X)\n",
    "    \n",
    "    print(f\"{model_type} done\")\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import neural_network\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize empty lists for results\n",
    "results_train = []\n",
    "results_test = []\n",
    "\n",
    "def run_model(model, train_data, test_data, model_type, k):\n",
    "    input_cols = train_data.columns.tolist()\n",
    "    \n",
    "    # Remove target and identifier columns from input features\n",
    "    input_cols.remove('actual')\n",
    "    input_cols.remove('db2')\n",
    "    \n",
    "    # Extract input features and target variables\n",
    "    train_X = train_data[input_cols].values\n",
    "    train_y = train_data[['actual']].values.ravel()  # Flatten target to 1D\n",
    "    test_X = test_data[input_cols].values\n",
    "    test_y = test_data[['actual']].values.ravel()   # Flatten target to 1D\n",
    "    \n",
    "    # Convert to float32 for compatibility\n",
    "    train_X = np.asarray(train_X).astype('float32')\n",
    "    train_y = np.asarray(train_y).astype('float32')\n",
    "    test_X = np.asarray(test_X).astype('float32')\n",
    "    test_y = np.asarray(test_y).astype('float32')\n",
    "    \n",
    "    # Fit the model and make predictions\n",
    "    model.fit(train_X, train_y)\n",
    "    train_data[model_type] = model.predict(train_X)\n",
    "    test_data[model_type] = model.predict(test_X)\n",
    "    \n",
    "    print(f\"{model_type} done\")\n",
    "    return train_data, test_data\n",
    "\n",
    "for k in cluster_set:\n",
    "    file_name_train = f'cluster_data/train_workloads_final_{k}_clusters.csv'\n",
    "    file_name_test = f'cluster_data/test_workloads_final_{k}_clusters.csv'\n",
    "    workloads_train = pd.read_csv(file_name_train)\n",
    "    workloads_test = pd.read_csv(file_name_test)\n",
    "\n",
    "    # Flatten target variables\n",
    "    train_y = workloads_train['actual'].values.ravel()\n",
    "    test_y = workloads_test['actual'].values.ravel()\n",
    "\n",
    "    # Linear Regression - Ridge\n",
    "    ridge = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        Ridge(\n",
    "            alpha=3.2573287932867558,\n",
    "            fit_intercept=True,\n",
    "            solver='lsqr',\n",
    "            random_state=42\n",
    "        )\n",
    "    )\n",
    "    workloads_train, workloads_test = run_model(\n",
    "        ridge, \n",
    "        workloads_train, \n",
    "        workloads_test, \n",
    "        'ridge',\n",
    "        k\n",
    "    )\n",
    "    train_results = workloads_train[['actual', 'db2', 'ridge']].copy()\n",
    "    test_results = workloads_test[['actual', 'db2', 'ridge']].copy()\n",
    "    workloads_train.drop('ridge', axis=1, inplace=True)\n",
    "    workloads_test.drop('ridge', axis=1, inplace=True)\n",
    "\n",
    "    # Decision Tree Regressor\n",
    "    dtr = DecisionTreeRegressor(\n",
    "        random_state=42,\n",
    "        min_samples_leaf=3,\n",
    "        max_features=5,\n",
    "        max_depth=30,\n",
    "        criterion='squared_error'\n",
    "    )\n",
    "    workloads_train, workloads_test = run_model(\n",
    "        dtr, \n",
    "        workloads_train, \n",
    "        workloads_test, \n",
    "        'dtr',\n",
    "        k\n",
    "    )\n",
    "    train_results.loc[:, 'dtr'] = workloads_train['dtr'].values\n",
    "    test_results.loc[:, 'dtr'] = workloads_test['dtr'].values\n",
    "    workloads_train.drop('dtr', axis=1, inplace=True)\n",
    "    workloads_test.drop('dtr', axis=1, inplace=True)\n",
    "\n",
    "    # Random Forest Regressor\n",
    "    random_forest = RandomForestRegressor(\n",
    "        n_estimators=2000,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        max_depth=50,\n",
    "        bootstrap=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    workloads_train, workloads_test = run_model(\n",
    "        random_forest, \n",
    "        workloads_train, \n",
    "        workloads_test, \n",
    "        'random_forest',\n",
    "        k\n",
    "    )\n",
    "    train_results.loc[:, 'random_forest'] = workloads_train['random_forest'].values\n",
    "    test_results.loc[:, 'random_forest'] = workloads_test['random_forest'].values\n",
    "    workloads_train.drop('random_forest', axis=1, inplace=True)\n",
    "    workloads_test.drop('random_forest', axis=1, inplace=True)\n",
    "\n",
    "    # XGBoost Model\n",
    "    xgboost = xgb.XGBRegressor(\n",
    "        colsample_bytree=0.7165235326918536, \n",
    "        gamma=0.2573287932867558, \n",
    "        learning_rate=0.3895603296024942, \n",
    "        max_depth=2, \n",
    "        n_estimators=165, \n",
    "        objective='reg:squarederror',  # Updated objective\n",
    "        subsample=0.3234123573173331, \n",
    "        random_state=42\n",
    "    )\n",
    "    workloads_train, workloads_test = run_model(\n",
    "        xgboost, \n",
    "        workloads_train, \n",
    "        workloads_test, \n",
    "        'xgboost',\n",
    "        k\n",
    "    )\n",
    "    train_results.loc[:, 'xgboost'] = workloads_train['xgboost'].values\n",
    "    test_results.loc[:, 'xgboost'] = workloads_test['xgboost'].values\n",
    "    workloads_train.drop('xgboost', axis=1, inplace=True)\n",
    "    workloads_test.drop('xgboost', axis=1, inplace=True)\n",
    "\n",
    "    # Neural Network\n",
    "    optimal_nn_model = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Scale input features\n",
    "        ('nn', neural_network.MLPRegressor(\n",
    "            max_iter=1000,  # Increased iterations\n",
    "            alpha=0.001,\n",
    "            activation='identity',\n",
    "            learning_rate='constant',\n",
    "            random_state=6,\n",
    "            hidden_layer_sizes=(10, 20),  # Simplified architecture\n",
    "            solver='lbfgs'\n",
    "        ))\n",
    "    ])\n",
    "    workloads_train, workloads_test = run_model(\n",
    "        optimal_nn_model, \n",
    "        workloads_train, \n",
    "        workloads_test, \n",
    "        'nn',\n",
    "        k\n",
    "    )\n",
    "    train_results.loc[:, 'nn'] = workloads_train['nn'].values\n",
    "    test_results.loc[:, 'nn'] = workloads_test['nn'].values\n",
    "    workloads_train.drop('nn', axis=1, inplace=True)\n",
    "    workloads_test.drop('nn', axis=1, inplace=True)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse_train = rmse(train_results)\n",
    "    rmse_test = rmse(test_results)\n",
    "\n",
    "    # Add cluster info\n",
    "    rmse_train['cluster'] = k\n",
    "    rmse_test['cluster'] = k\n",
    "\n",
    "    # Append results to lists\n",
    "    results_train.append(rmse_train)\n",
    "    results_test.append(rmse_test)\n",
    "\n",
    "    print(f\"Models for k = {k} are done\")\n",
    "    print(\"-------------------------------\")\n",
    "\n",
    "# Convert lists to DataFrames\n",
    "df_results_train = pd.DataFrame(results_train)\n",
    "df_results_test = pd.DataFrame(results_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_train.to_csv(\"df_results_train.csv\")\n",
    "df_results_train.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_test.to_csv(\"df_results_test.csv\")\n",
    "df_results_test.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_results_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ref: https://www.python-graph-gallery.com/5-control-width-and-space-in-barplots\n",
    "mapes = df['nn']\n",
    "batch_sizes = df['cluster']\n",
    "#labels = df_batches['Label']\n",
    "\n",
    "x_pos = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "width = 10\n",
    "height = 3.5\n",
    "fig.set_size_inches(width, height)\n",
    "\n",
    "bars = ax.bar(x_pos, mapes, fill=False, hatch='ooo')\n",
    "\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "\n",
    "plt.xticks(x_pos, batch_sizes)\n",
    "\n",
    "# https://stackoverflow.com/questions/72970649/how-to-label-bars-with-multiple-custom-values\n",
    "for c in ax.containers:\n",
    "    #print(c)\n",
    "    ax.bar_label(c, label_type='edge', padding=1, fontsize=11, fontstyle='italic')\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.xlabel(\"Cluster Size\", labelpad=5, fontsize=15)\n",
    "#rotation: https://stackoverflow.com/questions/42100114/rotating-title-of-y-axis-to-be-horizontal-in-matplotlib\n",
    "# labelpad: https://stackoverflow.com/questions/21539018/how-to-change-separation-between-tick-labels-and-axis-labels-in-matplotlib\n",
    "plt.ylabel(\"RSME\", labelpad=5, fontsize=15)\n",
    "\n",
    "fig.savefig('job_cluster_result.png', bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
