{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from model.database_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.util import Normalizer\n",
    "\n",
    "# cost_norm = Normalizer(1, 100)\n",
    "# cost_norm = Normalizer(-3.61192, 12.290855)\n",
    "#cost_norm = Normalizer(5, 2611)\n",
    "cost_norm = Normalizer(8.26, 11.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # bs = 1024\n",
    "    # SQ: smaller batch size\n",
    "    bs = 1\n",
    "    #lr = 0.001\n",
    "    lr = 0.001\n",
    "    # epochs = 200\n",
    "    epochs = 50\n",
    "    clip_size = 50\n",
    "    embed_size = 64\n",
    "    pred_hid = 128\n",
    "    ffn_dim = 128\n",
    "    head_size = 12\n",
    "    n_layers = 8\n",
    "    dropout = 0.1\n",
    "    sch_decay = 0.6\n",
    "    # device = 'cuda:0'\n",
    "    device = 'cpu'\n",
    "    newpath = 'job_queries_training'\n",
    "    to_predict = 'cost'\n",
    "args = Args()\n",
    "\n",
    "import os\n",
    "if not os.path.exists(args.newpath):\n",
    "    os.makedirs(args.newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import QueryFormer\n",
    "\n",
    "model = QueryFormer(emb_size = args.embed_size ,ffn_dim = args.ffn_dim, head_size = args.head_size, \\\n",
    "                 dropout = args.dropout, n_layers = args.n_layers, \\\n",
    "                 use_sample = False, use_hist = False, \\\n",
    "                 pred_hid = args.pred_hid\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import PlanTreeDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([1, 30, 1161])\n",
      "rel_pos shape: torch.Size([1, 30, 30])\n",
      "attn_bias shape: torch.Size([1, 31, 31])\n",
      "heights shape: torch.Size([1, 30])\n",
      "cost_labels shape: torch.Size([1])\n",
      "raw_costs shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Load the saved tensor collection\n",
    "loaded_tensors = torch.load(\"./job_queries/tensors/query_1_2024-12-03-20.43.44.200877.pt\")\n",
    "\n",
    "# Access each tensor\n",
    "x_loaded = loaded_tensors[\"x\"]\n",
    "rel_pos_loaded = loaded_tensors[\"rel_pos\"]\n",
    "attn_bias_loaded = loaded_tensors[\"attn_bias\"]\n",
    "heights_loaded = loaded_tensors[\"heights\"]\n",
    "cost_labels_loaded = loaded_tensors[\"cost_labels\"]\n",
    "raw_costs_loaded = loaded_tensors[\"raw_costs\"]\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(f\"x shape: {x_loaded.shape}\")\n",
    "print(f\"rel_pos shape: {rel_pos_loaded.shape}\")\n",
    "print(f\"attn_bias shape: {attn_bias_loaded.shape}\")\n",
    "print(f\"heights shape: {heights_loaded.shape}\")\n",
    "print(f\"cost_labels shape: {cost_labels_loaded.shape}\")\n",
    "print(f\"raw_costs shape: {raw_costs_loaded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [2., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [3., 1., 0.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4,  4,  5,  5,  6,  6,  7,  8,  8,  9, 10, 10, 11, 11,\n",
       "           9,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61,  1,  2,  3,  3,  4,  4,  5,  5,  6,  7,  7,  8,  9,  9, 10, 10,\n",
       "           8,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61,  1,  2,  2,  3,  3,  4,  4,  5,  6,  6,  7,  8,  8,  9,  9,\n",
       "           7,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61,  1,  2,  2,  3,  3,  4,  5,  5,  6,  7,  7,  8,  8,\n",
       "           6,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  4,  4,  5,  6,  6,  7,  7,\n",
       "           5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  3,  3,  4,  5,  5,  6,  6,\n",
       "           4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  4,  4,  5,  5,\n",
       "           3, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  3,  3,  4,  4,\n",
       "           2, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  3,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
       "           1, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
       "          61,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_bias_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 11, 10,  2,  9,  2,  8,  2,  7,  6,  2,  2,  5,  4,  2,  2,  3,  2,\n",
       "          2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([66844.], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_costs_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9965], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_labels_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([1, 30, 1161])\n",
      "rel_pos shape: torch.Size([1, 30, 30])\n",
      "attn_bias shape: torch.Size([1, 31, 31])\n",
      "heights shape: torch.Size([1, 30])\n",
      "cost_labels shape: torch.Size([1])\n",
      "raw_costs shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes to verify\n",
    "print(f\"x shape: {x_loaded.shape}\")\n",
    "print(f\"rel_pos shape: {rel_pos_loaded.shape}\")\n",
    "print(f\"attn_bias shape: {attn_bias_loaded.shape}\")\n",
    "print(f\"heights shape: {heights_loaded.shape}\")\n",
    "print(f\"cost_labels shape: {cost_labels_loaded.shape}\")\n",
    "print(f\"raw_costs shape: {raw_costs_loaded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST - Loading 2 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved tensor collection\n",
    "loaded_tensors1 = torch.load(\"./job_queries/tensors/query_1_2024-12-03-20.43.44.200877.pt\")\n",
    "\n",
    "# Access each tensor\n",
    "x_loaded1 = loaded_tensors[\"x\"]\n",
    "rel_pos_loaded1 = loaded_tensors[\"rel_pos\"]\n",
    "attn_bias_loaded1 = loaded_tensors[\"attn_bias\"]\n",
    "heights_loaded1 = loaded_tensors[\"heights\"]\n",
    "cost_labels_loaded1 = loaded_tensors[\"cost_labels\"]\n",
    "raw_costs_loaded1 = loaded_tensors[\"raw_costs\"]\n",
    "\n",
    "# Load the saved tensor collection\n",
    "loaded_tensors2 = torch.load(\"./job_queries/tensors/query_2_2024-12-03-20.43.45.954590.pt\")\n",
    "\n",
    "# Access each tensor\n",
    "x_loaded2 = loaded_tensors[\"x\"]\n",
    "rel_pos_loaded2 = loaded_tensors[\"rel_pos\"]\n",
    "attn_bias_loaded2 = loaded_tensors[\"attn_bias\"]\n",
    "heights_loaded2 = loaded_tensors[\"heights\"]\n",
    "cost_labels_loaded2 = loaded_tensors[\"cost_labels\"]\n",
    "raw_costs_loaded2 = loaded_tensors[\"raw_costs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_list = [x_loaded1, x_loaded2]\n",
    "rel_pos_list = [rel_pos_loaded1, rel_pos_loaded2]\n",
    "attn_bias_list = [attn_bias_loaded1, attn_bias_loaded2]\n",
    "heights_list = [heights_loaded1, heights_loaded2]\n",
    "cost_labels_list = [cost_labels_loaded1, cost_labels_loaded2]\n",
    "raw_costs_list = [raw_costs_loaded1, raw_costs_loaded2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 2\n",
      "First sample: ({'x': tensor([[[1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [2., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [3., 1., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'attn_bias': tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]]), 'rel_pos': tensor([[[ 1,  2,  3,  4,  4,  5,  5,  6,  6,  7,  8,  8,  9, 10, 10, 11, 11,\n",
      "           9,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61,  1,  2,  3,  3,  4,  4,  5,  5,  6,  7,  7,  8,  9,  9, 10, 10,\n",
      "           8,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61,  1,  2,  2,  3,  3,  4,  4,  5,  6,  6,  7,  8,  8,  9,  9,\n",
      "           7,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61,  1,  2,  2,  3,  3,  4,  5,  5,  6,  7,  7,  8,  8,\n",
      "           6,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  4,  4,  5,  6,  6,  7,  7,\n",
      "           5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  3,  3,  4,  5,  5,  6,  6,\n",
      "           4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  4,  4,  5,  5,\n",
      "           3, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  3,  3,  4,  4,\n",
      "           2, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  3,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "           1, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "          61,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]]), 'heights': tensor([[12, 11, 10,  2,  9,  2,  8,  2,  7,  6,  2,  2,  5,  4,  2,  2,  3,  2,\n",
      "          2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])}, (tensor([0.9965], dtype=torch.float64), tensor([66844.], dtype=torch.float64)))\n"
     ]
    }
   ],
   "source": [
    "# Initialize dataset\n",
    "dataset = PlanTreeDataset(\n",
    "    2,\n",
    "    x_list,\n",
    "    attn_bias_list,\n",
    "    rel_pos_list,\n",
    "    heights_list,\n",
    "    cost_labels_list,\n",
    "    raw_costs_list\n",
    ")\n",
    "\n",
    "print(\"Dataset length:\", len(dataset))\n",
    "print(\"First sample:\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_copies = 2\n",
    "\n",
    "# Replicate data\n",
    "replicated_x = [x_loaded.clone() for _ in range(num_copies)]\n",
    "replicated_attn_bias = [attn_bias_loaded.clone() for _ in range(num_copies)]\n",
    "replicated_rel_pos = [rel_pos_loaded.clone() for _ in range(num_copies)]\n",
    "replicated_heights = [heights_loaded.clone() for _ in range(num_copies)]\n",
    "replicated_cost_labels = [cost_labels_loaded.clone() for _ in range(num_copies)]  # Tensor replicated\n",
    "replicated_raw_costs = [raw_costs_loaded.clone() for _ in range(num_copies)]  # List replicated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 2\n",
      "First sample: ({'x': tensor([[[1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [2., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [3., 1., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'attn_bias': tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]]), 'rel_pos': tensor([[[ 1,  2,  3,  4,  4,  5,  5,  6,  6,  7,  8,  8,  9, 10, 10, 11, 11,\n",
      "           9,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61,  1,  2,  3,  3,  4,  4,  5,  5,  6,  7,  7,  8,  9,  9, 10, 10,\n",
      "           8,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61,  1,  2,  2,  3,  3,  4,  4,  5,  6,  6,  7,  8,  8,  9,  9,\n",
      "           7,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61,  1,  2,  2,  3,  3,  4,  5,  5,  6,  7,  7,  8,  8,\n",
      "           6,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  4,  4,  5,  6,  6,  7,  7,\n",
      "           5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  3,  3,  4,  5,  5,  6,  6,\n",
      "           4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  4,  4,  5,  5,\n",
      "           3, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  3,  3,  4,  4,\n",
      "           2, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  3,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "           1, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "          61,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]]), 'heights': tensor([[12, 11, 10,  2,  9,  2,  8,  2,  7,  6,  2,  2,  5,  4,  2,  2,  3,  2,\n",
      "          2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])}, (tensor([0.9965], dtype=torch.float64), tensor([66844.], dtype=torch.float64)))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize dataset\n",
    "dataset = PlanTreeDataset(\n",
    "    num_copies,\n",
    "    replicated_x,\n",
    "    replicated_attn_bias,\n",
    "    replicated_rel_pos,\n",
    "    replicated_heights,\n",
    "    replicated_cost_labels,\n",
    "    replicated_raw_costs\n",
    ")\n",
    "\n",
    "print(\"Dataset length:\", len(dataset))\n",
    "print(\"First sample:\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(replicated_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(replicated_x) == num_copies\n",
    "assert len(replicated_attn_bias) == num_copies\n",
    "assert len(replicated_rel_pos) == num_copies\n",
    "assert len(replicated_heights) == num_copies\n",
    "assert len(replicated_cost_labels) == num_copies\n",
    "assert len(replicated_raw_costs) == num_copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PlanTreeDataset with optional costs\n",
    "#dataset = PlanTreeDataset(1, [x], [attn_bias], [rel_pos], [heights], cost_labels, raw_costs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2\n",
      "Sample contents:\n",
      "Feature Matrix (x): torch.Size([1, 30, 1161])\n",
      "Attention Bias (attn_bias): torch.Size([1, 31, 31])\n",
      "Relative Positions (rel_pos): torch.Size([1, 30, 30])\n",
      "Heights (heights): torch.Size([1, 30])\n",
      "Label: (tensor([0.9965], dtype=torch.float64), tensor([66844.], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "# Get the length of the dataset\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Access a single sample\n",
    "sample, label = dataset[0]\n",
    "\n",
    "# Print the sample contents\n",
    "print(\"Sample contents:\")\n",
    "print(\"Feature Matrix (x):\", sample['x'].shape)\n",
    "print(\"Attention Bias (attn_bias):\", sample['attn_bias'].shape)\n",
    "print(\"Relative Positions (rel_pos):\", sample['rel_pos'].shape)\n",
    "print(\"Heights (heights):\", sample['heights'].shape)\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/learnedwmp/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "Epoch: 0  Avg Loss: 0.17684863531030715, Time: 2.732783317565918\n",
      "Median: 3.31295773519345\n",
      "Mean: 3.31295773519345\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "Epoch: 20  Avg Loss: 3.595553721424949e-07, Time: 38.26681184768677\n",
      "Median: 1.0013360359485077\n",
      "Mean: 1.0013360359485077\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "Epoch: 40  Avg Loss: 6.658318341123959e-08, Time: 74.33961796760559\n",
      "Median: 1.0007380173426186\n",
      "Mean: 1.0007380173426186\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/learnedwmp/templates/job-templatizing-queryformer-modified/model/trainer.py:42: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = pearsonr(np.log(ps), np.log(ls))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/learnedwmp/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/learnedwmp/templates/job-templatizing-queryformer-modified/model/trainer.py:42: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = pearsonr(np.log(ps), np.log(ls))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/learnedwmp/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/learnedwmp/templates/job-templatizing-queryformer-modified/model/trainer.py:42: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = pearsonr(np.log(ps), np.log(ls))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/learnedwmp/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/learnedwmp/templates/job-templatizing-queryformer-modified/model/trainer.py:42: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = pearsonr(np.log(ps), np.log(ls))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/learnedwmp/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/learnedwmp/templates/job-templatizing-queryformer-modified/model/trainer.py:42: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = pearsonr(np.log(ps), np.log(ls))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([1])\n",
      "Cost Predictions Shape: torch.Size([])\n",
      "Batch Cost Label Shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n",
      "pred_output shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Example numpy label\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import importlib\n",
    "\n",
    "from model import trainer\n",
    "importlib.reload(trainer)\n",
    "from  model.trainer import train_single, train\n",
    "\n",
    "\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "# Train the model with the numpy label\n",
    "# trained_model = train_single(model, dataset, dataset, crit, cost_norm, args)\n",
    "model, best_model_path, train_embeddings, val_embeddings = train(model, dataset, dataset, crit, cost_norm, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training embeddings (best epoch): 2\n",
      "First training embedding shape: (1417,)\n",
      "Number of validation embeddings (best epoch): 2\n",
      "First validation embedding shape: (1417,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training embeddings (best epoch): {len(train_embeddings)}\")\n",
    "print(f\"First training embedding shape: {train_embeddings[0].shape}\")\n",
    "print(f\"Number of validation embeddings (best epoch): {len(val_embeddings)}\")\n",
    "print(f\"First validation embedding shape: {val_embeddings[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First validation embedding shape: [ 2.851161   20.531675    8.309177   ... 23.04402     4.2916102\n",
      " -0.70323336]\n"
     ]
    }
   ],
   "source": [
    "print(f\"First validation embedding shape: {val_embeddings[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First validation embedding shape: [ 2.851161   20.531675    8.309177   ... 23.04402     4.2916102\n",
      " -0.70323336]\n"
     ]
    }
   ],
   "source": [
    "print(f\"First validation embedding shape: {val_embeddings[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Embedding 0:\n",
      "[ 2.85116100e+00  2.05316753e+01  8.30917740e+00  2.77599354e+01  4.93734026e+00  5.56273651e+00  1.63346729e+01  2.36758995e+01  1.85432415e+01 -1.16897087e+01 -3.79956460e+00 -6.44580650e+00  5.27722692e+00 -5.79063606e+00  2.32282734e+01  1.37613554e+01 -2.06316757e+00  1.43070784e+01  7.50589943e+00  4.52252531e+00  1.11168356e+01 -2.93024063e+00 -1.60223408e+01  9.50275421e+00  8.65117550e-01 -9.10232830e+00 -1.42041359e+01 -1.67259426e+01 -2.07097797e+01  7.96444845e+00  1.29832888e+00  8.79665184e+00  2.46938286e+01 -3.13759661e+00 -1.47409973e+01 -3.42307329e+00 -8.52990532e+00 -6.18487024e+00 -2.07017732e+00 -6.73620558e+00  1.18425636e+01  1.78384900e+00  6.30518103e+00 -1.48744545e+01 -1.87404194e+01  1.53625002e+01  2.29943504e+01 -4.10873079e+00  9.99921227e+00 -1.58318691e+01  2.12915611e+01  1.41741533e+01 -1.25254164e+01 -2.31318970e+01  1.03742981e+01  2.48843918e+01  1.68221073e+01 -2.60557690e+01 -1.14073906e+01 -8.00450134e+00 -2.11683826e+01  6.30641460e+00\n",
      "  2.40060291e+01  7.42987394e+00  4.37662458e+00 -2.04897251e+01  1.61540833e+01 -7.07390499e+00  5.44595051e+00  1.10513105e+01  8.54345036e+00 -7.54892540e+00 -1.43485918e+01  9.27373314e+00  1.43732023e+01 -3.25032425e+00  1.21951170e+01  1.10058517e+01 -4.05525589e+00  2.14780769e+01 -1.13630495e+01  1.64201317e+01  2.00279236e+01  1.57427931e+01  9.35720503e-01  1.46048155e+01 -1.82020700e+00 -2.43036060e+01  1.42515545e+01  1.06535311e+01  3.05364418e+01 -1.27943211e+01 -8.70891571e+00 -8.21891117e+00 -1.29038582e+01  1.92072392e+01 -1.02474661e+01 -1.35600481e+01 -2.49298973e+01  1.21316843e+01  1.06142159e+01  5.01210642e+00  1.97072697e+01  3.25507236e+00  2.58122711e+01 -2.11612778e+01  7.61706352e+00  6.21892357e+00  1.59677324e+01  1.70974503e+01  1.08788162e-01  1.23369656e+01 -3.72215819e+00  3.26099586e+00 -1.89902954e+01  2.60883408e+01 -1.31938295e+01 -4.39675236e+00 -8.62354851e+00 -2.05968246e+01 -1.81188450e+01  2.45467682e+01 -8.25877666e+00  4.03724384e+00\n",
      "  2.46818371e+01 -1.80974503e+01 -1.07113123e+01  1.29289455e+01 -1.91886272e+01  6.14852548e-01  1.49536061e+00  1.24218597e+01  5.48743343e+00 -1.78819675e+01  2.57116299e+01 -1.37148905e+01 -9.79510498e+00 -1.56925459e+01 -1.29194508e+01  1.14943295e+01  5.03084898e+00 -2.00367298e+01 -2.20635529e+01 -1.06273861e+01  3.29696202e+00  1.96599255e+01 -1.05446358e+01  8.86569309e+00 -1.10868692e+01  6.09399796e+00 -2.02719955e+01  5.08969307e+00  1.93932190e+01 -2.63901901e+00  1.23106842e+01  2.93946719e+00 -9.09021759e+00  8.69574261e+00 -4.47090530e+00 -1.17757101e+01  2.62541943e+01 -2.69092026e+01  1.07129278e+01  1.69570565e+00 -2.30962777e+00 -3.71791697e+00  7.82326794e+00  1.08512154e+01  7.11082315e+00  1.70730438e+01  1.61550121e+01  1.45339499e+01 -9.17154121e+00 -1.99203229e+00  4.19264030e+00  3.98181856e-01 -1.99249268e+01  6.34595275e-01 -1.79033337e+01  1.25024481e+01  1.39445858e+01  1.87396641e+01  6.73657560e+00 -1.02146387e+01 -1.90990524e+01 -5.97611904e+00\n",
      "  7.46745050e-01  3.43830776e+00  1.31472282e+01 -6.07031393e+00  1.77240791e+01 -1.51210034e+00  2.28539562e+01 -2.81574059e+00 -3.04948187e+00 -1.14444876e+01  2.02621293e+00  8.13354194e-01  6.14400339e+00  5.25454426e+00 -1.05402060e+01 -1.13877344e+01 -1.85144558e+01  4.11683178e+00 -8.34581470e+00  3.31066895e+00 -4.10191393e+00  1.51643124e+01  1.64959087e+01  1.13759413e+01  1.07183390e+01  6.69435263e+00  4.00151873e+00  1.63416595e+01  5.21303844e+00 -2.73681278e+01 -1.17844906e+01 -1.02305679e+01 -1.63452396e+01 -1.27371817e+01 -4.25655270e+00 -1.88529072e+01 -1.37869740e+01  7.30856657e+00  5.62724543e+00 -2.07578678e+01  5.23661900e+00 -8.30124855e+00  6.96436691e+00  1.96106491e+01 -8.38685513e+00  1.40958385e+01  1.16866951e+01  1.81938496e+01  2.30948985e-01 -1.72957916e+01  1.01014442e+01 -1.67859440e+01 -1.38772602e+01  5.90283155e+00 -5.89195681e+00  6.29339027e+00 -1.16830740e+01 -1.42412004e+01  1.36068239e+01  1.15797291e+01  9.48634052e+00  1.68339043e+01\n",
      "  1.03962135e+00 -1.83325207e+00 -2.65938354e+00  6.34659481e+00 -1.03799248e+01  1.35805261e+00 -1.12258215e+01 -6.71915150e+00 -9.31715298e+00  2.12532449e+00 -1.19746590e+01 -2.75101604e+01 -1.33116379e+01  1.56362028e+01 -1.33662233e+01 -1.49942875e+01 -7.62966967e+00  3.27452736e+01  1.11236601e+01 -1.94220524e+01 -6.97145176e+00  7.45384598e+00  1.09965086e+01 -9.61211300e+00  1.01032076e+01 -2.64503813e+00  7.99698162e+00 -1.09549789e+01  1.10210733e+01 -1.02935247e+01  1.29686337e+01 -1.61125126e+01 -3.72336435e+00 -6.15545225e+00 -2.74320126e+00  1.21273403e+01 -1.39843884e+01 -8.59596062e+00 -3.97201490e+00 -4.92142773e+00 -3.93058586e+00 -7.28214931e+00 -1.40366888e+01 -1.14774075e+01 -1.58537045e+01  2.15422783e+01 -3.70448494e+00  5.17721462e+00  6.86063337e+00 -8.33185101e+00 -3.21387339e+00  1.43100443e+01  2.81921554e+00  1.93898449e+01 -8.99358749e+00  1.11234570e+01  2.20445299e+00  1.38322234e+00 -9.55499840e+00 -7.51355696e+00  1.75189853e+00 -5.59305382e+00\n",
      " -5.22281837e+00  9.47336006e+00 -1.27352552e+01 -2.02562027e+01  1.25539455e+01  1.96461239e+01 -5.81748295e+00  5.70448160e+00  1.48248136e+00  1.63479023e+01  1.68709831e+01  8.88283348e+00  8.75288546e-01 -1.41247454e+01  4.82168484e+00  1.63251228e+01  9.79373741e+00  1.77483559e+01 -2.21882458e+01 -2.58241105e+00  1.60657291e+01  1.08956547e+01  1.71236000e+01 -5.18566132e+00  7.45820904e+00 -2.47767277e+01 -4.30685568e+00  1.79727249e+01 -9.25273895e+00 -9.65740871e+00 -1.42640102e+00 -2.23096504e+01 -7.62145662e+00 -7.63627386e+00 -1.07442026e+01  3.40360785e+00  4.70657349e+00 -2.32698650e+01  2.64071679e+00  9.31102085e+00 -1.16344471e+01  7.07672215e+00  4.52783108e+00 -3.26316929e+00  7.20296049e+00 -1.07946758e+01 -3.29403925e+00  6.02764702e+00  7.64997482e+00 -1.42942457e+01  2.28814735e+01 -8.29971409e+00  1.57457752e+01 -9.68166447e+00  1.60309582e+01 -9.04385626e-01  2.30674267e+01 -2.29357395e+01 -1.03222501e+00  6.60805559e+00  8.71082020e+00  2.29275112e+01\n",
      "  1.27354765e+01  1.33594112e+01  1.42258148e+01 -1.11007881e+01  1.76030588e+00  1.43452568e+01 -2.48810101e+00  1.84988060e+01  1.55727692e+01 -1.77620316e+01 -2.72089529e+00 -9.53924274e+00 -1.07637691e+01  2.67968597e+01  5.60493469e+00  5.70373106e+00 -5.75134337e-01 -4.97296619e+00  5.72349882e+00 -1.79686756e+01 -7.02886677e+00 -2.85410690e+01  1.16001558e+00 -3.06276011e+00  5.19251728e+00  8.22912407e+00  1.85734291e+01  2.14550924e+00  3.61997414e+00 -1.61342316e+01 -1.58374300e+01  4.65176058e+00  8.88122559e+00  2.41278687e+01 -2.46466327e+00  4.18078709e+00  2.28828468e+01  2.36152244e+00  2.14304390e+01 -8.48633385e+00 -3.06125107e+01  1.62657776e+01 -2.16868496e+01 -8.18786025e-02  2.65234890e+01 -6.98239505e-02 -6.74680662e+00  1.07659435e+01  1.15859184e+01  1.28880501e+01 -7.51422942e-01  1.78964748e+01  2.11466007e+01 -1.58899889e+01  2.01079464e+01 -1.60545540e+01 -7.98998642e+00 -2.36329784e+01 -5.56599140e-01 -1.30146046e+01  9.11573887e+00 -9.85191643e-01\n",
      "  1.91665707e+01 -1.48252897e+01 -1.98952141e+01 -1.77357903e+01  6.62531376e+00  1.85580883e+01  5.34526348e+00  9.61287737e-01 -2.21364136e+01 -5.11510372e+00 -4.95137262e+00  1.04604788e+01 -2.15088634e+01  1.68001442e+01 -1.60757713e+01 -1.03016579e+00 -2.35177994e+00  1.58252850e+01  1.55432248e+00 -9.44428062e+00  8.01473808e+00 -1.08972044e+01 -2.53411617e+01 -4.09961796e+00 -1.69784336e+01  2.23504162e+01  3.04262161e+00  1.79447060e+01  8.95124817e+00 -7.32827854e+00  1.25014839e+01 -1.19587145e+01 -1.05939121e+01  6.44384813e+00  7.48316956e+00  2.94994116e+00 -4.62690687e+00 -1.59088564e+01  3.81198740e+00 -2.86737823e+00 -2.34428387e+01 -2.34798521e-01 -8.07698250e+00  1.47356596e+01  6.67070389e+00 -1.72239552e+01 -3.18950295e+00  1.65552597e+01 -2.35983639e+01 -1.09475584e+01 -1.29655876e+01  6.62692881e+00  2.57564974e+00 -7.64560890e+00 -2.76692085e+01  1.37669048e+01  5.46188450e+00 -9.74186134e+00 -5.43419409e+00 -5.88753319e+00  2.14955139e+01  1.18841763e+01\n",
      " -4.12470007e+00 -8.23065948e+00  2.20778008e+01  3.12201176e+01 -1.39169579e+01  1.18494205e+01 -2.25120888e+01 -1.45599098e+01 -1.15371008e+01 -1.76558266e+01  1.13361454e+01 -1.03665991e+01  4.55395412e+00 -6.74286938e+00  1.40801067e+01 -1.93354855e+01  1.77701321e+01 -6.04053545e+00 -1.02499933e+01 -3.12584734e+00 -5.92369938e+00 -5.73230505e+00 -5.78255355e-01  1.59819641e+01 -5.89258194e+00  4.67805386e+00  1.27735615e+01 -1.14136925e+01  9.66437912e+00  8.84517479e+00 -1.43763123e+01 -7.21393526e-01  3.97522187e+00 -1.01937084e+01  4.82519674e+00 -1.01703072e+01 -5.10623360e+00  9.61265469e+00 -2.45550108e+00 -6.27570200e+00  6.81131983e+00 -2.62764406e+00  1.70654316e+01 -6.74568844e+00  1.76555576e+01  1.01330156e+01  1.27696075e+01 -7.24594355e+00  1.93064995e+01  4.75721836e+00 -1.50246363e+01  2.01107693e+01  1.29131947e+01  3.47625804e+00  1.05034237e+01  4.70491171e+00  2.92515326e+00  2.44983120e+01  2.83148766e+00  7.09396076e+00  1.06569242e+01  2.24483891e+01\n",
      "  9.52382088e+00 -1.65487766e+01  1.34344454e+01 -5.18755817e+00  9.28996277e+00 -2.04712524e+01  2.00779381e+01  4.26811171e+00  2.71684322e+01  1.18541298e+01  2.43016338e+01  6.30688381e+00  2.39343147e+01  5.16061664e-01 -7.93094015e+00 -1.66124763e+01  1.32686605e+01 -6.80666018e+00  1.88025246e+01  9.15692902e+00 -1.18252287e+01  2.15733929e+01  1.05066853e+01 -1.85149364e+01  8.71696186e+00 -9.03061485e+00 -7.26254463e+00  2.00468998e+01  1.51153898e+01  1.36953468e+01 -1.02509153e+00 -2.40656815e+01  7.95920086e+00  1.40000677e+01  4.68887901e+00 -2.79211674e+01  1.91593227e+01 -7.87160444e+00 -7.85089302e+00  1.36105108e+01  8.73404312e+00 -1.02764521e+01 -2.39207447e-01 -2.56505928e+01 -1.37385149e+01 -1.87389507e+01  2.25985241e+01 -1.25906839e+01  2.70850639e+01 -2.23357563e+01 -1.02048130e+01  5.53245592e+00  1.17055473e+01 -1.39810400e+01 -1.91324368e+01 -1.06565781e+01 -1.32114248e+01 -1.98796883e+01 -2.87932086e+00  2.15462723e+01  1.55806618e+01 -1.23826885e+01\n",
      " -1.37705736e+01 -1.05820864e-01  8.73197746e+00 -1.85880280e+01  4.87584734e+00 -2.31125093e+00 -1.09576092e+01 -2.02911019e+00  4.76102114e+00 -1.75805416e+01  8.56838417e+00  1.05637331e+01 -6.21935701e+00  1.08551826e+01  9.05845046e-01  1.89796066e+01  1.13822708e+01  1.62591672e+00  2.13150654e+01 -1.31512794e+01  1.20916471e+01 -1.25893011e+01 -2.30167522e+01  7.71118593e+00  2.69611049e+00 -4.00530052e+00  6.60361195e+00 -1.39381242e+00 -1.27147079e+00 -1.67267113e+01 -4.11152411e+00 -9.98935699e-02 -8.98583221e+00  6.36828613e+00 -3.73679304e+00 -3.66066122e+00 -3.28396368e+00  1.90350342e+01 -7.60652065e+00  8.15215397e+00 -5.23231506e+00  7.52370501e+00  8.29451180e+00 -1.38963032e+00 -5.59883738e+00  9.93983936e+00  3.89074469e+00 -2.11774902e+01 -1.54696627e+01  2.38881550e+01 -3.17565584e+00  6.45833373e-01 -1.93167725e+01 -3.98097992e+00 -8.94104958e+00  1.96020088e+01 -1.75795307e+01 -8.60536671e+00 -1.78826004e-01 -1.73116951e+01 -1.31495514e+01  1.46661320e+01\n",
      " -4.71872759e+00  4.21562910e+00 -1.83395233e+01 -5.67693806e+00 -1.15910664e+01 -3.16403484e+00  4.61431503e+00  1.40135813e+00  4.73532629e+00 -4.94795179e+00  1.18038149e+01 -9.39599991e+00  7.20925856e+00  1.45991344e+01  2.82135510e+00 -2.67636909e+01 -1.48856583e+01  1.72692528e+01 -6.38006639e+00  1.24583216e+01  1.90721302e+01 -2.08752289e+01  5.02786160e-01 -8.57693672e+00  1.57225618e+01  1.03100863e+01  1.40377665e+01  1.39262867e+01 -1.74333763e+01  3.47778964e+00 -8.87400246e+00  2.15256405e+01  2.77776003e+00  5.61432362e+00  8.49061966e+00  1.25931587e+01  2.05610065e+01  5.25289059e+00 -1.95738907e+01  9.41693783e+00 -8.36818314e+00 -5.44386959e+00 -1.25345745e+01 -1.44597664e+01  1.75897431e+00  2.80733910e+01  1.54812136e+01 -1.83130531e+01 -7.13068104e+00  1.38814039e+01 -1.22684383e+01 -7.50024176e+00  3.08403492e+00 -7.83740711e+00 -1.29699764e+01 -1.62274990e+01  1.00600710e+01  1.04719725e+01  1.15474052e+01 -2.56144691e+00  1.13415794e+01  2.47606583e+01\n",
      "  2.07262707e+01 -8.26008034e+00 -4.48270226e+00  1.66715279e+01 -7.89482641e+00  1.68031292e+01 -1.93397903e+01 -1.59997673e+01 -1.71902809e+01  3.33488792e-01  1.25700111e+01 -1.63966990e+00  2.11778984e+01  9.40174961e+00  8.53121948e+00 -1.05721550e+01 -2.51739526e+00  2.77921829e+01  1.21511211e+01  1.43963966e+01  1.39684782e+01 -2.23588543e+01  1.67158604e+01 -6.19843388e+00 -1.14702063e+01 -7.56706858e+00  6.29593945e+00 -7.10847950e+00  1.15451756e+01 -9.13354492e+00  2.41936326e+00 -1.39798880e+01  9.94439793e+00 -1.37748432e+01 -2.49377880e+01 -1.33318167e+01 -4.08722925e+00 -1.01628294e+01 -1.33217640e+01  1.19916687e+01  2.03871036e+00  9.36752892e+00  8.61335468e+00  6.19411755e+00  1.42655563e+01 -1.42026720e+01  1.06748800e+01  2.00208950e+00  1.03524523e+01 -1.04485261e+00  3.12598839e+01  1.70599520e+00  9.89449883e+00  5.83135509e+00  1.44919038e-01 -1.14049208e+00  2.23458729e+01  1.88115768e+01 -1.59143238e+01 -2.67723179e+01 -1.77586098e+01 -1.34664154e+01\n",
      "  1.37093735e+01 -5.30203009e+00  1.12909555e+01 -3.12378597e+00 -9.59579086e+00 -1.48518384e+00 -6.39071274e+00 -6.79215145e+00  1.72173977e+01 -1.28801432e+01  1.02758398e+01 -2.11237831e+01  1.97614193e+01  8.87255955e+00  1.30877962e+01  9.99099064e+00  1.04180841e+01 -1.58766289e+01 -1.50166674e+01  1.34951744e+01 -6.06835270e+00  4.89994526e+00  1.70303459e+01 -1.67420635e+01 -1.09582062e+01  1.74726558e+00  1.08736658e+01  2.16829128e+01  4.61913776e+00  1.31580276e+01 -1.48158875e+01  8.35214233e+00 -9.82205296e+00  3.33199763e+00 -1.04207935e+01 -1.30601530e+01  2.11861782e+01 -8.61929226e+00 -5.68513298e+00  1.22356796e+01 -1.30466115e+00 -1.89447937e+01  1.32727451e+01 -2.39980049e+01 -7.94388056e+00 -1.37343121e+01  1.00506771e+00  7.70433855e+00  2.49036579e+01 -1.13595695e+01 -1.40974426e+01  4.95247173e+00 -1.89690266e+01 -7.70342636e+00 -2.86863098e+01 -1.46105337e+00  3.00784540e+00  1.05949430e+01  7.61600351e+00  2.43239069e+00 -7.34284258e+00  1.44439209e+00\n",
      "  2.17495232e+01 -1.46408272e+01  2.27646866e+01 -1.23703032e+01  1.66194592e+01 -1.03780642e+01  4.66952085e+00  1.41947079e+00 -1.20479031e+01  1.35593281e+01  9.79774189e+00  9.54801655e+00 -1.80425816e+01  8.04353714e+00 -3.09125519e+01  9.11199951e+00  1.58135757e+01  1.86350574e+01 -1.37423220e+01  2.56211586e+01 -5.16545200e+00  4.23042583e+00 -2.07376728e+01  1.08216848e+01  1.23061571e+01  6.29351044e+00 -1.59747581e+01 -1.59004507e+01 -1.06017494e+00  1.37043023e+00 -1.92726479e+01 -4.14642525e+00 -1.07280207e+01  4.65437841e+00 -1.04775505e+01 -2.03819008e+01 -7.99035072e-01  1.04860554e+01 -5.18739176e+00 -3.40557289e+00  7.58474970e+00 -2.01150494e+01 -1.35914879e+01  6.71237707e-01 -1.30308876e+01  7.11896467e+00 -1.10416527e+01 -2.04841881e+01  2.32382526e+01 -1.77064667e+01  1.62964458e+01  8.68677616e-01 -1.72806740e+01  5.27643299e+00 -2.32448840e+00  8.60806942e+00 -9.85224724e-01 -1.87663937e+01 -1.40767298e+01  7.09296608e+00  2.57004452e+01 -1.76368008e+01\n",
      " -1.09731092e+01 -1.70315189e+01 -1.14442930e+01 -1.69107666e+01  2.90745044e+00 -1.36482430e+01  2.82400227e+01 -9.13505268e+00 -1.36113338e+01 -7.02358961e+00 -1.53973055e+00  1.96436729e+01 -2.59827957e+01  2.63320026e+01 -6.54500771e+00 -4.70681190e+00  2.41923084e+01 -1.31607962e+01 -1.20527220e+01  1.32950792e+01 -3.12672853e-02 -3.98044562e+00  1.11265481e+00  1.96505809e+00 -1.03135071e+01  1.90741711e+01 -1.51913614e+01 -2.65668106e+01  1.03486061e+00 -8.92714024e+00  8.46207142e+00 -4.29173565e+00  4.95354843e+00 -3.23404884e+01 -2.56129408e+00  4.49347591e+00  1.66878338e+01  1.16154156e+01  3.85297000e-01  8.22041512e+00 -1.36306248e+01  7.03523540e+00  4.46780539e+00 -1.36307840e+01  9.53442383e+00  1.39666281e+01  1.68617573e+01  2.29142323e+01 -1.35160704e+01  6.56937838e+00  5.52732658e+00 -1.29161043e+01 -1.54204664e+01 -1.17084246e+01 -1.20690041e+01  9.55934811e+00 -8.58126926e+00 -2.45298405e+01  1.85081997e+01  1.76917477e+01  3.71105814e+00 -4.73802757e+00\n",
      "  1.89064140e+01  1.03088350e+01 -1.94689941e+01 -7.90207052e+00  1.16865883e+01 -1.42535601e+01  1.70612259e+01 -1.88244667e+01  1.09536095e+01 -2.30414810e+01 -9.01025772e+00  3.53330040e+00  1.60092525e+01  2.00607224e+01 -1.53984232e+01  9.70432854e+00 -6.22728729e+00  3.57887840e+00 -1.17556834e+00  1.71154747e+01  1.01273556e+01 -1.22909794e+01 -1.50184612e+01 -5.23901463e-01 -1.47814703e+01 -1.23385131e+00 -4.09170771e+00  1.43485384e+01 -5.52958632e+00  6.69087076e+00 -6.72304535e+00 -7.15662384e+00  1.38755093e+01 -7.62565792e-01  5.95905685e+00 -1.04400158e+01 -2.13192539e+01 -1.50680077e+00 -2.72456264e+00  9.79028988e+00 -7.04792976e+00 -1.60956268e+01 -7.80917501e+00 -1.06220016e+01 -1.55736675e+01 -1.67266273e+01 -1.60005074e+01 -2.83075333e+00 -1.68870277e+01 -1.37632027e+01 -1.18431225e+01 -9.77906227e+00  1.07569370e+01 -1.43007765e+01 -4.68197584e-01  1.57654829e+01  2.08397617e+01 -3.63840532e+00 -1.21712761e+01 -1.49442244e+01  8.22919607e-03 -5.87022161e+00\n",
      " -5.92486048e+00  5.10055971e+00 -2.97015381e+00  3.23695421e+00  1.22873230e+01 -1.30602865e+01  1.30953443e+00  1.41264677e+01  9.07264519e+00  2.80468374e-01  4.58197498e+00  2.32536812e+01  1.00610638e+01 -2.84329662e+01 -9.22135830e+00  1.23769960e+01 -1.15614805e+01 -2.12532921e+01  9.21905994e+00  1.33295135e+01  1.10647478e+01  6.87080193e+00  8.95841408e+00  1.11722231e+01 -1.72590218e+01  1.70132961e+01 -2.38894310e+01 -1.96404684e+00  1.67844162e+01  1.18169575e+01 -1.48855839e+01 -9.91752720e+00 -1.92530327e+01  1.82987347e+01  2.07829738e+00  6.27079630e+00 -2.02659518e-01 -1.46644392e+01 -1.20157557e+01  2.88722744e+01 -6.49857807e+00 -5.32904530e+00  1.04062281e+01  2.40459180e+00  5.45185947e+00  1.26887817e+01 -9.69974613e+00  1.76867867e+01  8.47007573e-01  3.26407862e+00 -1.59524746e+01 -1.12893753e+01  7.83559227e+00 -4.84886694e+00  4.92734671e+00 -3.81232214e+00 -1.10597353e+01  2.05318546e+01 -2.45999107e+01  4.25042629e+00  1.98595657e+01  7.28840733e+00\n",
      "  1.12874069e+01  2.78848457e+01 -2.18925667e+00 -1.82903366e+01  1.09105444e+01  8.64392471e+00 -1.35092602e+01  1.75468807e+01 -1.48528652e+01  9.65112019e+00 -4.31401253e+00  1.45127563e+01 -7.37691498e+00 -2.04253445e+01 -8.02913380e+00 -1.01552820e+01  2.11486435e+00  3.92982054e+00 -1.16251898e+01 -2.30527992e+01  9.00153923e+00 -9.68387127e+00  3.07007885e+01 -2.29921646e+01  9.06035328e+00 -1.00700397e+01 -2.53881683e+01 -6.33497095e+00 -6.49411154e+00  2.60312233e+01 -5.56244516e+00 -8.25705814e+00 -1.55388224e+00  1.84591084e+01  8.40070724e+00  2.05970573e+00 -3.88182926e+00 -1.41848764e+01  9.91840935e+00  1.20124969e+01 -1.00851941e+00 -1.88769188e+01  8.76615238e+00 -8.79746628e+00  9.47153854e+00  1.57968836e+01  5.85398340e+00  1.64438057e+01  6.61163902e+00  1.03349042e+00 -1.07147636e+01  4.07199264e-02 -6.90103149e+00 -1.35966206e+00 -2.25108414e+01  6.59121799e+00  4.21499300e+00 -2.69593525e+00  1.04471540e+01  1.28148298e+01  1.18638535e+01  6.13639295e-01\n",
      "  1.45615921e+01 -2.05455189e+01 -1.97401962e+01 -2.49579811e+01  1.61746159e+01  1.17647181e+01  3.85128546e+00  1.98494186e+01 -2.05162888e+01  2.91066551e+01 -1.99457531e+01 -1.73458176e+01  1.55912018e+01 -1.16363468e+01 -4.73422050e+00 -7.80984402e-01  2.19488945e+01 -2.42689838e+01  1.22735424e+01  2.20120792e+01  1.63054504e+01 -9.03785229e+00  1.03548708e+01  6.41395903e+00  2.69563246e+00  9.26376629e+00  2.62744026e+01 -7.51005507e+00  1.76269608e+01  1.42345915e+01 -2.03293419e+01 -4.82991695e+00 -1.57234631e+01 -1.49905415e+01  1.15409327e+01 -3.51941586e+00  1.07974310e+01 -9.37300777e+00 -1.35149155e+01 -4.43598652e+00 -1.56554937e+01 -2.13649788e+01 -1.65599899e+01  4.18624544e+00  4.13836908e+00  1.34002018e+01  2.12916565e+01  1.13841562e+01  1.36058159e+01 -1.25745554e+01 -5.62341452e+00  2.45710659e+00 -1.72951565e+01  5.82547426e+00  1.71059799e+01 -2.14438572e+01  6.13192844e+00 -7.18259239e+00  5.83690786e+00 -9.53930187e+00  7.43608618e+00  1.09738312e+01\n",
      "  1.62036972e+01  9.69499302e+00 -9.36362386e-01 -1.12601781e+00 -2.36416225e+01  6.70979261e+00  4.59766579e+00  1.08950558e+01  1.13043966e+01  1.36384830e+01  1.61724014e+01 -7.28558111e+00 -9.68096256e+00 -1.55302334e+01  2.27990646e+01 -1.83781872e+01  8.94353771e+00 -9.27481365e+00 -1.88056362e+00  1.45752373e+01  1.39730568e+01 -7.96615791e+00 -1.27350225e+01  8.69257545e+00 -1.04487667e+01  6.18421936e+00  1.77571392e+01 -5.72827673e+00 -2.23897514e+01  1.09126434e+01 -1.32893729e+00 -2.17288518e+00 -8.10369301e+00 -1.31390781e+01 -5.51106882e+00  1.60480595e+01 -5.67179346e+00 -9.70903969e+00  7.67108250e+00 -1.28905859e+01 -1.05428581e+01 -9.66395378e+00  8.53011417e+00 -1.12786341e+01 -2.15514183e+01 -1.42844677e-01 -5.30789232e+00 -4.69698763e+00  1.86795559e+01 -1.58988256e+01  2.52068329e+00  5.38501310e+00 -2.36964169e+01  1.57589006e+01  1.69447155e+01  9.28431129e+00 -1.63835392e+01 -1.68491535e+01 -2.11161060e+01  1.35536641e-01  2.23935204e+01 -1.09639568e+01\n",
      "  1.78641930e+01  2.43059521e+01 -1.31314983e+01  1.26885481e+01  6.62738132e+00  3.29100704e+00  1.91397552e+01  1.46144972e+01 -1.55170274e+00  1.39673738e+01  3.57897043e+00  2.81217718e+00  6.19319201e-01 -4.53999329e+00 -1.49783001e+01  1.19955235e+01  1.41925168e+00 -2.48109722e+00 -4.15164590e-01  3.75908780e+00 -1.33082943e+01 -1.40716171e+01  1.25111971e+01  9.20116329e+00 -1.95623760e+01 -2.42230740e+01  3.65020370e+01  7.86227465e-01  1.11640453e+01 -1.50684500e+00  7.91312408e+00 -5.98942459e-01  2.65765362e+01 -1.44944525e+01 -8.26131058e+00 -5.04601860e+00 -1.45093117e+01 -2.13134785e+01 -2.53297520e+01 -1.31274319e+01 -1.86542950e+01 -4.52299643e+00 -1.73349228e+01 -1.49467621e+01  2.07691612e+01  1.57731075e+01  4.18746376e+00  1.25095046e+00  3.30261779e+00  1.26160679e+01  2.96159725e+01 -2.65794683e+00  2.06434708e+01  1.20568762e+01  2.11228538e+00 -5.11035395e+00 -1.73076057e+01 -6.89462948e+00  4.91306257e+00 -8.83447266e+00  3.37165427e+00  1.77567368e+01\n",
      "  1.15197115e+01 -1.97400055e+01 -1.54033146e+01 -4.37792969e+00  5.31834316e+00 -1.48906727e+01  1.50864630e+01 -2.41156425e+01 -1.04824753e+01 -6.38819170e+00 -7.72970676e+00  9.34290600e+00  2.16665993e+01  1.14066925e+01  1.37180958e+01 -1.65202351e+01 -1.79296227e+01 -1.27236319e+01  1.91351051e+01  1.46595764e+01 -1.61907024e+01  7.04441452e+00  1.25620222e+01 -7.84876442e+00 -1.03058891e+01 -7.52558088e+00  3.45317841e+00 -3.25789273e-01 -2.58232689e+00  1.81377449e+01  1.63276634e+01 -6.25531292e+00 -1.54468040e+01  3.27237988e+00  6.37569284e+00 -1.28836370e+00  1.48575706e+01 -1.03319378e+01  2.90882397e+00 -9.41898823e+00  2.79310966e+00  8.60959339e+00  1.44389763e+01  9.58852577e+00  1.35657892e+01 -1.17108860e+01 -1.54715605e+01 -8.23498249e+00  5.67048371e-01  1.34822836e+01  2.30440197e+01  4.29161024e+00 -7.03233361e-01]\n",
      "Full Embedding 1:\n",
      "[ 2.85116100e+00  2.05316753e+01  8.30917740e+00  2.77599354e+01  4.93734026e+00  5.56273651e+00  1.63346729e+01  2.36758995e+01  1.85432415e+01 -1.16897087e+01 -3.79956460e+00 -6.44580650e+00  5.27722692e+00 -5.79063606e+00  2.32282734e+01  1.37613554e+01 -2.06316757e+00  1.43070784e+01  7.50589943e+00  4.52252531e+00  1.11168356e+01 -2.93024063e+00 -1.60223408e+01  9.50275421e+00  8.65117550e-01 -9.10232830e+00 -1.42041359e+01 -1.67259426e+01 -2.07097797e+01  7.96444845e+00  1.29832888e+00  8.79665184e+00  2.46938286e+01 -3.13759661e+00 -1.47409973e+01 -3.42307329e+00 -8.52990532e+00 -6.18487024e+00 -2.07017732e+00 -6.73620558e+00  1.18425636e+01  1.78384900e+00  6.30518103e+00 -1.48744545e+01 -1.87404194e+01  1.53625002e+01  2.29943504e+01 -4.10873079e+00  9.99921227e+00 -1.58318691e+01  2.12915611e+01  1.41741533e+01 -1.25254164e+01 -2.31318970e+01  1.03742981e+01  2.48843918e+01  1.68221073e+01 -2.60557690e+01 -1.14073906e+01 -8.00450134e+00 -2.11683826e+01  6.30641460e+00\n",
      "  2.40060291e+01  7.42987394e+00  4.37662458e+00 -2.04897251e+01  1.61540833e+01 -7.07390499e+00  5.44595051e+00  1.10513105e+01  8.54345036e+00 -7.54892540e+00 -1.43485918e+01  9.27373314e+00  1.43732023e+01 -3.25032425e+00  1.21951170e+01  1.10058517e+01 -4.05525589e+00  2.14780769e+01 -1.13630495e+01  1.64201317e+01  2.00279236e+01  1.57427931e+01  9.35720503e-01  1.46048155e+01 -1.82020700e+00 -2.43036060e+01  1.42515545e+01  1.06535311e+01  3.05364418e+01 -1.27943211e+01 -8.70891571e+00 -8.21891117e+00 -1.29038582e+01  1.92072392e+01 -1.02474661e+01 -1.35600481e+01 -2.49298973e+01  1.21316843e+01  1.06142159e+01  5.01210642e+00  1.97072697e+01  3.25507236e+00  2.58122711e+01 -2.11612778e+01  7.61706352e+00  6.21892357e+00  1.59677324e+01  1.70974503e+01  1.08788162e-01  1.23369656e+01 -3.72215819e+00  3.26099586e+00 -1.89902954e+01  2.60883408e+01 -1.31938295e+01 -4.39675236e+00 -8.62354851e+00 -2.05968246e+01 -1.81188450e+01  2.45467682e+01 -8.25877666e+00  4.03724384e+00\n",
      "  2.46818371e+01 -1.80974503e+01 -1.07113123e+01  1.29289455e+01 -1.91886272e+01  6.14852548e-01  1.49536061e+00  1.24218597e+01  5.48743343e+00 -1.78819675e+01  2.57116299e+01 -1.37148905e+01 -9.79510498e+00 -1.56925459e+01 -1.29194508e+01  1.14943295e+01  5.03084898e+00 -2.00367298e+01 -2.20635529e+01 -1.06273861e+01  3.29696202e+00  1.96599255e+01 -1.05446358e+01  8.86569309e+00 -1.10868692e+01  6.09399796e+00 -2.02719955e+01  5.08969307e+00  1.93932190e+01 -2.63901901e+00  1.23106842e+01  2.93946719e+00 -9.09021759e+00  8.69574261e+00 -4.47090530e+00 -1.17757101e+01  2.62541943e+01 -2.69092026e+01  1.07129278e+01  1.69570565e+00 -2.30962777e+00 -3.71791697e+00  7.82326794e+00  1.08512154e+01  7.11082315e+00  1.70730438e+01  1.61550121e+01  1.45339499e+01 -9.17154121e+00 -1.99203229e+00  4.19264030e+00  3.98181856e-01 -1.99249268e+01  6.34595275e-01 -1.79033337e+01  1.25024481e+01  1.39445858e+01  1.87396641e+01  6.73657560e+00 -1.02146387e+01 -1.90990524e+01 -5.97611904e+00\n",
      "  7.46745050e-01  3.43830776e+00  1.31472282e+01 -6.07031393e+00  1.77240791e+01 -1.51210034e+00  2.28539562e+01 -2.81574059e+00 -3.04948187e+00 -1.14444876e+01  2.02621293e+00  8.13354194e-01  6.14400339e+00  5.25454426e+00 -1.05402060e+01 -1.13877344e+01 -1.85144558e+01  4.11683178e+00 -8.34581470e+00  3.31066895e+00 -4.10191393e+00  1.51643124e+01  1.64959087e+01  1.13759413e+01  1.07183390e+01  6.69435263e+00  4.00151873e+00  1.63416595e+01  5.21303844e+00 -2.73681278e+01 -1.17844906e+01 -1.02305679e+01 -1.63452396e+01 -1.27371817e+01 -4.25655270e+00 -1.88529072e+01 -1.37869740e+01  7.30856657e+00  5.62724543e+00 -2.07578678e+01  5.23661900e+00 -8.30124855e+00  6.96436691e+00  1.96106491e+01 -8.38685513e+00  1.40958385e+01  1.16866951e+01  1.81938496e+01  2.30948985e-01 -1.72957916e+01  1.01014442e+01 -1.67859440e+01 -1.38772602e+01  5.90283155e+00 -5.89195681e+00  6.29339027e+00 -1.16830740e+01 -1.42412004e+01  1.36068239e+01  1.15797291e+01  9.48634052e+00  1.68339043e+01\n",
      "  1.03962135e+00 -1.83325207e+00 -2.65938354e+00  6.34659481e+00 -1.03799248e+01  1.35805261e+00 -1.12258215e+01 -6.71915150e+00 -9.31715298e+00  2.12532449e+00 -1.19746590e+01 -2.75101604e+01 -1.33116379e+01  1.56362028e+01 -1.33662233e+01 -1.49942875e+01 -7.62966967e+00  3.27452736e+01  1.11236601e+01 -1.94220524e+01 -6.97145176e+00  7.45384598e+00  1.09965086e+01 -9.61211300e+00  1.01032076e+01 -2.64503813e+00  7.99698162e+00 -1.09549789e+01  1.10210733e+01 -1.02935247e+01  1.29686337e+01 -1.61125126e+01 -3.72336435e+00 -6.15545225e+00 -2.74320126e+00  1.21273403e+01 -1.39843884e+01 -8.59596062e+00 -3.97201490e+00 -4.92142773e+00 -3.93058586e+00 -7.28214931e+00 -1.40366888e+01 -1.14774075e+01 -1.58537045e+01  2.15422783e+01 -3.70448494e+00  5.17721462e+00  6.86063337e+00 -8.33185101e+00 -3.21387339e+00  1.43100443e+01  2.81921554e+00  1.93898449e+01 -8.99358749e+00  1.11234570e+01  2.20445299e+00  1.38322234e+00 -9.55499840e+00 -7.51355696e+00  1.75189853e+00 -5.59305382e+00\n",
      " -5.22281837e+00  9.47336006e+00 -1.27352552e+01 -2.02562027e+01  1.25539455e+01  1.96461239e+01 -5.81748295e+00  5.70448160e+00  1.48248136e+00  1.63479023e+01  1.68709831e+01  8.88283348e+00  8.75288546e-01 -1.41247454e+01  4.82168484e+00  1.63251228e+01  9.79373741e+00  1.77483559e+01 -2.21882458e+01 -2.58241105e+00  1.60657291e+01  1.08956547e+01  1.71236000e+01 -5.18566132e+00  7.45820904e+00 -2.47767277e+01 -4.30685568e+00  1.79727249e+01 -9.25273895e+00 -9.65740871e+00 -1.42640102e+00 -2.23096504e+01 -7.62145662e+00 -7.63627386e+00 -1.07442026e+01  3.40360785e+00  4.70657349e+00 -2.32698650e+01  2.64071679e+00  9.31102085e+00 -1.16344471e+01  7.07672215e+00  4.52783108e+00 -3.26316929e+00  7.20296049e+00 -1.07946758e+01 -3.29403925e+00  6.02764702e+00  7.64997482e+00 -1.42942457e+01  2.28814735e+01 -8.29971409e+00  1.57457752e+01 -9.68166447e+00  1.60309582e+01 -9.04385626e-01  2.30674267e+01 -2.29357395e+01 -1.03222501e+00  6.60805559e+00  8.71082020e+00  2.29275112e+01\n",
      "  1.27354765e+01  1.33594112e+01  1.42258148e+01 -1.11007881e+01  1.76030588e+00  1.43452568e+01 -2.48810101e+00  1.84988060e+01  1.55727692e+01 -1.77620316e+01 -2.72089529e+00 -9.53924274e+00 -1.07637691e+01  2.67968597e+01  5.60493469e+00  5.70373106e+00 -5.75134337e-01 -4.97296619e+00  5.72349882e+00 -1.79686756e+01 -7.02886677e+00 -2.85410690e+01  1.16001558e+00 -3.06276011e+00  5.19251728e+00  8.22912407e+00  1.85734291e+01  2.14550924e+00  3.61997414e+00 -1.61342316e+01 -1.58374300e+01  4.65176058e+00  8.88122559e+00  2.41278687e+01 -2.46466327e+00  4.18078709e+00  2.28828468e+01  2.36152244e+00  2.14304390e+01 -8.48633385e+00 -3.06125107e+01  1.62657776e+01 -2.16868496e+01 -8.18786025e-02  2.65234890e+01 -6.98239505e-02 -6.74680662e+00  1.07659435e+01  1.15859184e+01  1.28880501e+01 -7.51422942e-01  1.78964748e+01  2.11466007e+01 -1.58899889e+01  2.01079464e+01 -1.60545540e+01 -7.98998642e+00 -2.36329784e+01 -5.56599140e-01 -1.30146046e+01  9.11573887e+00 -9.85191643e-01\n",
      "  1.91665707e+01 -1.48252897e+01 -1.98952141e+01 -1.77357903e+01  6.62531376e+00  1.85580883e+01  5.34526348e+00  9.61287737e-01 -2.21364136e+01 -5.11510372e+00 -4.95137262e+00  1.04604788e+01 -2.15088634e+01  1.68001442e+01 -1.60757713e+01 -1.03016579e+00 -2.35177994e+00  1.58252850e+01  1.55432248e+00 -9.44428062e+00  8.01473808e+00 -1.08972044e+01 -2.53411617e+01 -4.09961796e+00 -1.69784336e+01  2.23504162e+01  3.04262161e+00  1.79447060e+01  8.95124817e+00 -7.32827854e+00  1.25014839e+01 -1.19587145e+01 -1.05939121e+01  6.44384813e+00  7.48316956e+00  2.94994116e+00 -4.62690687e+00 -1.59088564e+01  3.81198740e+00 -2.86737823e+00 -2.34428387e+01 -2.34798521e-01 -8.07698250e+00  1.47356596e+01  6.67070389e+00 -1.72239552e+01 -3.18950295e+00  1.65552597e+01 -2.35983639e+01 -1.09475584e+01 -1.29655876e+01  6.62692881e+00  2.57564974e+00 -7.64560890e+00 -2.76692085e+01  1.37669048e+01  5.46188450e+00 -9.74186134e+00 -5.43419409e+00 -5.88753319e+00  2.14955139e+01  1.18841763e+01\n",
      " -4.12470007e+00 -8.23065948e+00  2.20778008e+01  3.12201176e+01 -1.39169579e+01  1.18494205e+01 -2.25120888e+01 -1.45599098e+01 -1.15371008e+01 -1.76558266e+01  1.13361454e+01 -1.03665991e+01  4.55395412e+00 -6.74286938e+00  1.40801067e+01 -1.93354855e+01  1.77701321e+01 -6.04053545e+00 -1.02499933e+01 -3.12584734e+00 -5.92369938e+00 -5.73230505e+00 -5.78255355e-01  1.59819641e+01 -5.89258194e+00  4.67805386e+00  1.27735615e+01 -1.14136925e+01  9.66437912e+00  8.84517479e+00 -1.43763123e+01 -7.21393526e-01  3.97522187e+00 -1.01937084e+01  4.82519674e+00 -1.01703072e+01 -5.10623360e+00  9.61265469e+00 -2.45550108e+00 -6.27570200e+00  6.81131983e+00 -2.62764406e+00  1.70654316e+01 -6.74568844e+00  1.76555576e+01  1.01330156e+01  1.27696075e+01 -7.24594355e+00  1.93064995e+01  4.75721836e+00 -1.50246363e+01  2.01107693e+01  1.29131947e+01  3.47625804e+00  1.05034237e+01  4.70491171e+00  2.92515326e+00  2.44983120e+01  2.83148766e+00  7.09396076e+00  1.06569242e+01  2.24483891e+01\n",
      "  9.52382088e+00 -1.65487766e+01  1.34344454e+01 -5.18755817e+00  9.28996277e+00 -2.04712524e+01  2.00779381e+01  4.26811171e+00  2.71684322e+01  1.18541298e+01  2.43016338e+01  6.30688381e+00  2.39343147e+01  5.16061664e-01 -7.93094015e+00 -1.66124763e+01  1.32686605e+01 -6.80666018e+00  1.88025246e+01  9.15692902e+00 -1.18252287e+01  2.15733929e+01  1.05066853e+01 -1.85149364e+01  8.71696186e+00 -9.03061485e+00 -7.26254463e+00  2.00468998e+01  1.51153898e+01  1.36953468e+01 -1.02509153e+00 -2.40656815e+01  7.95920086e+00  1.40000677e+01  4.68887901e+00 -2.79211674e+01  1.91593227e+01 -7.87160444e+00 -7.85089302e+00  1.36105108e+01  8.73404312e+00 -1.02764521e+01 -2.39207447e-01 -2.56505928e+01 -1.37385149e+01 -1.87389507e+01  2.25985241e+01 -1.25906839e+01  2.70850639e+01 -2.23357563e+01 -1.02048130e+01  5.53245592e+00  1.17055473e+01 -1.39810400e+01 -1.91324368e+01 -1.06565781e+01 -1.32114248e+01 -1.98796883e+01 -2.87932086e+00  2.15462723e+01  1.55806618e+01 -1.23826885e+01\n",
      " -1.37705736e+01 -1.05820864e-01  8.73197746e+00 -1.85880280e+01  4.87584734e+00 -2.31125093e+00 -1.09576092e+01 -2.02911019e+00  4.76102114e+00 -1.75805416e+01  8.56838417e+00  1.05637331e+01 -6.21935701e+00  1.08551826e+01  9.05845046e-01  1.89796066e+01  1.13822708e+01  1.62591672e+00  2.13150654e+01 -1.31512794e+01  1.20916471e+01 -1.25893011e+01 -2.30167522e+01  7.71118593e+00  2.69611049e+00 -4.00530052e+00  6.60361195e+00 -1.39381242e+00 -1.27147079e+00 -1.67267113e+01 -4.11152411e+00 -9.98935699e-02 -8.98583221e+00  6.36828613e+00 -3.73679304e+00 -3.66066122e+00 -3.28396368e+00  1.90350342e+01 -7.60652065e+00  8.15215397e+00 -5.23231506e+00  7.52370501e+00  8.29451180e+00 -1.38963032e+00 -5.59883738e+00  9.93983936e+00  3.89074469e+00 -2.11774902e+01 -1.54696627e+01  2.38881550e+01 -3.17565584e+00  6.45833373e-01 -1.93167725e+01 -3.98097992e+00 -8.94104958e+00  1.96020088e+01 -1.75795307e+01 -8.60536671e+00 -1.78826004e-01 -1.73116951e+01 -1.31495514e+01  1.46661320e+01\n",
      " -4.71872759e+00  4.21562910e+00 -1.83395233e+01 -5.67693806e+00 -1.15910664e+01 -3.16403484e+00  4.61431503e+00  1.40135813e+00  4.73532629e+00 -4.94795179e+00  1.18038149e+01 -9.39599991e+00  7.20925856e+00  1.45991344e+01  2.82135510e+00 -2.67636909e+01 -1.48856583e+01  1.72692528e+01 -6.38006639e+00  1.24583216e+01  1.90721302e+01 -2.08752289e+01  5.02786160e-01 -8.57693672e+00  1.57225618e+01  1.03100863e+01  1.40377665e+01  1.39262867e+01 -1.74333763e+01  3.47778964e+00 -8.87400246e+00  2.15256405e+01  2.77776003e+00  5.61432362e+00  8.49061966e+00  1.25931587e+01  2.05610065e+01  5.25289059e+00 -1.95738907e+01  9.41693783e+00 -8.36818314e+00 -5.44386959e+00 -1.25345745e+01 -1.44597664e+01  1.75897431e+00  2.80733910e+01  1.54812136e+01 -1.83130531e+01 -7.13068104e+00  1.38814039e+01 -1.22684383e+01 -7.50024176e+00  3.08403492e+00 -7.83740711e+00 -1.29699764e+01 -1.62274990e+01  1.00600710e+01  1.04719725e+01  1.15474052e+01 -2.56144691e+00  1.13415794e+01  2.47606583e+01\n",
      "  2.07262707e+01 -8.26008034e+00 -4.48270226e+00  1.66715279e+01 -7.89482641e+00  1.68031292e+01 -1.93397903e+01 -1.59997673e+01 -1.71902809e+01  3.33488792e-01  1.25700111e+01 -1.63966990e+00  2.11778984e+01  9.40174961e+00  8.53121948e+00 -1.05721550e+01 -2.51739526e+00  2.77921829e+01  1.21511211e+01  1.43963966e+01  1.39684782e+01 -2.23588543e+01  1.67158604e+01 -6.19843388e+00 -1.14702063e+01 -7.56706858e+00  6.29593945e+00 -7.10847950e+00  1.15451756e+01 -9.13354492e+00  2.41936326e+00 -1.39798880e+01  9.94439793e+00 -1.37748432e+01 -2.49377880e+01 -1.33318167e+01 -4.08722925e+00 -1.01628294e+01 -1.33217640e+01  1.19916687e+01  2.03871036e+00  9.36752892e+00  8.61335468e+00  6.19411755e+00  1.42655563e+01 -1.42026720e+01  1.06748800e+01  2.00208950e+00  1.03524523e+01 -1.04485261e+00  3.12598839e+01  1.70599520e+00  9.89449883e+00  5.83135509e+00  1.44919038e-01 -1.14049208e+00  2.23458729e+01  1.88115768e+01 -1.59143238e+01 -2.67723179e+01 -1.77586098e+01 -1.34664154e+01\n",
      "  1.37093735e+01 -5.30203009e+00  1.12909555e+01 -3.12378597e+00 -9.59579086e+00 -1.48518384e+00 -6.39071274e+00 -6.79215145e+00  1.72173977e+01 -1.28801432e+01  1.02758398e+01 -2.11237831e+01  1.97614193e+01  8.87255955e+00  1.30877962e+01  9.99099064e+00  1.04180841e+01 -1.58766289e+01 -1.50166674e+01  1.34951744e+01 -6.06835270e+00  4.89994526e+00  1.70303459e+01 -1.67420635e+01 -1.09582062e+01  1.74726558e+00  1.08736658e+01  2.16829128e+01  4.61913776e+00  1.31580276e+01 -1.48158875e+01  8.35214233e+00 -9.82205296e+00  3.33199763e+00 -1.04207935e+01 -1.30601530e+01  2.11861782e+01 -8.61929226e+00 -5.68513298e+00  1.22356796e+01 -1.30466115e+00 -1.89447937e+01  1.32727451e+01 -2.39980049e+01 -7.94388056e+00 -1.37343121e+01  1.00506771e+00  7.70433855e+00  2.49036579e+01 -1.13595695e+01 -1.40974426e+01  4.95247173e+00 -1.89690266e+01 -7.70342636e+00 -2.86863098e+01 -1.46105337e+00  3.00784540e+00  1.05949430e+01  7.61600351e+00  2.43239069e+00 -7.34284258e+00  1.44439209e+00\n",
      "  2.17495232e+01 -1.46408272e+01  2.27646866e+01 -1.23703032e+01  1.66194592e+01 -1.03780642e+01  4.66952085e+00  1.41947079e+00 -1.20479031e+01  1.35593281e+01  9.79774189e+00  9.54801655e+00 -1.80425816e+01  8.04353714e+00 -3.09125519e+01  9.11199951e+00  1.58135757e+01  1.86350574e+01 -1.37423220e+01  2.56211586e+01 -5.16545200e+00  4.23042583e+00 -2.07376728e+01  1.08216848e+01  1.23061571e+01  6.29351044e+00 -1.59747581e+01 -1.59004507e+01 -1.06017494e+00  1.37043023e+00 -1.92726479e+01 -4.14642525e+00 -1.07280207e+01  4.65437841e+00 -1.04775505e+01 -2.03819008e+01 -7.99035072e-01  1.04860554e+01 -5.18739176e+00 -3.40557289e+00  7.58474970e+00 -2.01150494e+01 -1.35914879e+01  6.71237707e-01 -1.30308876e+01  7.11896467e+00 -1.10416527e+01 -2.04841881e+01  2.32382526e+01 -1.77064667e+01  1.62964458e+01  8.68677616e-01 -1.72806740e+01  5.27643299e+00 -2.32448840e+00  8.60806942e+00 -9.85224724e-01 -1.87663937e+01 -1.40767298e+01  7.09296608e+00  2.57004452e+01 -1.76368008e+01\n",
      " -1.09731092e+01 -1.70315189e+01 -1.14442930e+01 -1.69107666e+01  2.90745044e+00 -1.36482430e+01  2.82400227e+01 -9.13505268e+00 -1.36113338e+01 -7.02358961e+00 -1.53973055e+00  1.96436729e+01 -2.59827957e+01  2.63320026e+01 -6.54500771e+00 -4.70681190e+00  2.41923084e+01 -1.31607962e+01 -1.20527220e+01  1.32950792e+01 -3.12672853e-02 -3.98044562e+00  1.11265481e+00  1.96505809e+00 -1.03135071e+01  1.90741711e+01 -1.51913614e+01 -2.65668106e+01  1.03486061e+00 -8.92714024e+00  8.46207142e+00 -4.29173565e+00  4.95354843e+00 -3.23404884e+01 -2.56129408e+00  4.49347591e+00  1.66878338e+01  1.16154156e+01  3.85297000e-01  8.22041512e+00 -1.36306248e+01  7.03523540e+00  4.46780539e+00 -1.36307840e+01  9.53442383e+00  1.39666281e+01  1.68617573e+01  2.29142323e+01 -1.35160704e+01  6.56937838e+00  5.52732658e+00 -1.29161043e+01 -1.54204664e+01 -1.17084246e+01 -1.20690041e+01  9.55934811e+00 -8.58126926e+00 -2.45298405e+01  1.85081997e+01  1.76917477e+01  3.71105814e+00 -4.73802757e+00\n",
      "  1.89064140e+01  1.03088350e+01 -1.94689941e+01 -7.90207052e+00  1.16865883e+01 -1.42535601e+01  1.70612259e+01 -1.88244667e+01  1.09536095e+01 -2.30414810e+01 -9.01025772e+00  3.53330040e+00  1.60092525e+01  2.00607224e+01 -1.53984232e+01  9.70432854e+00 -6.22728729e+00  3.57887840e+00 -1.17556834e+00  1.71154747e+01  1.01273556e+01 -1.22909794e+01 -1.50184612e+01 -5.23901463e-01 -1.47814703e+01 -1.23385131e+00 -4.09170771e+00  1.43485384e+01 -5.52958632e+00  6.69087076e+00 -6.72304535e+00 -7.15662384e+00  1.38755093e+01 -7.62565792e-01  5.95905685e+00 -1.04400158e+01 -2.13192539e+01 -1.50680077e+00 -2.72456264e+00  9.79028988e+00 -7.04792976e+00 -1.60956268e+01 -7.80917501e+00 -1.06220016e+01 -1.55736675e+01 -1.67266273e+01 -1.60005074e+01 -2.83075333e+00 -1.68870277e+01 -1.37632027e+01 -1.18431225e+01 -9.77906227e+00  1.07569370e+01 -1.43007765e+01 -4.68197584e-01  1.57654829e+01  2.08397617e+01 -3.63840532e+00 -1.21712761e+01 -1.49442244e+01  8.22919607e-03 -5.87022161e+00\n",
      " -5.92486048e+00  5.10055971e+00 -2.97015381e+00  3.23695421e+00  1.22873230e+01 -1.30602865e+01  1.30953443e+00  1.41264677e+01  9.07264519e+00  2.80468374e-01  4.58197498e+00  2.32536812e+01  1.00610638e+01 -2.84329662e+01 -9.22135830e+00  1.23769960e+01 -1.15614805e+01 -2.12532921e+01  9.21905994e+00  1.33295135e+01  1.10647478e+01  6.87080193e+00  8.95841408e+00  1.11722231e+01 -1.72590218e+01  1.70132961e+01 -2.38894310e+01 -1.96404684e+00  1.67844162e+01  1.18169575e+01 -1.48855839e+01 -9.91752720e+00 -1.92530327e+01  1.82987347e+01  2.07829738e+00  6.27079630e+00 -2.02659518e-01 -1.46644392e+01 -1.20157557e+01  2.88722744e+01 -6.49857807e+00 -5.32904530e+00  1.04062281e+01  2.40459180e+00  5.45185947e+00  1.26887817e+01 -9.69974613e+00  1.76867867e+01  8.47007573e-01  3.26407862e+00 -1.59524746e+01 -1.12893753e+01  7.83559227e+00 -4.84886694e+00  4.92734671e+00 -3.81232214e+00 -1.10597353e+01  2.05318546e+01 -2.45999107e+01  4.25042629e+00  1.98595657e+01  7.28840733e+00\n",
      "  1.12874069e+01  2.78848457e+01 -2.18925667e+00 -1.82903366e+01  1.09105444e+01  8.64392471e+00 -1.35092602e+01  1.75468807e+01 -1.48528652e+01  9.65112019e+00 -4.31401253e+00  1.45127563e+01 -7.37691498e+00 -2.04253445e+01 -8.02913380e+00 -1.01552820e+01  2.11486435e+00  3.92982054e+00 -1.16251898e+01 -2.30527992e+01  9.00153923e+00 -9.68387127e+00  3.07007885e+01 -2.29921646e+01  9.06035328e+00 -1.00700397e+01 -2.53881683e+01 -6.33497095e+00 -6.49411154e+00  2.60312233e+01 -5.56244516e+00 -8.25705814e+00 -1.55388224e+00  1.84591084e+01  8.40070724e+00  2.05970573e+00 -3.88182926e+00 -1.41848764e+01  9.91840935e+00  1.20124969e+01 -1.00851941e+00 -1.88769188e+01  8.76615238e+00 -8.79746628e+00  9.47153854e+00  1.57968836e+01  5.85398340e+00  1.64438057e+01  6.61163902e+00  1.03349042e+00 -1.07147636e+01  4.07199264e-02 -6.90103149e+00 -1.35966206e+00 -2.25108414e+01  6.59121799e+00  4.21499300e+00 -2.69593525e+00  1.04471540e+01  1.28148298e+01  1.18638535e+01  6.13639295e-01\n",
      "  1.45615921e+01 -2.05455189e+01 -1.97401962e+01 -2.49579811e+01  1.61746159e+01  1.17647181e+01  3.85128546e+00  1.98494186e+01 -2.05162888e+01  2.91066551e+01 -1.99457531e+01 -1.73458176e+01  1.55912018e+01 -1.16363468e+01 -4.73422050e+00 -7.80984402e-01  2.19488945e+01 -2.42689838e+01  1.22735424e+01  2.20120792e+01  1.63054504e+01 -9.03785229e+00  1.03548708e+01  6.41395903e+00  2.69563246e+00  9.26376629e+00  2.62744026e+01 -7.51005507e+00  1.76269608e+01  1.42345915e+01 -2.03293419e+01 -4.82991695e+00 -1.57234631e+01 -1.49905415e+01  1.15409327e+01 -3.51941586e+00  1.07974310e+01 -9.37300777e+00 -1.35149155e+01 -4.43598652e+00 -1.56554937e+01 -2.13649788e+01 -1.65599899e+01  4.18624544e+00  4.13836908e+00  1.34002018e+01  2.12916565e+01  1.13841562e+01  1.36058159e+01 -1.25745554e+01 -5.62341452e+00  2.45710659e+00 -1.72951565e+01  5.82547426e+00  1.71059799e+01 -2.14438572e+01  6.13192844e+00 -7.18259239e+00  5.83690786e+00 -9.53930187e+00  7.43608618e+00  1.09738312e+01\n",
      "  1.62036972e+01  9.69499302e+00 -9.36362386e-01 -1.12601781e+00 -2.36416225e+01  6.70979261e+00  4.59766579e+00  1.08950558e+01  1.13043966e+01  1.36384830e+01  1.61724014e+01 -7.28558111e+00 -9.68096256e+00 -1.55302334e+01  2.27990646e+01 -1.83781872e+01  8.94353771e+00 -9.27481365e+00 -1.88056362e+00  1.45752373e+01  1.39730568e+01 -7.96615791e+00 -1.27350225e+01  8.69257545e+00 -1.04487667e+01  6.18421936e+00  1.77571392e+01 -5.72827673e+00 -2.23897514e+01  1.09126434e+01 -1.32893729e+00 -2.17288518e+00 -8.10369301e+00 -1.31390781e+01 -5.51106882e+00  1.60480595e+01 -5.67179346e+00 -9.70903969e+00  7.67108250e+00 -1.28905859e+01 -1.05428581e+01 -9.66395378e+00  8.53011417e+00 -1.12786341e+01 -2.15514183e+01 -1.42844677e-01 -5.30789232e+00 -4.69698763e+00  1.86795559e+01 -1.58988256e+01  2.52068329e+00  5.38501310e+00 -2.36964169e+01  1.57589006e+01  1.69447155e+01  9.28431129e+00 -1.63835392e+01 -1.68491535e+01 -2.11161060e+01  1.35536641e-01  2.23935204e+01 -1.09639568e+01\n",
      "  1.78641930e+01  2.43059521e+01 -1.31314983e+01  1.26885481e+01  6.62738132e+00  3.29100704e+00  1.91397552e+01  1.46144972e+01 -1.55170274e+00  1.39673738e+01  3.57897043e+00  2.81217718e+00  6.19319201e-01 -4.53999329e+00 -1.49783001e+01  1.19955235e+01  1.41925168e+00 -2.48109722e+00 -4.15164590e-01  3.75908780e+00 -1.33082943e+01 -1.40716171e+01  1.25111971e+01  9.20116329e+00 -1.95623760e+01 -2.42230740e+01  3.65020370e+01  7.86227465e-01  1.11640453e+01 -1.50684500e+00  7.91312408e+00 -5.98942459e-01  2.65765362e+01 -1.44944525e+01 -8.26131058e+00 -5.04601860e+00 -1.45093117e+01 -2.13134785e+01 -2.53297520e+01 -1.31274319e+01 -1.86542950e+01 -4.52299643e+00 -1.73349228e+01 -1.49467621e+01  2.07691612e+01  1.57731075e+01  4.18746376e+00  1.25095046e+00  3.30261779e+00  1.26160679e+01  2.96159725e+01 -2.65794683e+00  2.06434708e+01  1.20568762e+01  2.11228538e+00 -5.11035395e+00 -1.73076057e+01 -6.89462948e+00  4.91306257e+00 -8.83447266e+00  3.37165427e+00  1.77567368e+01\n",
      "  1.15197115e+01 -1.97400055e+01 -1.54033146e+01 -4.37792969e+00  5.31834316e+00 -1.48906727e+01  1.50864630e+01 -2.41156425e+01 -1.04824753e+01 -6.38819170e+00 -7.72970676e+00  9.34290600e+00  2.16665993e+01  1.14066925e+01  1.37180958e+01 -1.65202351e+01 -1.79296227e+01 -1.27236319e+01  1.91351051e+01  1.46595764e+01 -1.61907024e+01  7.04441452e+00  1.25620222e+01 -7.84876442e+00 -1.03058891e+01 -7.52558088e+00  3.45317841e+00 -3.25789273e-01 -2.58232689e+00  1.81377449e+01  1.63276634e+01 -6.25531292e+00 -1.54468040e+01  3.27237988e+00  6.37569284e+00 -1.28836370e+00  1.48575706e+01 -1.03319378e+01  2.90882397e+00 -9.41898823e+00  2.79310966e+00  8.60959339e+00  1.44389763e+01  9.58852577e+00  1.35657892e+01 -1.17108860e+01 -1.54715605e+01 -8.23498249e+00  5.67048371e-01  1.34822836e+01  2.30440197e+01  4.29161024e+00 -7.03233361e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Disable truncation\n",
    "np.set_printoptions(threshold=np.inf, linewidth=1000)\n",
    "\n",
    "for i, emb in enumerate(val_embeddings):\n",
    "    print(f\"Full Embedding {i}:\\n{emb}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
