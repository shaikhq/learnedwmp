{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from model.database_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.util import Normalizer\n",
    "\n",
    "# cost_norm = Normalizer(1, 100)\n",
    "# cost_norm = Normalizer(-3.61192, 12.290855)\n",
    "#cost_norm = Normalizer(5, 2611)\n",
    "cost_norm = Normalizer(8.26, 11.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # bs = 1024\n",
    "    # SQ: smaller batch size\n",
    "    bs = 2\n",
    "    #lr = 0.001\n",
    "    lr = 0.001\n",
    "    # epochs = 200\n",
    "    epochs = 50\n",
    "    clip_size = 50\n",
    "    embed_size = 64\n",
    "    pred_hid = 128\n",
    "    ffn_dim = 128\n",
    "    head_size = 12\n",
    "    n_layers = 8\n",
    "    dropout = 0.1\n",
    "    sch_decay = 0.6\n",
    "    # device = 'cuda:0'\n",
    "    device = 'cpu'\n",
    "    newpath = 'job_queries_training'\n",
    "    to_predict = 'cost'\n",
    "args = Args()\n",
    "\n",
    "import os\n",
    "if not os.path.exists(args.newpath):\n",
    "    os.makedirs(args.newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import QueryFormer\n",
    "\n",
    "model = QueryFormer(emb_size = args.embed_size ,ffn_dim = args.ffn_dim, head_size = args.head_size, \\\n",
    "                 dropout = args.dropout, n_layers = args.n_layers, \\\n",
    "                 use_sample = False, use_hist = False, \\\n",
    "                 pred_hid = args.pred_hid\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import PlanTreeDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([1, 30, 1161])\n",
      "rel_pos shape: torch.Size([1, 30, 30])\n",
      "attn_bias shape: torch.Size([1, 31, 31])\n",
      "heights shape: torch.Size([1, 30])\n",
      "cost_labels shape: torch.Size([1])\n",
      "raw_costs shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Load the saved tensor collection\n",
    "loaded_tensors = torch.load(\"./job_queries/tensors/query_1_2024-12-03-20.43.44.200877.pt\")\n",
    "\n",
    "# Access each tensor\n",
    "x_loaded = loaded_tensors[\"x\"]\n",
    "rel_pos_loaded = loaded_tensors[\"rel_pos\"]\n",
    "attn_bias_loaded = loaded_tensors[\"attn_bias\"]\n",
    "heights_loaded = loaded_tensors[\"heights\"]\n",
    "cost_labels_loaded = loaded_tensors[\"cost_labels\"]\n",
    "raw_costs_loaded = loaded_tensors[\"raw_costs\"]\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(f\"x shape: {x_loaded.shape}\")\n",
    "print(f\"rel_pos shape: {rel_pos_loaded.shape}\")\n",
    "print(f\"attn_bias shape: {attn_bias_loaded.shape}\")\n",
    "print(f\"heights shape: {heights_loaded.shape}\")\n",
    "print(f\"cost_labels shape: {cost_labels_loaded.shape}\")\n",
    "print(f\"raw_costs shape: {raw_costs_loaded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [2., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [3., 1., 0.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4,  4,  5,  5,  6,  6,  7,  8,  8,  9, 10, 10, 11, 11,\n",
       "           9,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61,  1,  2,  3,  3,  4,  4,  5,  5,  6,  7,  7,  8,  9,  9, 10, 10,\n",
       "           8,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61,  1,  2,  2,  3,  3,  4,  4,  5,  6,  6,  7,  8,  8,  9,  9,\n",
       "           7,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61,  1,  2,  2,  3,  3,  4,  5,  5,  6,  7,  7,  8,  8,\n",
       "           6,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  4,  4,  5,  6,  6,  7,  7,\n",
       "           5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  3,  3,  4,  5,  5,  6,  6,\n",
       "           4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  4,  4,  5,  5,\n",
       "           3, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  3,  3,  4,  4,\n",
       "           2, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  3,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,\n",
       "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
       "           1, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
       "          61,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
       "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_bias_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 11, 10,  2,  9,  2,  8,  2,  7,  6,  2,  2,  5,  4,  2,  2,  3,  2,\n",
       "          2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([66844.], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_costs_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9965], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_labels_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([1, 30, 1161])\n",
      "rel_pos shape: torch.Size([1, 30, 30])\n",
      "attn_bias shape: torch.Size([1, 31, 31])\n",
      "heights shape: torch.Size([1, 30])\n",
      "cost_labels shape: torch.Size([1])\n",
      "raw_costs shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes to verify\n",
    "print(f\"x shape: {x_loaded.shape}\")\n",
    "print(f\"rel_pos shape: {rel_pos_loaded.shape}\")\n",
    "print(f\"attn_bias shape: {attn_bias_loaded.shape}\")\n",
    "print(f\"heights shape: {heights_loaded.shape}\")\n",
    "print(f\"cost_labels shape: {cost_labels_loaded.shape}\")\n",
    "print(f\"raw_costs shape: {raw_costs_loaded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST - Loading 2 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 2\n",
      "First sample: ({'x': tensor([[[1., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [2., 0., 0.,  ..., 1., 1., 1.],\n",
      "         [3., 1., 0.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), 'attn_bias': tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf,\n",
      "          -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]]), 'rel_pos': tensor([[[ 1,  2,  3,  4,  4,  5,  5,  6,  6,  7,  8,  8,  9, 10, 10, 11, 11,\n",
      "           9,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61,  1,  2,  3,  3,  4,  4,  5,  5,  6,  7,  7,  8,  9,  9, 10, 10,\n",
      "           8,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61,  1,  2,  2,  3,  3,  4,  4,  5,  6,  6,  7,  8,  8,  9,  9,\n",
      "           7,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61,  1,  2,  2,  3,  3,  4,  5,  5,  6,  7,  7,  8,  8,\n",
      "           6,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  4,  4,  5,  6,  6,  7,  7,\n",
      "           5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  3,  3,  4,  5,  5,  6,  6,\n",
      "           4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  4,  4,  5,  5,\n",
      "           3, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  3,  3,  4,  4,\n",
      "           2, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,  3,  3,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61, 61, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,  2,  2,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1, 61,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,  1,\n",
      "          61, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "           1, 61,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61,\n",
      "          61,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]]), 'heights': tensor([[12, 11, 10,  2,  9,  2,  8,  2,  7,  6,  2,  2,  5,  4,  2,  2,  3,  2,\n",
      "          2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])}, (tensor([0.9965], dtype=torch.float64), tensor([66844.], dtype=torch.float64)))\n"
     ]
    }
   ],
   "source": [
    "# Load the saved tensor collection\n",
    "loaded_tensors1 = torch.load(\"./job_queries/tensors/query_1_2024-12-03-20.43.44.200877.pt\")\n",
    "\n",
    "# Access each tensor\n",
    "x_loaded1 = loaded_tensors1[\"x\"]\n",
    "rel_pos_loaded1 = loaded_tensors1[\"rel_pos\"]\n",
    "attn_bias_loaded1 = loaded_tensors1[\"attn_bias\"]\n",
    "heights_loaded1 = loaded_tensors1[\"heights\"]\n",
    "cost_labels_loaded1 = loaded_tensors1[\"cost_labels\"]\n",
    "raw_costs_loaded1 = loaded_tensors1[\"raw_costs\"]\n",
    "\n",
    "# Load the saved tensor collection\n",
    "loaded_tensors2 = torch.load(\"./job_queries/tensors/query_2_2024-12-03-20.43.45.954590.pt\")\n",
    "\n",
    "# Access each tensor\n",
    "x_loaded2 = loaded_tensors2[\"x\"]\n",
    "rel_pos_loaded2 = loaded_tensors2[\"rel_pos\"]\n",
    "attn_bias_loaded2 = loaded_tensors2[\"attn_bias\"]\n",
    "heights_loaded2 = loaded_tensors2[\"heights\"]\n",
    "cost_labels_loaded2 = loaded_tensors2[\"cost_labels\"]\n",
    "raw_costs_loaded2 = loaded_tensors2[\"raw_costs\"]\n",
    "\n",
    "x_list = [x_loaded1, x_loaded2]\n",
    "rel_pos_list = [rel_pos_loaded1, rel_pos_loaded2]\n",
    "attn_bias_list = [attn_bias_loaded1, attn_bias_loaded2]\n",
    "heights_list = [heights_loaded1, heights_loaded2]\n",
    "cost_labels_list = [cost_labels_loaded1, cost_labels_loaded2]\n",
    "raw_costs_list = [raw_costs_loaded1, raw_costs_loaded2]\n",
    "\n",
    "dataset = PlanTreeDataset(\n",
    "    2, # number of training examples \n",
    "    x_list,\n",
    "    attn_bias_list,\n",
    "    rel_pos_list,\n",
    "    heights_list,\n",
    "    cost_labels_list,\n",
    "    raw_costs_list\n",
    ")\n",
    "\n",
    "print(\"Dataset length:\", len(dataset))\n",
    "print(\"First sample:\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2\n",
      "Sample contents:\n",
      "Feature Matrix (x): torch.Size([1, 30, 1161])\n",
      "Attention Bias (attn_bias): torch.Size([1, 31, 31])\n",
      "Relative Positions (rel_pos): torch.Size([1, 30, 30])\n",
      "Heights (heights): torch.Size([1, 30])\n",
      "Label: (tensor([0.9965], dtype=torch.float64), tensor([66844.], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "# Get the length of the dataset\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Access a single sample\n",
    "sample, label = dataset[0]\n",
    "\n",
    "# Print the sample contents\n",
    "print(\"Sample contents:\")\n",
    "print(\"Feature Matrix (x):\", sample['x'].shape)\n",
    "print(\"Attention Bias (attn_bias):\", sample['attn_bias'].shape)\n",
    "print(\"Relative Positions (rel_pos):\", sample['rel_pos'].shape)\n",
    "print(\"Heights (heights):\", sample['heights'].shape)\n",
    "print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.5416, 0.5031], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "Epoch: 0  Avg Loss: 0.11264064908027649, Time: 1.23982834815979\n",
      "Median: 3.8878654855117554\n",
      "Mean: 3.8878654855117554\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9912, 0.9917], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9968, 0.9966], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9983, 0.9984], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9989, 0.9989], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9991, 0.9992], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9993, 0.9993], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9993, 0.9994], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9993, 0.9993], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9992, 0.9992], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9991, 0.9991], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9988, 0.9990], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9985, 0.9982], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9978, 0.9975], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9960, 0.9955], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9950, 0.9943], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9962, 0.9965], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9965, 0.9972], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9976, 0.9972], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9973, 0.9973], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9979, 0.9975], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "Epoch: 20  Avg Loss: 6.272103405535745e-07, Time: 22.80106782913208\n",
      "Median: 1.003117167840816\n",
      "Mean: 1.003117167840816\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9979, 0.9970], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9972, 0.9980], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9969, 0.9971], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9972, 0.9960], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9965, 0.9961], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9964, 0.9963], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9962, 0.9956], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9957, 0.9969], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9959, 0.9957], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9969, 0.9965], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9968, 0.9967], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9969, 0.9967], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9967, 0.9970], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9973, 0.9972], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9965, 0.9970], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9975, 0.9971], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9964, 0.9969], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9963, 0.9966], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9973, 0.9965], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9955, 0.9970], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "Epoch: 40  Avg Loss: 3.1071027706275345e-07, Time: 39.20470309257507\n",
      "Median: 1.0020323906840714\n",
      "Mean: 1.0020323906840714\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9966, 0.9960], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "ps: [66848.28 66848.34]\n",
      "ls: [tensor([66844.], dtype=torch.float64), tensor([66860.], dtype=torch.float64)]\n",
      "len(ls): 2\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9964, 0.9970], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "ps: [66851.27 66851.27]\n",
      "ls: [tensor([66844.], dtype=torch.float64), tensor([66860.], dtype=torch.float64)]\n",
      "len(ls): 2\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9959, 0.9965], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/db2inst1/learnedwmp/templates/job-templatizing-queryformer-modified/model/trainer.py:46: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  corr, _ = pearsonr(np.log(ps), np.log(ls))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_output shape: torch.Size([2, 1])\n",
      "ps: [66869.125 66869.125]\n",
      "ls: [tensor([66844.], dtype=torch.float64), tensor([66860.], dtype=torch.float64)]\n",
      "len(ls): 2\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9965, 0.9966], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "ps: [66884.76 66884.76]\n",
      "ls: [tensor([66844.], dtype=torch.float64), tensor([66860.], dtype=torch.float64)]\n",
      "len(ls): 2\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9972, 0.9968], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "ps: [66887.945 66888.01 ]\n",
      "ls: [tensor([66844.], dtype=torch.float64), tensor([66860.], dtype=torch.float64)]\n",
      "len(ls): 2\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9964, 0.9960], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "ps: [66903.96 66903.96]\n",
      "ls: [tensor([66844.], dtype=torch.float64), tensor([66860.], dtype=torch.float64)]\n",
      "len(ls): 2\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9977, 0.9973], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9965, 0.9966])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "ps: [66898.59 66898.59]\n",
      "ls: [tensor([66844.], dtype=torch.float64), tensor([66860.], dtype=torch.float64)]\n",
      "len(ls): 2\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9969, 0.9966], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "ps: [66890.18 66890.24]\n",
      "ls: [tensor([66844.], dtype=torch.float64), tensor([66860.], dtype=torch.float64)]\n",
      "len(ls): 2\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "Cost Predictions Shape: torch.Size([2])\n",
      "Batch Cost Label Shape: torch.Size([2])\n",
      "cost_preds: tensor([0.9973, 0.9967], grad_fn=<SqueezeBackward1>)\n",
      "batch_cost_label: tensor([0.9966, 0.9965])\n",
      "pred_output shape: torch.Size([2, 1])\n",
      "ps: [66871.555 66871.555]\n",
      "ls: [tensor([66844.], dtype=torch.float64), tensor([66860.], dtype=torch.float64)]\n",
      "len(ls): 2\n"
     ]
    }
   ],
   "source": [
    "# Example numpy label\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import importlib\n",
    "\n",
    "from model import trainer\n",
    "importlib.reload(trainer)\n",
    "from  model.trainer import train_single, train\n",
    "\n",
    "\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "# Train the model with the numpy label\n",
    "# trained_model = train_single(model, dataset, dataset, crit, cost_norm, args)\n",
    "model, best_model_path, train_embeddings, val_embeddings = train(model, dataset, dataset, crit, cost_norm, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training embeddings (best epoch): 2\n",
      "First training embedding shape: (1417,)\n",
      "Number of validation embeddings (best epoch): 2\n",
      "First validation embedding shape: (1417,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training embeddings (best epoch): {len(train_embeddings)}\")\n",
    "print(f\"First training embedding shape: {train_embeddings[0].shape}\")\n",
    "print(f\"Number of validation embeddings (best epoch): {len(val_embeddings)}\")\n",
    "print(f\"First validation embedding shape: {val_embeddings[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First validation embedding shape: [-1.031179   6.3488293  9.419984  ... 14.152625  -3.491641   3.5418007]\n"
     ]
    }
   ],
   "source": [
    "print(f\"First validation embedding shape: {val_embeddings[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First validation embedding shape: [-1.0309108  6.349203   9.420137  ... 14.152448  -3.492036   3.541753 ]\n"
     ]
    }
   ],
   "source": [
    "print(f\"First validation embedding shape: {val_embeddings[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Embedding 0:\n",
      "[-1.03117895e+00  6.34882927e+00  9.41998386e+00 -3.90928221e+00  1.19157000e+01 -4.41831064e+00 -4.93687630e+00  7.61633444e+00  5.80436993e+00  8.62499714e+00 -6.53570175e+00 -1.01669443e+00 -7.70654678e+00  1.25583693e-01 -1.79030740e+00 -3.69698000e+00  3.85393143e+00 -3.38230073e-01 -7.21047544e+00 -3.81462550e+00  7.42372894e+00  1.04346838e+01 -6.55981588e+00 -7.76527071e+00  7.35305548e+00 -9.54584503e+00 -3.08760428e+00  2.51115704e+00  5.99572754e+00 -5.49904776e+00  2.72151899e+00  5.53854084e+00 -4.44113255e+00  8.77591419e+00  5.81835938e+00  5.76462507e+00  6.33167505e+00  9.28437948e-01  1.02848399e+00  2.37525880e-01 -4.31308460e+00 -1.18324304e+00  9.76466560e+00  4.99428988e+00 -6.50026751e+00 -1.14985061e+00  4.78108168e+00  8.52582741e+00 -2.12309241e+00 -2.19312608e-01  3.42280817e+00 -1.48343205e+00  2.83918977e+00 -4.97438335e+00 -5.27138281e+00 -6.36840343e+00  2.01121664e+00 -4.98556566e+00  7.60432899e-01 -6.79205561e+00  1.36579502e+00  1.92791617e+00\n",
      " -4.09803820e+00  5.01127243e+00 -1.00444257e-02  5.98586845e+00  8.19968987e+00  1.41487608e+01  1.45276976e+00  6.93904209e+00  1.28503299e+00  8.21361732e+00 -4.83020878e+00 -4.90867168e-01  7.12667227e+00  6.75485754e+00 -3.34911251e+00 -8.19584274e+00 -1.10194063e+01  1.24850521e+01  4.70167923e+00 -9.16348648e+00 -1.43424377e+01  7.57669640e+00  1.31716204e+00  1.88717735e+00 -5.09582329e+00  1.36654959e+01  6.44681978e+00 -4.78490543e+00  1.51013777e-01 -3.38208288e-01 -9.13832378e+00 -3.01421881e-02 -1.08298359e+01 -8.02693748e+00 -8.13884854e-01 -7.27361250e+00 -5.02705956e+00  6.04836416e+00  5.79512215e+00 -4.50519845e-02 -7.94628024e-01  4.61958218e+00 -1.37120886e+01 -2.71981072e+00 -1.40668411e+01  5.90566921e+00 -4.89240438e-01  6.49555826e+00 -1.03729630e+01 -2.76135111e+00 -2.47083306e+00  9.16737175e+00  7.05081797e+00  8.34339708e-02 -7.35256910e+00  9.42540264e+00 -6.17928886e+00  6.46038771e+00 -6.68546772e+00 -6.24464178e+00 -4.41748095e+00  1.16539788e+00\n",
      "  5.21697140e+00 -2.51285225e-01  8.27881718e+00 -4.63690042e+00  1.12515104e+00  7.02878714e+00  2.38660359e+00 -2.95052075e+00 -5.83821154e+00 -4.23196030e+00 -2.38895798e+00  1.15042009e+01  8.68738270e+00  1.14410770e+00 -3.65949893e+00 -5.69184303e+00 -8.03316021e+00  8.35082817e+00  1.21363699e+00  8.44948673e+00 -5.65100908e-01 -7.70726776e+00 -6.27758789e+00  9.08779526e+00  1.01067519e+00  4.18382645e+00  1.41713285e+01 -6.34739828e+00 -1.09080553e+01 -6.66737413e+00  9.37911451e-01  9.45827770e+00  6.69007397e+00  5.23435926e+00 -8.71350956e+00  1.23301334e+01  4.98010874e+00 -3.95623899e+00  1.08455586e+00  9.76650524e+00 -3.03448725e+00  3.96222377e+00 -2.78011799e+00  4.30715036e+00  9.03866100e+00  9.95101166e+00  4.69356203e+00 -8.40833783e-02 -1.18382931e+01 -1.04910576e+00 -8.11187625e-01 -2.14471412e+00 -8.47514248e+00 -6.73819208e+00 -2.79229188e+00  6.20863056e+00  2.88526773e+00 -3.55864096e+00 -3.79137683e+00  8.65139294e+00  7.68241453e+00 -9.83526707e+00\n",
      "  1.41199076e+00 -8.19188881e+00  3.28229833e+00 -6.92134428e+00 -3.96994877e+00  4.17870522e+00  1.76403618e+00 -4.59405851e+00  4.56365156e+00 -4.70046902e+00  5.53550100e+00  9.20791745e-01 -5.85168600e+00  1.08743649e+01  5.86033106e+00  4.22654676e+00  7.11280251e+00  3.36815500e+00  1.53036308e+00  9.03748512e+00  4.31996822e+00  5.58333683e+00 -3.67970562e+00  1.48073363e+00 -5.35314894e+00  2.05284023e+00  8.11017990e+00 -3.77445865e+00 -1.36436443e+01  7.99412346e+00 -6.33746958e+00 -1.25372610e+01 -1.10872984e+01  6.52474737e+00 -8.48116302e+00 -2.09745193e+00  1.16479206e+01 -5.48924112e+00 -1.86326849e+00 -7.18318367e+00  1.99008942e+00  2.26559567e+00  2.47732329e+00  1.03288393e+01 -1.07572186e+00  2.09011769e+00  5.82231569e+00 -6.95679283e+00  6.58202696e+00  6.15756798e+00 -2.09558868e+00  3.07396483e+00 -7.03897142e+00 -7.61871910e+00 -1.50759954e+01 -2.92947221e+00 -9.28392410e+00 -1.13507538e+01 -7.27604771e+00 -4.00950480e+00  5.79215145e+00  5.15515208e-01\n",
      " -1.08639631e+01  6.75037575e+00  1.00360098e+01 -1.43957958e+01  1.01578236e-03 -6.96745300e+00  6.55296803e-01  9.25948524e+00  6.26485634e+00 -1.62162137e+00  5.23615599e+00  5.28814173e+00  8.83284748e-01  7.98667717e+00  1.72141516e+00 -8.91415405e+00 -3.77530903e-01  9.59828091e+00 -6.73307514e+00 -2.22789258e-01 -2.52438450e+00 -4.02417946e+00  2.53521419e+00  2.20165062e+00 -5.02826095e-01  3.36028290e+00  6.58388615e+00  7.35899878e+00  8.52253437e+00  1.89098310e+00  6.66635656e+00  1.79670358e+00  5.98565936e-01 -1.00752583e+01 -8.33450854e-01 -1.02317543e+01 -1.68778439e+01 -7.42105424e-01 -3.74185252e+00 -4.95752668e+00 -6.47826481e+00 -1.18566780e+01  6.59857464e+00 -2.06414819e+00  8.07405663e+00 -7.73970270e+00  5.86816549e+00 -8.06814957e+00  6.56806993e+00 -3.37964296e+00 -2.73583412e+00 -7.25466299e+00 -3.32668734e+00 -1.02012835e+01 -8.56006050e+00 -6.94617605e+00 -1.96450603e+00 -6.74570465e+00  2.12360692e+00 -4.68931389e+00  4.93095875e+00  5.75498867e+00\n",
      " -3.93541050e+00  1.19845610e+01 -1.07408972e+01  6.04780960e+00  3.68208671e+00 -6.33439541e+00  1.18658285e+01 -5.08896685e+00 -6.59249640e+00 -2.23995423e+00  8.38877773e+00 -3.52935529e+00  2.79352856e+00 -8.57930279e+00  4.04974222e+00 -3.02545571e+00  1.08170633e+01 -1.22017365e+01 -5.88545370e+00  1.16450596e+00 -4.17956161e+00 -7.88814878e+00 -9.07074070e+00  1.05280123e+01 -3.40866852e+00 -9.76899338e+00 -9.30390453e+00  9.60370541e+00  4.25746250e+00  2.43339968e+00 -8.48002338e+00 -1.97493315e+00 -4.18549109e+00 -5.28878403e+00  6.79731655e+00  1.03296623e+01  1.47684603e+01  4.92512369e+00 -1.14216404e+01  4.85486078e+00  9.80912209e+00 -8.71251392e+00 -2.52379799e+00  3.84879947e+00  1.07091789e+01  1.07748365e+01 -5.64438248e+00  5.44540644e+00  7.88652325e+00  4.18856716e+00 -1.10572014e+01  9.11660480e+00 -5.17825174e+00 -6.62035990e+00 -1.07984400e+01 -4.53355885e+00  6.85252237e+00  5.42862654e+00  9.63630676e+00 -8.67400932e+00  3.85957098e+00  4.84419775e+00\n",
      " -3.57875198e-01 -1.80422783e+00 -1.02117310e+01  2.21503043e+00 -2.75731635e+00 -1.06111717e+01 -2.15611286e+01  4.81101656e+00  5.49384880e+00  9.09283543e+00  9.91324806e+00 -2.78953934e+00  3.75487447e+00 -7.62377214e+00  2.53626561e+00  1.24787827e+01  7.40955877e+00 -1.43304644e+01  6.74111032e+00  6.56083536e+00  1.19373150e+01 -7.33651936e-01  9.35687828e+00  8.79015732e+00  7.84555531e+00 -7.38604724e-01  1.31819963e+00 -5.76595497e+00  5.65740585e+00 -3.56136465e+00  6.64535522e+00 -2.34000063e+00 -8.16268158e+00 -5.29204273e+00  6.15289307e+00 -3.27062535e+00  9.53399372e+00 -6.13499546e+00  1.37782288e+00 -1.75845850e+00 -1.85409164e+00 -3.25350022e+00 -5.73101473e+00 -1.02873268e+01  3.90672231e+00  8.43819427e+00 -3.59552264e+00  1.55509293e+00 -8.04871368e+00  5.79709816e+00  1.89351130e+00  4.89624262e+00  9.80526257e+00 -1.25126190e+01 -4.31483793e+00  8.93750000e+00 -4.42586327e+00 -5.87874031e+00  5.68691540e+00 -3.52590585e+00 -6.28084517e+00  3.61872673e+00\n",
      " -5.70513248e+00  1.11571312e+01 -2.54872084e+00  5.26997423e+00  1.23681974e+01 -3.48870277e+00  9.48285460e-01  3.69933558e+00 -4.59369707e+00  8.00154972e+00 -1.94663799e+00 -1.09925613e+01 -6.60123396e+00 -1.31184978e+01  4.10392332e+00 -2.27987099e+00 -4.09104061e+00 -8.41473484e+00  3.30834389e+00 -7.81304836e-01  5.98843908e+00 -4.62647867e+00  1.20487728e+01  6.14433146e+00  4.24923658e+00 -6.51352787e+00 -1.33752251e+00 -4.50148010e+00  9.01025105e+00 -8.33113766e+00  3.62249470e+00  1.98256493e+00 -4.68078279e+00  1.47652617e+01  9.10926056e+00  6.58351958e-01 -4.68202019e+00  5.64554024e+00  7.32337761e+00 -8.17629528e+00  6.54401541e+00  8.85851026e-01 -6.72314501e+00 -1.09969969e+01 -1.44938354e+01 -1.01538734e+01  1.08434649e+01 -4.10165215e+00  5.80105114e+00  8.96239471e+00 -9.08985424e+00  4.22735214e+00  1.68957889e+00 -8.18095493e+00  7.93177891e+00  3.75855923e+00 -4.55459690e+00 -1.23213172e-01  4.40116704e-01  3.56498265e+00 -3.81785727e+00  6.75083447e+00\n",
      "  1.62995338e+00  1.83571873e+01  2.21558189e+00 -1.90781832e+00  6.53080797e+00 -8.15248013e+00  6.75680113e+00  9.72766972e+00 -6.81230903e-01 -8.36130428e+00  7.08286428e+00 -4.59562683e+00 -5.40400505e-01  9.75198650e+00  5.84811115e+00  6.18927860e+00  1.13702469e+01  6.32400370e+00 -1.45438862e+01  2.72866631e+00 -8.02416611e+00  6.09854507e+00  7.91501188e+00  6.43203640e+00 -6.96010113e+00  1.26689005e+00  4.21257353e+00 -9.59931755e+00 -9.09736252e+00 -2.96889043e+00 -9.72142124e+00  3.15977836e+00  1.33219013e+01 -1.00561798e+00 -7.31485319e+00  1.26803102e+01  6.73613453e+00 -2.17187047e+00 -5.72423553e+00 -4.96197653e+00  7.58811712e-01 -2.61892056e+00 -4.83659744e+00 -8.50021267e+00  1.01693325e+01  4.85214901e+00  4.72942877e+00 -6.72871351e+00 -9.34765434e+00 -7.29489470e+00  4.58331394e+00 -1.08781624e+00 -5.80782413e-01  4.64061213e+00  3.28094292e+00  2.90681124e+00 -1.82406902e+00  3.49242020e+00  1.32255516e+01 -1.75445538e+01 -8.65463448e+00  1.33915081e+01\n",
      " -4.64107186e-01 -2.08807850e+00  2.32919073e+00 -2.59133101e+00  9.21474934e+00 -6.73090458e-01  4.93881941e+00 -5.42830372e+00  1.63185668e+00 -8.08950806e+00  5.89820194e+00  1.08236513e+01  1.10182428e+01  6.73983002e+00  9.68713188e+00  2.07712317e+00  9.38755322e+00 -5.39334249e+00  4.40096951e+00  7.01120520e+00 -6.12856150e+00  8.22865963e+00  4.19569016e+00  3.53647375e+00 -9.71680450e+00 -5.23585320e+00 -6.59139633e-01 -7.19945908e+00  3.41160506e-01  7.80367517e+00 -7.96943760e+00  8.99200535e+00  1.02393751e+01 -7.26324272e+00 -5.10059500e+00  3.90532804e+00  1.12880087e+01 -3.90360522e+00  1.47585833e+00 -6.61242914e+00 -9.23421860e+00  5.74868298e+00 -4.60285139e+00 -2.50478506e+00 -9.01036644e+00  1.15246258e+01  7.93367195e+00 -7.80424118e-01 -1.04771268e+00  7.91282845e+00 -5.62491083e+00  9.88863182e+00  6.67590523e+00  2.24692202e+00 -6.06297827e+00  3.35498643e+00 -2.95778275e+00 -2.17157769e+00  1.00168972e+01 -5.20899010e+00  6.63155556e+00  6.07035160e+00\n",
      "  1.20831060e+01 -1.34932528e+01 -7.79249191e+00 -7.85778332e+00 -9.14496231e+00 -3.65957880e+00  9.32726669e+00  1.01524515e+01  3.63310003e+00  7.90248728e+00  7.67476797e+00  1.00609951e+01 -4.05678511e+00  4.61933899e+00 -3.53958964e+00  2.55194330e+00  4.17506313e+00  6.55610085e+00  2.22749615e+00 -8.57025528e+00 -1.15060396e+01  1.04960890e+01 -1.17835636e+01 -7.50939846e+00 -8.58412623e-01  2.17249250e+00  1.02298203e+01  2.95772217e-02 -4.62612486e+00 -3.90377855e+00  1.10399067e+00 -1.17805882e+01  1.11554918e+01  3.13510108e+00 -5.51279128e-01  8.17364216e+00  4.46743059e+00  7.69350433e+00  1.34166315e-01 -1.31457424e+00  7.96015024e+00  7.22244549e+00  6.45001030e+00  3.76548219e+00  4.12630463e+00  1.23998938e+01  9.23277664e+00 -4.35295820e+00  7.65727091e+00 -9.48617649e+00  3.70459914e+00 -4.76166582e+00  1.46246624e+00 -1.48414642e-01 -3.37650490e+00 -1.17647743e+00 -6.58013439e+00 -4.08793879e+00 -6.44259930e+00  5.79406691e+00  5.73095942e+00  9.66257954e+00\n",
      "  6.24913263e+00  1.66098535e+00  7.51104116e+00 -6.63484001e+00 -8.91398430e+00  2.72724104e+00 -9.78841782e+00  2.79986715e+00 -6.92537546e+00 -2.52656507e+00  4.67484617e+00 -8.49798584e+00 -5.12175465e+00 -8.99449158e+00 -7.50052929e+00 -3.53667521e+00 -5.94457245e+00  9.13332844e+00 -1.06166830e+01 -1.10737247e+01 -1.47082090e+00  8.45751286e+00 -5.91379595e+00  5.69719696e+00 -2.46791720e+00  1.27394066e+01  2.85700297e+00 -3.88810134e+00  1.89172506e+00  3.88639688e+00 -6.73847437e+00 -9.07938004e+00  7.76641035e+00  4.25287580e+00 -8.90913391e+00  9.30121136e+00  1.27227783e+00  8.56081390e+00 -1.99824893e+00 -1.09729471e+01  8.64146328e+00  6.34919357e+00 -3.81752968e-01 -5.06991529e+00  5.51321602e+00  5.89077568e+00  6.46198452e-01  9.51008606e+00 -3.62775111e+00  2.37173820e+00  7.81877565e+00  1.40075922e+00  9.29571724e+00  1.86916673e+00  4.91440630e+00 -8.93368626e+00 -6.63098907e+00 -3.43863416e+00 -4.78064442e+00  1.75332642e+00  1.47666769e+01  5.52785683e+00\n",
      "  1.35669155e+01  5.69671726e+00 -4.14627171e+00  6.39627266e+00  4.27777338e+00 -4.79864788e+00 -9.60740852e+00 -6.15907907e+00  6.12874699e+00  2.17981625e+00  4.81973410e-01  5.84480476e+00  2.15409708e+00  1.16101122e+00  8.09806252e+00 -5.61293983e+00  1.00987189e-01  6.25143003e+00  9.15326786e+00 -6.49496841e+00  2.51039410e+00 -4.79391623e+00  1.88206410e+00 -7.72121906e+00  2.93269324e+00 -8.82040501e-01  1.93818784e+00 -4.45309973e+00  7.24182844e+00  5.36843920e+00  1.23638868e+01  5.23354292e+00  1.35706015e+01  1.75881588e+00 -1.04604590e+00 -1.01354523e+01  4.58605671e+00  3.17556930e+00 -2.96268892e+00 -3.58597398e+00  7.49077415e+00 -3.31662369e+00  3.09622788e+00  3.12777901e+00  9.41804981e+00 -5.22178078e+00 -7.03666639e+00  7.97839642e+00  1.89253211e+00 -4.31653166e+00 -2.67963314e+00  4.83838463e+00  1.00811672e+01  5.12998295e+00  9.64537430e+00  8.34631538e+00 -9.90145779e+00  1.21965575e+00 -3.18541169e+00  7.57542753e+00 -6.85129690e+00  2.12544942e+00\n",
      "  6.17805338e+00 -3.54604292e+00 -7.00088322e-01  6.59453964e+00 -7.29985905e+00  7.80331087e+00 -6.89646435e+00  4.92680645e+00  9.04941654e+00 -6.49245310e+00 -5.23395586e+00  5.44347644e-01 -9.08577085e-01  3.91810608e+00 -6.31433344e+00  8.38027000e+00 -7.50633597e-01 -6.43992710e+00  6.70542669e+00 -1.19400387e+01  3.12582302e+00 -1.01457634e+01  9.75483513e+00  6.09721088e+00 -3.99666500e+00 -5.38453531e+00 -1.09273762e-01  1.91170633e-01  2.19820642e+00  8.99184608e+00  6.84648991e+00  6.80616617e+00 -8.40248108e+00  1.88099891e-01 -2.56956840e+00  2.12340093e+00 -6.57158756e+00  2.67424107e+00 -8.65900612e+00  5.52891350e+00  4.02160978e+00 -3.49949861e+00 -7.26856041e+00  4.27106714e+00 -5.64149332e+00  6.47399724e-01 -1.91406703e+00  1.91180551e+00  5.92167807e+00 -9.72779369e+00  1.87243462e+00 -5.96129942e+00  1.32557392e+01  4.86883593e+00  5.37710190e+00 -3.55520678e+00  1.23274536e+01  3.33391368e-01 -6.77779102e+00 -7.63024282e+00 -7.86902380e+00 -2.66241741e+00\n",
      " -9.42637253e+00  3.19455767e+00  1.19522133e+01 -8.58034706e+00  5.04467583e+00  4.27323723e+00  2.27539206e+00 -4.37111568e+00 -5.67565346e+00  4.95282650e+00  1.12105141e+01 -7.67751598e+00 -5.05437756e+00  3.40646648e+00 -1.11809993e+00  1.22488470e+01  3.82991934e+00  1.06225863e+01 -6.01330948e+00 -5.68174028e+00 -5.31662583e-01 -2.48579755e-01  4.52739811e+00  4.61322069e+00  1.12885790e+01 -1.47771168e+00  1.06439435e+00 -5.09436512e+00 -4.20635271e+00 -9.84140575e-01  3.29765081e+00  4.66821074e-01  2.66861409e-01  4.65300131e+00  9.07073116e+00 -1.13915625e+01  1.96302402e+00 -5.29354048e+00 -1.15860716e-01  6.34758997e+00  3.36096025e+00 -3.04766107e+00 -4.82540989e+00 -9.52495766e+00 -1.84802723e+00 -8.07558346e+00 -2.23430133e+00 -4.17581892e+00 -2.32843447e+00 -7.53244734e+00 -1.39888787e+00  2.67819810e+00  1.25579345e+00 -5.62513113e+00 -2.45479178e+00 -1.61966431e+00  5.95802307e+00  2.21946120e+00 -1.61859202e+00 -4.31591415e+00  8.79378223e+00  9.77781677e+00\n",
      " -1.86179352e+00  2.40081286e+00 -9.05512238e+00  3.64684033e+00  8.11300564e+00 -2.83142090e+00  1.00670805e+01  8.15749264e+00  1.12359180e+01  6.22037315e+00  2.82589054e+00  6.10472631e+00  2.60564828e+00  9.33524847e-01 -7.82294130e+00 -1.24408798e+01 -5.41601133e+00  7.29808521e+00 -2.23246098e+00 -9.38109493e+00  4.65802073e-01  5.95255518e+00  1.02518597e+01  7.81800127e+00  4.04813957e+00  5.59577179e+00 -8.91047096e+00 -4.78873587e+00 -5.66523218e+00 -7.87665009e-01 -1.22024345e+00 -1.64049768e+00  5.51758432e+00 -1.98598421e+00  3.25464773e+00  1.31357956e+01 -4.25806224e-01 -6.49619818e+00 -9.65282059e+00 -1.49897361e+00 -5.91918766e-01  6.06545925e+00 -4.36693573e+00  3.00995827e+00 -6.30191040e+00 -3.66302729e-01 -4.28826904e+00 -8.13055706e+00 -5.17486095e+00 -2.96606839e-01  1.50245857e+00  1.10516138e+01  5.56577110e+00 -7.03975677e+00 -3.70280886e+00 -3.03706527e+00  4.14190412e-01  7.27520561e+00 -6.82465124e+00  6.90252829e+00 -3.27513766e+00 -7.05504036e+00\n",
      "  1.68597507e+00 -8.06781864e+00  4.98095655e+00 -5.22933102e+00  6.30445033e-02  8.12850475e+00 -3.66965604e+00  8.71589184e+00 -2.85489154e+00 -3.55844069e+00  5.29621220e+00 -5.95773506e+00  5.61308527e+00  6.87635899e-01 -2.18084502e+00 -1.05484114e+01 -6.12653828e+00  8.10329080e-01 -9.79368305e+00  2.65990758e+00 -7.33296299e+00 -9.75357914e+00  7.70674109e-01 -1.25369179e+00  4.07916021e+00 -8.28198624e+00 -8.53772163e+00 -7.32262802e+00 -6.80854130e+00  1.30169475e+00  7.52274752e+00  3.06416774e+00  6.04364824e+00 -5.00788832e+00  6.78186059e-01  8.07542324e+00  7.06337833e+00  5.28884268e+00  7.56853390e+00  8.97372627e+00  1.47673702e+00 -1.20602341e+01  6.40480328e+00  2.74710608e+00 -4.13298273e+00  2.91129851e+00 -1.09865990e+01  5.17444074e-01  4.04859352e+00 -1.47575736e+00 -5.90554118e-01 -3.27015668e-01 -5.24444246e+00 -1.76846623e-01  1.19836559e+01 -5.03676081e+00 -6.87843561e+00  3.07133150e+00  5.81549823e-01 -3.99351358e+00 -5.42410851e-01  3.69724298e+00\n",
      "  8.74775410e+00 -7.16183376e+00 -6.98501921e+00  1.27258940e+01  9.59574318e+00  3.79445171e+00  9.47266388e+00  7.36678982e+00 -5.47567940e+00  8.73185062e+00  5.16037494e-02 -1.51068556e+00  1.01773901e+01  2.33440375e+00 -6.01938152e+00  7.86596537e+00  3.73005390e-01 -1.56921899e+00  1.76066380e+01  7.46392190e-01  3.75491428e+00 -3.38285995e+00 -6.34821224e+00  8.82574749e+00  4.33326435e+00  8.76277733e+00 -8.36586952e+00  6.74444866e+00 -8.89892864e+00  9.40841794e-01  5.91625690e+00  4.84908295e+00 -7.24428225e+00 -3.84461689e+00 -1.16405363e+01  9.72668171e-01  7.06442165e+00  2.59843230e+00  6.02218270e-01  6.30197716e+00 -2.29061508e+00 -2.13999629e+00  9.06494808e+00  8.31126213e+00 -3.34095192e+00 -2.36006522e+00  3.20038152e+00 -2.79210162e+00 -8.78608418e+00  3.67757487e+00  6.14827251e+00  6.17053223e+00 -1.07814236e+01  8.04069710e+00  2.79266000e+00 -3.40864515e+00 -4.31810284e+00 -1.49886990e+00  8.10921788e-01 -3.03841662e+00 -4.48082447e+00  5.67376614e-04\n",
      " -9.70647812e+00 -3.48923683e+00  5.19808006e+00 -1.11281071e+01  3.99283504e+00 -9.87593079e+00 -3.80830145e+00 -3.04238707e-01 -5.42059278e+00 -7.44690514e+00  9.65367794e+00  5.30331421e+00  8.51460648e+00  3.30586016e-01 -1.13466203e+00 -5.76105976e+00 -5.98925829e+00 -5.96562910e+00 -9.26857853e+00  5.71133852e+00  4.24173927e+00 -7.86358976e+00 -9.80736637e+00 -4.47012377e+00  1.42354488e+00  8.77705193e+00 -6.50586748e+00 -1.78704238e+00  7.27129030e+00  5.66903448e+00 -5.67338848e+00 -5.18283939e+00 -3.22349143e+00 -3.22959757e+00  7.16778994e+00  3.50381565e+00 -8.24779224e+00  3.08140588e+00 -1.81504893e+00 -5.56737757e+00  6.28820276e+00  8.69097424e+00 -7.33882618e+00 -3.33038878e+00 -5.66271019e+00 -2.44658327e+00 -7.81337404e+00 -9.66198981e-01  5.21371794e+00 -4.25330591e+00 -4.64149904e+00  8.15089321e+00 -1.53956485e+00 -1.91420305e+00  6.44783735e+00 -2.41915226e+00  4.37195873e+00  1.20053663e+01 -4.00146675e+00 -2.12859154e+00  7.27897358e+00  5.88311291e+00\n",
      " -6.18261051e+00  4.18510771e+00 -4.48243856e+00  3.51089215e+00  7.44444323e+00 -9.33971882e+00 -2.49419856e+00 -6.52411127e+00 -1.01582127e+01  2.02787781e+00  3.94204712e+00 -6.33592558e+00 -1.07122326e+01 -6.13632083e-01 -5.01395941e+00 -5.12266016e+00 -3.58092928e+00 -4.85737896e+00 -3.09075773e-01 -6.42612982e+00 -9.94293118e+00  6.63157225e+00  1.09616196e+00 -5.25157166e+00  1.88260555e+00 -3.72264791e+00  2.90956450e+00  6.64796638e+00 -5.44975615e+00 -8.43018293e-01 -2.81458569e+00  3.31252098e+00  4.99352407e+00  2.85960126e+00 -3.63690734e+00 -1.02198191e+01  1.05249786e+01 -3.40205836e+00 -9.38081932e+00 -1.89471710e+00  6.30691385e+00 -7.29286337e+00 -6.12104130e+00  3.83655167e+00 -2.50914407e+00 -2.15326160e-01  9.10223424e-01  1.38008184e+01 -1.75002918e+01  5.78078222e+00  4.52705622e+00  1.64877071e+01  9.37374020e+00 -2.24313831e+00 -2.65590692e+00  5.59634686e+00  4.77862167e+00 -5.81998682e+00 -2.65248203e+00  8.90730953e+00 -4.98048306e+00  1.83452189e+00\n",
      " -9.58272815e-02  1.09643674e+00 -1.19059162e+01  4.35115004e+00  8.16697025e+00  6.78399277e+00  3.12234569e+00  1.25024099e+01 -3.92752028e+00 -3.76583505e+00 -4.65153980e+00  8.43847847e+00  6.86992121e+00  8.31578445e+00 -1.01993351e+01  1.53655386e+00 -8.62857461e-01 -4.19436598e+00 -6.35721064e+00  9.76828766e+00 -3.52430344e+00 -3.10801387e+00 -6.00579977e+00 -1.32631922e+00  7.36045074e+00 -5.47405958e+00  6.54293251e+00 -6.45240164e+00  7.27757454e+00  4.89893150e+00 -6.71394444e+00 -3.38201451e+00  4.87968731e+00 -1.65350246e+00  3.47155666e+00 -3.47595334e-01  8.07674503e+00 -3.67292809e+00  2.29994178e+00 -6.18867695e-01 -9.48787880e+00  1.66064799e+00 -9.70335674e+00 -9.53087616e+00  2.37166691e+00  5.60701656e+00 -7.97577953e+00 -3.88868308e+00 -4.45428467e+00 -8.60595322e+00 -5.03538227e+00 -3.55739999e+00  7.84887075e+00 -7.50715637e+00  6.77875805e+00  5.42010069e+00  1.47932351e+00  3.81482291e+00  4.18693781e+00 -6.68947077e+00  4.97312355e+00 -4.67024755e+00\n",
      "  5.97823095e+00  1.06015420e+00  1.28857727e+01 -6.16318989e+00 -1.23202586e+00 -6.16144276e+00  8.22373867e+00  5.29784143e-01  6.41539526e+00 -6.59037018e+00 -1.06449957e+01 -3.86591697e+00  5.51224995e+00 -1.11293516e+01 -1.04995365e+01  8.00075722e+00  4.85664034e+00 -3.86186075e+00  1.11040106e+01 -9.01620770e+00  6.32901859e+00 -2.17306662e+00  5.86598992e-01 -2.69093704e+00 -4.89083290e+00 -7.74082947e+00  6.12457895e+00 -2.51074076e-01 -6.04223299e+00 -7.98073387e+00 -2.23326397e+00 -1.68326390e+00  2.57689476e+00  5.96895552e+00  1.05319099e+01 -6.68159866e+00 -4.50457668e+00  5.23656368e+00  4.43580031e-01  7.61389256e+00 -1.18046713e+00  1.13045464e+01 -9.19029415e-01 -3.32275581e+00  6.57658577e+00 -2.77424514e-01  4.10058308e+00  7.84141445e+00 -6.23144150e+00 -1.38372648e+00  2.24606943e+00  1.91229165e+00  7.60718441e+00 -9.49930549e-01 -5.55079269e+00  3.66472483e+00 -9.33508456e-01  2.21710467e+00  2.67003489e+00 -4.33719158e+00 -2.53759170e+00 -5.67637491e+00\n",
      " -5.96379280e+00 -5.61813593e+00 -7.62695694e+00  9.72131824e+00 -3.22022295e+00  6.87816906e+00  8.51271152e-02  2.58989429e+00 -2.54669619e+00  3.98661995e+00  1.04356871e+01  9.67673779e-01 -1.63725579e+00  6.80840969e+00 -8.36095333e+00  9.06401062e+00  5.40786326e-01 -9.67641830e-01  4.13719654e+00  4.53556061e+00  2.95167208e-01  1.80874276e+00  2.10750028e-01 -6.21604586e+00 -1.70718402e-01  5.68143654e+00  3.81232113e-01 -4.04904509e+00  1.90181026e+01 -8.92697906e+00 -2.17365646e+00  2.09592915e+00  2.39046663e-01 -4.60819197e+00 -3.41095448e-01  5.06678671e-02 -9.36458015e+00  8.19857311e+00 -5.24605942e+00  1.76449001e+00  2.43944979e+00  2.40884495e+00  1.56656146e+00  6.27427721e+00  1.00983801e+01 -3.77285886e+00  6.76229858e+00  4.88421631e+00 -3.36842465e+00 -9.91624355e+00  1.41526251e+01 -3.49164104e+00  3.54180074e+00]\n",
      "Full Embedding 1:\n",
      "[-1.03091085e+00  6.34920311e+00  9.42013741e+00 -3.90948701e+00  1.19156885e+01 -4.41817713e+00 -4.93683910e+00  7.61588383e+00  5.80425072e+00  8.62511730e+00 -6.53570795e+00 -1.01694643e+00 -7.70615339e+00  1.25909552e-01 -1.79067802e+00 -3.69667125e+00  3.85425758e+00 -3.38214219e-01 -7.21030855e+00 -3.81471515e+00  7.42329121e+00  1.04346514e+01 -6.55958319e+00 -7.76544380e+00  7.35324669e+00 -9.54504204e+00 -3.08733630e+00  2.51102066e+00  5.99584198e+00 -5.49875307e+00  2.72141695e+00  5.53807068e+00 -4.44123220e+00  8.77570534e+00  5.81870937e+00  5.76472759e+00  6.33151674e+00  9.28648293e-01  1.02800572e+00  2.37345502e-01 -4.31374884e+00 -1.18312860e+00  9.76449680e+00  4.99424076e+00 -6.50026226e+00 -1.14991271e+00  4.78132963e+00  8.52534771e+00 -2.12310696e+00 -2.19012320e-01  3.42275691e+00 -1.48297369e+00  2.83922052e+00 -4.97445107e+00 -5.27142906e+00 -6.36868715e+00  2.01113224e+00 -4.98558807e+00  7.60537207e-01 -6.79218483e+00  1.36593282e+00  1.92792690e+00\n",
      " -4.09865713e+00  5.01145172e+00 -9.59557295e-03  5.98563051e+00  8.19948387e+00  1.41483908e+01  1.45297086e+00  6.93903971e+00  1.28470361e+00  8.21381378e+00 -4.82978630e+00 -4.90683943e-01  7.12669134e+00  6.75471830e+00 -3.34890747e+00 -8.19583321e+00 -1.10192585e+01  1.24846668e+01  4.70197392e+00 -9.16369915e+00 -1.43417606e+01  7.57629347e+00  1.31718385e+00  1.88728702e+00 -5.09592056e+00  1.36655607e+01  6.44693661e+00 -4.78499985e+00  1.51062101e-01 -3.38205844e-01 -9.13819504e+00 -3.02441269e-02 -1.08297873e+01 -8.02671528e+00 -8.13588381e-01 -7.27323103e+00 -5.02694178e+00  6.04767323e+00  5.79548693e+00 -4.55085188e-02 -7.94749022e-01  4.62023115e+00 -1.37121525e+01 -2.71953392e+00 -1.40666475e+01  5.90549707e+00 -4.89332944e-01  6.49551201e+00 -1.03731270e+01 -2.76146221e+00 -2.47096062e+00  9.16743374e+00  7.05105543e+00  8.34705383e-02 -7.35245848e+00  9.42523575e+00 -6.17894936e+00  6.46053696e+00 -6.68568468e+00 -6.24477482e+00 -4.41738844e+00  1.16596580e+00\n",
      "  5.21693707e+00 -2.51276374e-01  8.27823257e+00 -4.63646126e+00  1.12514305e+00  7.02830791e+00  2.38673186e+00 -2.95021105e+00 -5.83815193e+00 -4.23251152e+00 -2.38926291e+00  1.15042295e+01  8.68744469e+00  1.14416456e+00 -3.65937853e+00 -5.69177341e+00 -8.03320026e+00  8.35080719e+00  1.21317351e+00  8.44946766e+00 -5.65256715e-01 -7.70717239e+00 -6.27781963e+00  9.08754826e+00  1.01053405e+00  4.18400145e+00  1.41709890e+01 -6.34747744e+00 -1.09083796e+01 -6.66731596e+00  9.38232362e-01  9.45827007e+00  6.69025993e+00  5.23444986e+00 -8.71353149e+00  1.23301220e+01  4.97988510e+00 -3.95637751e+00  1.08453918e+00  9.76671886e+00 -3.03483510e+00  3.96208358e+00 -2.78026772e+00  4.30734539e+00  9.03841400e+00  9.95083714e+00  4.69365025e+00 -8.38527083e-02 -1.18383999e+01 -1.04920912e+00 -8.11424136e-01 -2.14470601e+00 -8.47503948e+00 -6.73781681e+00 -2.79215908e+00  6.20851183e+00  2.88491392e+00 -3.55892563e+00 -3.79119921e+00  8.65151119e+00  7.68208838e+00 -9.83530140e+00\n",
      "  1.41185856e+00 -8.19156933e+00  3.28232861e+00 -6.92152119e+00 -3.96980286e+00  4.17877436e+00  1.76388133e+00 -4.59377575e+00  4.56367493e+00 -4.70054245e+00  5.53551245e+00  9.20699000e-01 -5.85127735e+00  1.08740120e+01  5.86022711e+00  4.22685575e+00  7.11267662e+00  3.36863112e+00  1.53022397e+00  9.03723335e+00  4.31985855e+00  5.58277798e+00 -3.67911506e+00  1.48079967e+00 -5.35342169e+00  2.05231071e+00  8.11009789e+00 -3.77463007e+00 -1.36437540e+01  7.99389553e+00 -6.33751917e+00 -1.25374088e+01 -1.10876198e+01  6.52468824e+00 -8.48123074e+00 -2.09764791e+00  1.16473799e+01 -5.48929739e+00 -1.86345065e+00 -7.18283796e+00  1.98966491e+00  2.26578188e+00  2.47721791e+00  1.03287859e+01 -1.07550073e+00  2.08995152e+00  5.82266855e+00 -6.95685673e+00  6.58204746e+00  6.15775013e+00 -2.09555507e+00  3.07385993e+00 -7.03892326e+00 -7.61876249e+00 -1.50756931e+01 -2.92938852e+00 -9.28402042e+00 -1.13507919e+01 -7.27599001e+00 -4.00922251e+00  5.79235172e+00  5.15590906e-01\n",
      " -1.08642941e+01  6.75000095e+00  1.00362453e+01 -1.43955841e+01  1.09957159e-03 -6.96783066e+00  6.55674934e-01  9.25960445e+00  6.26527596e+00 -1.62147117e+00  5.23662901e+00  5.28781414e+00  8.83149326e-01  7.98615456e+00  1.72127247e+00 -8.91391182e+00 -3.77205640e-01  9.59832001e+00 -6.73291302e+00 -2.22581118e-01 -2.52456021e+00 -4.02396297e+00  2.53479338e+00  2.20153785e+00 -5.02124429e-01  3.35992503e+00  6.58421612e+00  7.35880613e+00  8.52281857e+00  1.89076662e+00  6.66683912e+00  1.79671192e+00  5.98459125e-01 -1.00751381e+01 -8.33148360e-01 -1.02318573e+01 -1.68775215e+01 -7.42116570e-01 -3.74187493e+00 -4.95718861e+00 -6.47826719e+00 -1.18567257e+01  6.59855223e+00 -2.06438088e+00  8.07416821e+00 -7.73969316e+00  5.86875677e+00 -8.06819630e+00  6.56777287e+00 -3.37970233e+00 -2.73586702e+00 -7.25478506e+00 -3.32695031e+00 -1.02010727e+01 -8.56033134e+00 -6.94580126e+00 -1.96438217e+00 -6.74534607e+00  2.12359238e+00 -4.68912697e+00  4.93119049e+00  5.75518799e+00\n",
      " -3.93559718e+00  1.19840031e+01 -1.07402544e+01  6.04767847e+00  3.68209934e+00 -6.33433867e+00  1.18660097e+01 -5.08878469e+00 -6.59277725e+00 -2.24011683e+00  8.38864899e+00 -3.52963686e+00  2.79400992e+00 -8.57924652e+00  4.04967594e+00 -3.02526093e+00  1.08171072e+01 -1.22015181e+01 -5.88561630e+00  1.16437006e+00 -4.17916441e+00 -7.88811398e+00 -9.07086849e+00  1.05278749e+01 -3.40839958e+00 -9.76883984e+00 -9.30407906e+00  9.60387230e+00  4.25727654e+00  2.43367004e+00 -8.47976112e+00 -1.97504556e+00 -4.18525362e+00 -5.28888798e+00  6.79738092e+00  1.03296509e+01  1.47682886e+01  4.92489052e+00 -1.14215097e+01  4.85461140e+00  9.80873108e+00 -8.71245766e+00 -2.52390146e+00  3.84913039e+00  1.07089348e+01  1.07749128e+01 -5.64405966e+00  5.44555426e+00  7.88655472e+00  4.18852472e+00 -1.10575790e+01  9.11654091e+00 -5.17825174e+00 -6.62035847e+00 -1.07986269e+01 -4.53291035e+00  6.85228682e+00  5.42860889e+00  9.63583946e+00 -8.67392254e+00  3.85994840e+00  4.84422779e+00\n",
      " -3.57795298e-01 -1.80376458e+00 -1.02117510e+01  2.21498346e+00 -2.75753975e+00 -1.06113091e+01 -2.15610218e+01  4.81080866e+00  5.49381447e+00  9.09283257e+00  9.91334820e+00 -2.78936172e+00  3.75518680e+00 -7.62341881e+00  2.53632665e+00  1.24788828e+01  7.40974188e+00 -1.43301735e+01  6.74095201e+00  6.56063557e+00  1.19372063e+01 -7.33252406e-01  9.35693169e+00  8.79014587e+00  7.84545708e+00 -7.38587916e-01  1.31788385e+00 -5.76564455e+00  5.65747118e+00 -3.56128359e+00  6.64497709e+00 -2.33989239e+00 -8.16306591e+00 -5.29195547e+00  6.15279341e+00 -3.27062988e+00  9.53393173e+00 -6.13502359e+00  1.37814391e+00 -1.75852716e+00 -1.85378623e+00 -3.25348973e+00 -5.73116112e+00 -1.02876501e+01  3.90683913e+00  8.43880749e+00 -3.59539843e+00  1.55518794e+00 -8.04873276e+00  5.79707718e+00  1.89319587e+00  4.89631176e+00  9.80531883e+00 -1.25127029e+01 -4.31489706e+00  8.93749142e+00 -4.42575264e+00 -5.87842035e+00  5.68705940e+00 -3.52581239e+00 -6.28094149e+00  3.61865234e+00\n",
      " -5.70513487e+00  1.11570110e+01 -2.54840350e+00  5.26987743e+00  1.23682108e+01 -3.48855805e+00  9.48399544e-01  3.69944668e+00 -4.59356451e+00  8.00180244e+00 -1.94692266e+00 -1.09925356e+01 -6.60120487e+00 -1.31185112e+01  4.10397911e+00 -2.27991867e+00 -4.09135914e+00 -8.41443920e+00  3.30849791e+00 -7.81164169e-01  5.98854923e+00 -4.62654829e+00  1.20488377e+01  6.14404488e+00  4.24921799e+00 -6.51382494e+00 -1.33740282e+00 -4.50116491e+00  9.01049042e+00 -8.33110046e+00  3.62244320e+00  1.98248172e+00 -4.68092918e+00  1.47652216e+01  9.10921478e+00  6.58338666e-01 -4.68157911e+00  5.64529085e+00  7.32367897e+00 -8.17644405e+00  6.54431820e+00  8.86194825e-01 -6.72291279e+00 -1.09967098e+01 -1.44940233e+01 -1.01536245e+01  1.08435307e+01 -4.10183001e+00  5.80157614e+00  8.96268654e+00 -9.08949757e+00  4.22749615e+00  1.68951190e+00 -8.18109322e+00  7.93181610e+00  3.75848794e+00 -4.55437708e+00 -1.23272687e-01  4.40449715e-01  3.56495571e+00 -3.81764507e+00  6.75088453e+00\n",
      "  1.63001430e+00  1.83567696e+01  2.21591282e+00 -1.90803361e+00  6.53093481e+00 -8.15244198e+00  6.75685310e+00  9.72759342e+00 -6.81278467e-01 -8.36096001e+00  7.08262157e+00 -4.59560490e+00 -5.40904105e-01  9.75194931e+00  5.84781456e+00  6.18924332e+00  1.13700447e+01  6.32367754e+00 -1.45435114e+01  2.72897863e+00 -8.02430153e+00  6.09820414e+00  7.91505861e+00  6.43181562e+00 -6.95983887e+00  1.26705146e+00  4.21245337e+00 -9.59952545e+00 -9.09713173e+00 -2.96896482e+00 -9.72133732e+00  3.15999484e+00  1.33218746e+01 -1.00544107e+00 -7.31487656e+00  1.26802340e+01  6.73605251e+00 -2.17164373e+00 -5.72457552e+00 -4.96204948e+00  7.58899331e-01 -2.61911321e+00 -4.83647251e+00 -8.50070477e+00  1.01696901e+01  4.85191870e+00  4.72930813e+00 -6.72895718e+00 -9.34747791e+00 -7.29478359e+00  4.58349037e+00 -1.08819175e+00 -5.80975533e-01  4.64074326e+00  3.28064537e+00  2.90657067e+00 -1.82421172e+00  3.49238086e+00  1.32249699e+01 -1.75444012e+01 -8.65451527e+00  1.33913155e+01\n",
      " -4.64005828e-01 -2.08818793e+00  2.32934999e+00 -2.59137774e+00  9.21463394e+00 -6.73582256e-01  4.93916941e+00 -5.42789841e+00  1.63155234e+00 -8.08969021e+00  5.89838219e+00  1.08234797e+01  1.10182171e+01  6.74034452e+00  9.68686390e+00  2.07722139e+00  9.38745308e+00 -5.39376450e+00  4.40123367e+00  7.01121855e+00 -6.12840700e+00  8.22892094e+00  4.19602346e+00  3.53676224e+00 -9.71724796e+00 -5.23586035e+00 -6.59521639e-01 -7.19953346e+00  3.40835661e-01  7.80348301e+00 -7.96916056e+00  8.99218750e+00  1.02393827e+01 -7.26317120e+00 -5.10060930e+00  3.90514851e+00  1.12881317e+01 -3.90367079e+00  1.47556758e+00 -6.61241770e+00 -9.23476028e+00  5.74846077e+00 -4.60290384e+00 -2.50467324e+00 -9.01066017e+00  1.15245361e+01  7.93369865e+00 -7.80544519e-01 -1.04747975e+00  7.91328526e+00 -5.62489223e+00  9.88895130e+00  6.67585754e+00  2.24717951e+00 -6.06316233e+00  3.35454345e+00 -2.95756626e+00 -2.17175174e+00  1.00171127e+01 -5.20948553e+00  6.63127089e+00  6.07031345e+00\n",
      "  1.20830812e+01 -1.34933424e+01 -7.79241323e+00 -7.85751057e+00 -9.14485168e+00 -3.65969324e+00  9.32733440e+00  1.01521769e+01  3.63292766e+00  7.90224743e+00  7.67505646e+00  1.00607548e+01 -4.05679989e+00  4.61927795e+00 -3.53981996e+00  2.55175519e+00  4.17508221e+00  6.55621147e+00  2.22770667e+00 -8.57066822e+00 -1.15061436e+01  1.04959126e+01 -1.17836180e+01 -7.50913143e+00 -8.58111739e-01  2.17239928e+00  1.02294321e+01  2.99136452e-02 -4.62578535e+00 -3.90382481e+00  1.10386944e+00 -1.17804575e+01  1.11557655e+01  3.13472652e+00 -5.50937176e-01  8.17364883e+00  4.46763039e+00  7.69329357e+00  1.33707047e-01 -1.31441784e+00  7.96010113e+00  7.22231722e+00  6.45022154e+00  3.76577687e+00  4.12656307e+00  1.23997869e+01  9.23282051e+00 -4.35331869e+00  7.65704823e+00 -9.48581600e+00  3.70471549e+00 -4.76175308e+00  1.46241570e+00 -1.48559391e-01 -3.37673712e+00 -1.17677331e+00 -6.58018589e+00 -4.08804655e+00 -6.44296360e+00  5.79348898e+00  5.73091269e+00  9.66286945e+00\n",
      "  6.24897718e+00  1.66094089e+00  7.51094723e+00 -6.63508224e+00 -8.91374874e+00  2.72707820e+00 -9.78833675e+00  2.79979563e+00 -6.92552471e+00 -2.52656913e+00  4.67478943e+00 -8.49775505e+00 -5.12197781e+00 -8.99459743e+00 -7.50075150e+00 -3.53651023e+00 -5.94447708e+00  9.13354778e+00 -1.06164694e+01 -1.10736923e+01 -1.47046006e+00  8.45775795e+00 -5.91381502e+00  5.69727612e+00 -2.46800303e+00  1.27393780e+01  2.85679770e+00 -3.88828349e+00  1.89172626e+00  3.88618374e+00 -6.73870564e+00 -9.07929420e+00  7.76649714e+00  4.25282955e+00 -8.90920925e+00  9.30155849e+00  1.27247000e+00  8.56067562e+00 -1.99818349e+00 -1.09724836e+01  8.64148521e+00  6.34928465e+00 -3.81834507e-01 -5.06969738e+00  5.51380062e+00  5.89058590e+00  6.45936131e-01  9.50956059e+00 -3.62757635e+00  2.37187219e+00  7.81856728e+00  1.40088439e+00  9.29614735e+00  1.86924696e+00  4.91434097e+00 -8.93366337e+00 -6.63101292e+00 -3.43890810e+00 -4.78054953e+00  1.75360274e+00  1.47662239e+01  5.52783251e+00\n",
      "  1.35668774e+01  5.69653082e+00 -4.14622307e+00  6.39629889e+00  4.27816200e+00 -4.79885149e+00 -9.60729599e+00 -6.15943527e+00  6.12855577e+00  2.18025589e+00  4.82242197e-01  5.84468222e+00  2.15348148e+00  1.16126716e+00  8.09798622e+00 -5.61311626e+00  1.01403967e-01  6.25154352e+00  9.15322399e+00 -6.49542189e+00  2.51053357e+00 -4.79369116e+00  1.88239908e+00 -7.72095060e+00  2.93248701e+00 -8.82525980e-01  1.93762660e+00 -4.45348167e+00  7.24233055e+00  5.36839008e+00  1.23638544e+01  5.23339319e+00  1.35706854e+01  1.75899160e+00 -1.04635596e+00 -1.01353302e+01  4.58626413e+00  3.17518854e+00 -2.96264076e+00 -3.58616972e+00  7.49022484e+00 -3.31667185e+00  3.09674215e+00  3.12738752e+00  9.41816330e+00 -5.22228193e+00 -7.03655624e+00  7.97848797e+00  1.89247262e+00 -4.31673193e+00 -2.67993259e+00  4.83832359e+00  1.00810823e+01  5.13029051e+00  9.64534283e+00  8.34677315e+00 -9.90148258e+00  1.21931231e+00 -3.18545771e+00  7.57530022e+00 -6.85149288e+00  2.12539697e+00\n",
      "  6.17772007e+00 -3.54569626e+00 -6.99885905e-01  6.59448767e+00 -7.30006266e+00  7.80324936e+00 -6.89620113e+00  4.92697334e+00  9.04964066e+00 -6.49241352e+00 -5.23430109e+00  5.44290304e-01 -9.08292949e-01  3.91799569e+00 -6.31451464e+00  8.38070297e+00 -7.50480235e-01 -6.43989754e+00  6.70570087e+00 -1.19397593e+01  3.12576652e+00 -1.01457510e+01  9.75485039e+00  6.09737921e+00 -3.99686503e+00 -5.38462019e+00 -1.09345198e-01  1.91442907e-01  2.19782639e+00  8.99119568e+00  6.84639168e+00  6.80601168e+00 -8.40264893e+00  1.88981444e-01 -2.56917524e+00  2.12338448e+00 -6.57139874e+00  2.67411423e+00 -8.65886307e+00  5.52919626e+00  4.02150440e+00 -3.49960518e+00 -7.26881504e+00  4.27075052e+00 -5.64195538e+00  6.47364497e-01 -1.91401184e+00  1.91166663e+00  5.92177105e+00 -9.72788429e+00  1.87253475e+00 -5.96164656e+00  1.32553625e+01  4.86889982e+00  5.37716484e+00 -3.55554175e+00  1.23271399e+01  3.32920998e-01 -6.77770519e+00 -7.63040161e+00 -7.86897945e+00 -2.66236973e+00\n",
      " -9.42636776e+00  3.19454479e+00  1.19519634e+01 -8.57994556e+00  5.04458475e+00  4.27329826e+00  2.27547169e+00 -4.37135553e+00 -5.67559910e+00  4.95292568e+00  1.12102623e+01 -7.67776251e+00 -5.05433130e+00  3.40620828e+00 -1.11796713e+00  1.22490778e+01  3.83035302e+00  1.06226969e+01 -6.01334429e+00 -5.68193054e+00 -5.31584680e-01 -2.48124197e-01  4.52753401e+00  4.61311483e+00  1.12888784e+01 -1.47791541e+00  1.06459880e+00 -5.09466124e+00 -4.20622873e+00 -9.84026134e-01  3.29805136e+00  4.66699004e-01  2.66770184e-01  4.65309668e+00  9.07093525e+00 -1.13914509e+01  1.96320581e+00 -5.29363585e+00 -1.16261348e-01  6.34755611e+00  3.36108732e+00 -3.04764223e+00 -4.82527542e+00 -9.52515984e+00 -1.84816420e+00 -8.07553005e+00 -2.23457837e+00 -4.17569399e+00 -2.32864642e+00 -7.53215170e+00 -1.39816165e+00  2.67845201e+00  1.25607419e+00 -5.62514544e+00 -2.45493865e+00 -1.61974132e+00  5.95797110e+00  2.21951842e+00 -1.61853194e+00 -4.31568766e+00  8.79383945e+00  9.77784920e+00\n",
      " -1.86121559e+00  2.40073013e+00 -9.05537701e+00  3.64689517e+00  8.11322021e+00 -2.83100080e+00  1.00669193e+01  8.15732288e+00  1.12356558e+01  6.22024441e+00  2.82636356e+00  6.10496235e+00  2.60586858e+00  9.33604181e-01 -7.82307863e+00 -1.24407272e+01 -5.41595936e+00  7.29830074e+00 -2.23238063e+00 -9.38078213e+00  4.66171801e-01  5.95245218e+00  1.02519226e+01  7.81796741e+00  4.04809809e+00  5.59612656e+00 -8.91029549e+00 -4.78873253e+00 -5.66514492e+00 -7.87722707e-01 -1.22018087e+00 -1.64024448e+00  5.51786661e+00 -1.98604405e+00  3.25489712e+00  1.31357508e+01 -4.25324082e-01 -6.49618530e+00 -9.65244102e+00 -1.49887085e+00 -5.92099547e-01  6.06554270e+00 -4.36684370e+00  3.00934839e+00 -6.30206203e+00 -3.66073996e-01 -4.28811073e+00 -8.13037872e+00 -5.17499590e+00 -2.96969175e-01  1.50274587e+00  1.10516071e+01  5.56614828e+00 -7.03986502e+00 -3.70291138e+00 -3.03654742e+00  4.14033592e-01  7.27513027e+00 -6.82453394e+00  6.90278578e+00 -3.27532601e+00 -7.05520487e+00\n",
      "  1.68594003e+00 -8.06772709e+00  4.98058510e+00 -5.22932243e+00  6.32008165e-02  8.12857533e+00 -3.66957331e+00  8.71592808e+00 -2.85482740e+00 -3.55880547e+00  5.29651546e+00 -5.95794153e+00  5.61339951e+00  6.87810183e-01 -2.18134689e+00 -1.05485678e+01 -6.12612677e+00  8.10267448e-01 -9.79351330e+00  2.66012740e+00 -7.33303738e+00 -9.75392532e+00  7.70812511e-01 -1.25380564e+00  4.07909298e+00 -8.28177738e+00 -8.53818798e+00 -7.32260036e+00 -6.80827427e+00  1.30170202e+00  7.52272940e+00  3.06380033e+00  6.04399586e+00 -5.00773859e+00  6.77847207e-01  8.07534885e+00  7.06348896e+00  5.28859711e+00  7.56845665e+00  8.97385406e+00  1.47659647e+00 -1.20597391e+01  6.40491390e+00  2.74687672e+00 -4.13309908e+00  2.91129875e+00 -1.09865189e+01  5.17696321e-01  4.04869938e+00 -1.47520185e+00 -5.90529442e-01 -3.26641709e-01 -5.24428177e+00 -1.76898181e-01  1.19836226e+01 -5.03639030e+00 -6.87836933e+00  3.07168603e+00  5.81556082e-01 -3.99357414e+00 -5.42136967e-01  3.69725084e+00\n",
      "  8.74753094e+00 -7.16164827e+00 -6.98482704e+00  1.27258615e+01  9.59587479e+00  3.79385757e+00  9.47279644e+00  7.36650801e+00 -5.47536707e+00  8.73175907e+00  5.16793579e-02 -1.51048505e+00  1.01770859e+01  2.33448458e+00 -6.01931953e+00  7.86564445e+00  3.73270571e-01 -1.56903255e+00  1.76062241e+01  7.46163547e-01  3.75460744e+00 -3.38291550e+00 -6.34843922e+00  8.82538700e+00  4.33284712e+00  8.76286697e+00 -8.36585236e+00  6.74455500e+00 -8.89901829e+00  9.40679789e-01  5.91660786e+00  4.84909582e+00 -7.24453449e+00 -3.84437346e+00 -1.16401243e+01  9.72719252e-01  7.06460476e+00  2.59865808e+00  6.02048159e-01  6.30202723e+00 -2.29030895e+00 -2.13976169e+00  9.06470585e+00  8.31143284e+00 -3.34124374e+00 -2.35992146e+00  3.20061469e+00 -2.79238510e+00 -8.78612995e+00  3.67745757e+00  6.14848137e+00  6.17060852e+00 -1.07815161e+01  8.04029655e+00  2.79277086e+00 -3.40881252e+00 -4.31790733e+00 -1.49889863e+00  8.11114192e-01 -3.03838873e+00 -4.48084831e+00  6.08876348e-04\n",
      " -9.70608616e+00 -3.48899221e+00  5.19813681e+00 -1.11281528e+01  3.99292493e+00 -9.87576485e+00 -3.80842400e+00 -3.04526985e-01 -5.42044020e+00 -7.44674587e+00  9.65393639e+00  5.30336285e+00  8.51479626e+00  3.30430150e-01 -1.13448572e+00 -5.76113510e+00 -5.98934221e+00 -5.96569538e+00 -9.26859093e+00  5.71127796e+00  4.24171829e+00 -7.86391211e+00 -9.80766010e+00 -4.47044373e+00  1.42357540e+00  8.77702236e+00 -6.50635195e+00 -1.78739285e+00  7.27090454e+00  5.66927290e+00 -5.67377234e+00 -5.18286419e+00 -3.22341490e+00 -3.23014903e+00  7.16762018e+00  3.50374055e+00 -8.24767017e+00  3.08172464e+00 -1.81511343e+00 -5.56766987e+00  6.28865957e+00  8.69088936e+00 -7.33874035e+00 -3.33055401e+00 -5.66243887e+00 -2.44644880e+00 -7.81346226e+00 -9.66386259e-01  5.21351814e+00 -4.25366306e+00 -4.64129734e+00  8.15106583e+00 -1.53934240e+00 -1.91403615e+00  6.44758797e+00 -2.41945529e+00  4.37186241e+00  1.20055599e+01 -4.00086212e+00 -2.12869716e+00  7.27900124e+00  5.88332176e+00\n",
      " -6.18284416e+00  4.18487930e+00 -4.48272800e+00  3.51091766e+00  7.44442606e+00 -9.33957577e+00 -2.49446630e+00 -6.52417231e+00 -1.01583138e+01  2.02730489e+00  3.94137001e+00 -6.33610058e+00 -1.07120724e+01 -6.13559365e-01 -5.01401281e+00 -5.12240171e+00 -3.58130336e+00 -4.85753870e+00 -3.09387207e-01 -6.42612743e+00 -9.94329262e+00  6.63145876e+00  1.09651780e+00 -5.25142717e+00  1.88256502e+00 -3.72219896e+00  2.90968037e+00  6.64737940e+00 -5.44985294e+00 -8.43170285e-01 -2.81451178e+00  3.31238055e+00  4.99336147e+00  2.86010289e+00 -3.63700294e+00 -1.02199144e+01  1.05247917e+01 -3.40238476e+00 -9.38066769e+00 -1.89472091e+00  6.30701399e+00 -7.29292297e+00 -6.12109756e+00  3.83669519e+00 -2.50895047e+00 -2.15178117e-01  9.10373867e-01  1.38005495e+01 -1.75004082e+01  5.78048277e+00  4.52695274e+00  1.64872322e+01  9.37365532e+00 -2.24325752e+00 -2.65568471e+00  5.59656286e+00  4.77880812e+00 -5.81972313e+00 -2.65253758e+00  8.90719604e+00 -4.98062420e+00  1.83440578e+00\n",
      " -9.63610411e-02  1.09693384e+00 -1.19056511e+01  4.35142040e+00  8.16691399e+00  6.78353691e+00  3.12210178e+00  1.25022469e+01 -3.92790484e+00 -3.76557398e+00 -4.65108252e+00  8.43865395e+00  6.86971045e+00  8.31584072e+00 -1.01994114e+01  1.53605843e+00 -8.62784624e-01 -4.19419336e+00 -6.35698986e+00  9.76832771e+00 -3.52443647e+00 -3.10768437e+00 -6.00598812e+00 -1.32606745e+00  7.36055517e+00 -5.47431564e+00  6.54297304e+00 -6.45214844e+00  7.27763510e+00  4.89859486e+00 -6.71387959e+00 -3.38166738e+00  4.88021088e+00 -1.65353334e+00  3.47194433e+00 -3.47329319e-01  8.07650757e+00 -3.67310596e+00  2.29975867e+00 -6.18828773e-01 -9.48799706e+00  1.66092312e+00 -9.70331860e+00 -9.53095150e+00  2.37183332e+00  5.60638475e+00 -7.97550011e+00 -3.88873959e+00 -4.45432138e+00 -8.60565853e+00 -5.03565788e+00 -3.55736566e+00  7.84907532e+00 -7.50706244e+00  6.77824736e+00  5.42037868e+00  1.47962987e+00  3.81475878e+00  4.18695068e+00 -6.68931055e+00  4.97332859e+00 -4.67054796e+00\n",
      "  5.97846889e+00  1.06030846e+00  1.28853302e+01 -6.16314030e+00 -1.23231006e+00 -6.16147089e+00  8.22352123e+00  5.29240072e-01  6.41524315e+00 -6.59042263e+00 -1.06448641e+01 -3.86634088e+00  5.51236200e+00 -1.11295786e+01 -1.04996004e+01  8.00103188e+00  4.85690212e+00 -3.86196876e+00  1.11038313e+01 -9.01568317e+00  6.32907820e+00 -2.17331958e+00  5.86437464e-01 -2.69101977e+00 -4.89093399e+00 -7.74108601e+00  6.12437820e+00 -2.51070797e-01 -6.04275274e+00 -7.98044729e+00 -2.23297262e+00 -1.68372428e+00  2.57667756e+00  5.96924734e+00  1.05317907e+01 -6.68152046e+00 -4.50507641e+00  5.23638487e+00  4.43889529e-01  7.61360168e+00 -1.18060446e+00  1.13044901e+01 -9.18917775e-01 -3.32294846e+00  6.57660294e+00 -2.78103948e-01  4.10077047e+00  7.84115076e+00 -6.23161125e+00 -1.38396001e+00  2.24572802e+00  1.91264558e+00  7.60748768e+00 -9.50028598e-01 -5.55074501e+00  3.66420364e+00 -9.33690667e-01  2.21717620e+00  2.66994429e+00 -4.33748913e+00 -2.53768301e+00 -5.67639685e+00\n",
      " -5.96371412e+00 -5.61797714e+00 -7.62687635e+00  9.72125912e+00 -3.22034597e+00  6.87769604e+00  8.48739892e-02  2.59004498e+00 -2.54658961e+00  3.98696280e+00  1.04355831e+01  9.67391968e-01 -1.63718557e+00  6.80845833e+00 -8.36092663e+00  9.06392384e+00  5.40453136e-01 -9.67578232e-01  4.13711691e+00  4.53544760e+00  2.95399427e-01  1.80883622e+00  2.10752785e-01 -6.21590042e+00 -1.70574099e-01  5.68152142e+00  3.81421864e-01 -4.04916096e+00  1.90180569e+01 -8.92706490e+00 -2.17324257e+00  2.09550595e+00  2.38822803e-01 -4.60803223e+00 -3.41043055e-01  5.03351353e-02 -9.36468506e+00  8.19890022e+00 -5.24601603e+00  1.76433325e+00  2.43943477e+00  2.40892172e+00  1.56651497e+00  6.27406597e+00  1.00986328e+01 -3.77327657e+00  6.76247597e+00  4.88440466e+00 -3.36860824e+00 -9.91593933e+00  1.41524477e+01 -3.49203610e+00  3.54175305e+00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Disable truncation\n",
    "np.set_printoptions(threshold=np.inf, linewidth=1000)\n",
    "\n",
    "for i, emb in enumerate(val_embeddings):\n",
    "    print(f\"Full Embedding {i}:\\n{emb}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
