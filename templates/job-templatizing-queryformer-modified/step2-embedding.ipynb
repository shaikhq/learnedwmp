{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from model.database_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.util import Normalizer\n",
    "\n",
    "# cost_norm = Normalizer(1, 100)\n",
    "# cost_norm = Normalizer(-3.61192, 12.290855)\n",
    "#cost_norm = Normalizer(5, 2611)\n",
    "cost_norm = Normalizer(8.26, 11.12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # bs = 1024\n",
    "    # SQ: smaller batch size\n",
    "    bs = 64\n",
    "    # bs = 10\n",
    "    #lr = 0.001\n",
    "    lr = 0.001\n",
    "    # epochs = 200\n",
    "    epochs = 50\n",
    "    clip_size = 50\n",
    "    embed_size = 64\n",
    "    pred_hid = 128\n",
    "    ffn_dim = 128\n",
    "    head_size = 12\n",
    "    n_layers = 8\n",
    "    dropout = 0.1\n",
    "    sch_decay = 0.6\n",
    "    # device = 'cuda:0'\n",
    "    device = 'cpu'\n",
    "    newpath = 'job_queries_training'\n",
    "    to_predict = 'cost'\n",
    "args = Args()\n",
    "\n",
    "import os\n",
    "if not os.path.exists(args.newpath):\n",
    "    os.makedirs(args.newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import QueryFormer\n",
    "\n",
    "model = QueryFormer(emb_size = args.embed_size ,ffn_dim = args.ffn_dim, head_size = args.head_size, \\\n",
    "                 dropout = args.dropout, n_layers = args.n_layers, \\\n",
    "                 use_sample = False, use_hist = False, \\\n",
    "                 pred_hid = args.pred_hid\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dataset import PlanTreeDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST - Loading 10 tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model.dataset import PlanTreeDataset  # Assuming PlanTreeDataset is defined elsewhere\n",
    "import json\n",
    "\n",
    "# Path to the tensors folder\n",
    "tensors_dir = \"./job_queries/tensors\"\n",
    "\n",
    "# Validate if the tensors directory exists\n",
    "if not os.path.exists(tensors_dir):\n",
    "    raise FileNotFoundError(f\"Tensors directory '{tensors_dir}' not found.\")\n",
    "\n",
    "# Get all tensor file paths from the directory\n",
    "tensor_files = sorted(os.listdir(tensors_dir))\n",
    "\n",
    "# Validate if tensor files exist in the directory\n",
    "if not tensor_files:\n",
    "    raise FileNotFoundError(f\"No tensor files found in '{tensors_dir}'.\")\n",
    "\n",
    "# Initialize lists to store tensor components\n",
    "x_list = []\n",
    "rel_pos_list = []\n",
    "attn_bias_list = []\n",
    "heights_list = []\n",
    "cost_labels_list = []\n",
    "raw_costs_list = []\n",
    "\n",
    "# Load tensors dynamically\n",
    "for tensor_file in tensor_files:\n",
    "    tensor_path = os.path.join(tensors_dir, tensor_file)\n",
    "    try:\n",
    "        loaded_tensors = torch.load(tensor_path)\n",
    "        # Append components to respective lists\n",
    "        x_list.append(loaded_tensors[\"x\"])\n",
    "        rel_pos_list.append(loaded_tensors[\"rel_pos\"])\n",
    "        attn_bias_list.append(loaded_tensors[\"attn_bias\"])\n",
    "        heights_list.append(loaded_tensors[\"heights\"])\n",
    "        cost_labels_list.append(loaded_tensors[\"cost_labels\"])\n",
    "        raw_costs_list.append(loaded_tensors[\"raw_costs\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tensor file '{tensor_file}': {e}\")\n",
    "\n",
    "# Generate indices for splitting\n",
    "num_examples = len(x_list)\n",
    "if num_examples == 0:\n",
    "    raise ValueError(\"No valid tensors loaded for dataset creation.\")\n",
    "    \n",
    "all_indices = np.arange(num_examples)\n",
    "\n",
    "# Perform train-validation split with fixed seed for reproducibility\n",
    "train_indices, val_indices = train_test_split(all_indices, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = PlanTreeDataset(\n",
    "    len(train_indices),\n",
    "    [x_list[i] for i in train_indices],\n",
    "    [attn_bias_list[i] for i in train_indices],\n",
    "    [rel_pos_list[i] for i in train_indices],\n",
    "    [heights_list[i] for i in train_indices],\n",
    "    [cost_labels_list[i] for i in train_indices],\n",
    "    [raw_costs_list[i] for i in train_indices]\n",
    ")\n",
    "\n",
    "val_dataset = PlanTreeDataset(\n",
    "    len(val_indices),\n",
    "    [x_list[i] for i in val_indices],\n",
    "    [attn_bias_list[i] for i in val_indices],\n",
    "    [rel_pos_list[i] for i in val_indices],\n",
    "    [heights_list[i] for i in val_indices],\n",
    "    [cost_labels_list[i] for i in val_indices],\n",
    "    [raw_costs_list[i] for i in val_indices]\n",
    ")\n",
    "\n",
    "# Save validation indices and file names\n",
    "val_file_names = [tensor_files[i] for i in val_indices]\n",
    "val_data = {\n",
    "    \"val_indices\": val_indices.tolist(),\n",
    "    \"file_names\": val_file_names,\n",
    "}\n",
    "\n",
    "val_data_file = \"./val_data.json\"\n",
    "with open(val_data_file, \"w\") as f:\n",
    "    json.dump(val_data, f)\n",
    "\n",
    "# Save training indices and file names\n",
    "train_file_names = [tensor_files[i] for i in train_indices]\n",
    "train_data = {\n",
    "    \"train_indices\": train_indices.tolist(),\n",
    "    \"file_names\": train_file_names,\n",
    "}\n",
    "\n",
    "train_data_file = \"./train_data.json\"\n",
    "with open(train_data_file, \"w\") as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "# Print information\n",
    "print(\"Training Dataset length:\", len(train_dataset))\n",
    "print(\"Validation Dataset length:\", len(val_dataset))\n",
    "print(f\"Validation data saved to {val_data_file}\")\n",
    "print(f\"Training data saved to {train_data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# File paths\n",
    "val_data_file = \"./val_data.json\"\n",
    "train_data_file = \"./train_data.json\"\n",
    "\n",
    "# Load validation data from JSON file\n",
    "with open(val_data_file, \"r\") as f:\n",
    "    loaded_val_data = json.load(f)\n",
    "\n",
    "loaded_val_indices = loaded_val_data[\"val_indices\"]\n",
    "loaded_val_file_names = loaded_val_data[\"file_names\"]\n",
    "\n",
    "# Create a DataFrame for validation data\n",
    "val_data_df = pd.DataFrame({\n",
    "    \"val_index\": loaded_val_indices,\n",
    "    \"filename\": loaded_val_file_names\n",
    "})\n",
    "\n",
    "# Display the Validation DataFrame\n",
    "print(\"Validation DataFrame:\")\n",
    "print(val_data_df.head())\n",
    "\n",
    "# Save validation data to CSV\n",
    "val_data_df.to_csv(\"./val_data.csv\", index=False)\n",
    "print(\"Validation data saved to ./val_data.csv\")\n",
    "\n",
    "# Load training data from JSON file\n",
    "with open(train_data_file, \"r\") as f:\n",
    "    loaded_train_data = json.load(f)\n",
    "\n",
    "loaded_train_indices = loaded_train_data[\"train_indices\"]\n",
    "loaded_train_file_names = loaded_train_data[\"file_names\"]\n",
    "\n",
    "# Create a DataFrame for training data\n",
    "train_data_df = pd.DataFrame({\n",
    "    \"train_index\": loaded_train_indices,\n",
    "    \"filename\": loaded_train_file_names\n",
    "})\n",
    "\n",
    "# Display the Training DataFrame\n",
    "print(\"Training DataFrame:\")\n",
    "print(train_data_df.head())\n",
    "\n",
    "# Save training data to CSV\n",
    "train_data_df.to_csv(\"./train_data.csv\", index=False)\n",
    "print(\"Training data saved to ./train_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example numpy label\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import importlib\n",
    "\n",
    "from model import trainer\n",
    "importlib.reload(trainer)\n",
    "from  model.trainer import train_single, train\n",
    "\n",
    "\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "# Train the model with the numpy label\n",
    "# trained_model = train_single(model, dataset, dataset, crit, cost_norm, args)\n",
    "model, best_model_path, train_embeddings, val_embeddings = train(model, train_dataset, val_dataset, crit, cost_norm, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loaded_val_indices[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume these are loaded or computed\n",
    "# val_embeddings: List or numpy array of embeddings (e.g., shape [num_examples, embedding_dim])\n",
    "# loaded_val_indices: List of validation indices\n",
    "# loaded_file_names: List of validation file names\n",
    "\n",
    "# Convert val_embeddings to a numpy array if it's not already\n",
    "val_embeddings = np.array(val_embeddings)\n",
    "\n",
    "# Ensure the size of all inputs matches\n",
    "if len(loaded_val_indices) != len(loaded_val_file_names) or len(loaded_val_indices) != len(val_embeddings):\n",
    "    raise ValueError(\"The sizes of validation indices, filenames, and embeddings must match.\")\n",
    "\n",
    "# Create a DataFrame\n",
    "val_data_df = pd.DataFrame({\n",
    "    \"val_index\": loaded_val_indices,\n",
    "    \"filename\": loaded_val_file_names,\n",
    "    \"embedding\": list(val_embeddings)  # Store embeddings as a list of numpy arrays\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = \"validation_embeddings.csv\"\n",
    "val_data_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Optionally save embeddings in a separate binary file for efficient storage\n",
    "embeddings_file_path = \"validation_embeddings.npy\"\n",
    "np.save(embeddings_file_path, val_embeddings)\n",
    "\n",
    "print(f\"Validation DataFrame created and saved to {csv_file_path}\")\n",
    "print(f\"Embeddings saved to {embeddings_file_path}\")\n",
    "\n",
    "# Print the first few rows for verification\n",
    "print(val_data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training embeddings (best epoch): {len(train_embeddings)}\")\n",
    "print(f\"First training embedding shape: {train_embeddings[0].shape}\")\n",
    "print(f\"Number of validation embeddings (best epoch): {len(val_embeddings)}\")\n",
    "print(f\"First validation embedding shape: {val_embeddings[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First validation embedding shape: {val_embeddings[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(val_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First validation embedding shape: {val_embeddings[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Disable truncation\n",
    "# np.set_printoptions(threshold=np.inf, linewidth=1000)\n",
    "\n",
    "# for i, emb in enumerate(val_embeddings):\n",
    "#     print(f\"Full Embedding {i}:\\n{emb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Ensure embeddings are numpy arrays\n",
    "train_embeddings = np.array(train_embeddings)\n",
    "val_embeddings = np.array(val_embeddings)\n",
    "\n",
    "# Paths for saving the JSON files\n",
    "train_embeddings_file = \"./train_embeddings.json\"\n",
    "val_embeddings_file = \"./val_embeddings.json\"\n",
    "\n",
    "# Create dictionaries with file names as keys and embeddings as values\n",
    "train_embedding_dict = {train_file_name: embedding.tolist() for train_file_name, embedding in zip(loaded_train_file_names, train_embeddings)}\n",
    "val_embedding_dict = {val_file_name: embedding.tolist() for val_file_name, embedding in zip(loaded_val_file_names, val_embeddings)}\n",
    "\n",
    "# Save training embeddings to JSON\n",
    "with open(train_embeddings_file, \"w\") as f:\n",
    "    json.dump(train_embedding_dict, f)\n",
    "\n",
    "# Save validation embeddings to JSON\n",
    "with open(val_embeddings_file, \"w\") as f:\n",
    "    json.dump(val_embedding_dict, f)\n",
    "\n",
    "print(f\"Train embeddings saved to {train_embeddings_file}\")\n",
    "print(f\"Validation embeddings saved to {val_embeddings_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Path to the JSON file\n",
    "val_embeddings_file = \"./val_embeddings.json\"  # Replace with train_embeddings.json for training data\n",
    "\n",
    "# Load the JSON file\n",
    "with open(val_embeddings_file, \"r\") as f:\n",
    "    embeddings_data = json.load(f)\n",
    "\n",
    "# Convert JSON data to a DataFrame\n",
    "val_embeddings_df = pd.DataFrame({\n",
    "    \"file_name\": list(embeddings_data.keys()),\n",
    "    \"embedding\": list(embeddings_data.values())\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(val_embeddings_df.head())\n",
    "\n",
    "# Optionally save it to a CSV\n",
    "val_embeddings_df.to_csv(\"val_embeddings.csv\", index=False)\n",
    "print(\"Validation embeddings saved to val_embeddings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
