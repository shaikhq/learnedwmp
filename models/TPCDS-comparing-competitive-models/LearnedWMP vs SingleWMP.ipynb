{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import loguniform\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn import neural_network\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.model_selection import validation_curve\n",
    "import os # new!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "box_plot_title = 'Memory Estimation Error (MB)'\n",
    "pd.set_option('display.max_columns', None)\n",
    "# cluster_set = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('tpcds_query_train.csv')\n",
    "    # df_test = pd.read_csv('job2_test_clean.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add queryId starting from 1\n",
    "df.insert(0, 'queryId', range(1, len(df) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train and Evaluate a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(estimator, batch_size, X, Y):\n",
    "    predicted = estimator.predict(X)\n",
    "    Y = np.insert(Y, Y.shape[1], predicted, axis=1)\n",
    "    \n",
    "    indices = np.linspace(0, X.shape[0]-1, X.shape[0], dtype=int)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    num_batches = int(np.floor(X.shape[0] / batch_size))\n",
    "    \n",
    "    df_batches = pd.DataFrame(columns=['actual', 'db2', 'ml'])\n",
    "    \n",
    "    for ibat in range(num_batches):\n",
    "        start = (ibat * batch_size)\n",
    "        end = (ibat * batch_size + batch_size) - 1\n",
    "        \n",
    "        ibat_Y = Y[indices[start:end]]\n",
    "        \n",
    "        actual = sum(ibat_Y[:,-1])\n",
    "        db2 = sum(ibat_Y[:,-2])\n",
    "        ml = sum(ibat_Y[:,-3])\n",
    "        \n",
    "        df_batches = df_batches.append({'actual':actual,\n",
    "                                       'db2':db2,\n",
    "                                       'ml':ml},\n",
    "                                      ignore_index=True)\n",
    "        \n",
    "    return df_batches\n",
    "\n",
    "def rmse(Y):\n",
    "    cols = Y.columns.values[1:]\n",
    "    rmse_dict = {}\n",
    "    \n",
    "    for col in cols:\n",
    "        rmse = np.round(np.sqrt(mean_squared_error(Y['actual'].values, Y[col].values)))\n",
    "        rmse_dict[col] = rmse\n",
    "    \n",
    "    return rmse_dict\n",
    "    \n",
    "def calculate_residuals(Y):\n",
    "    first_col = Y.columns[0]\n",
    "    cols = Y.columns[1:]\n",
    "    df_residuals = pd.DataFrame(columns=cols)\n",
    "\n",
    "    for col in cols:\n",
    "        df_residuals[col] = Y[col] - Y[first_col]\n",
    "        \n",
    "    return df_residuals\n",
    "\n",
    "def box_plot(Y, length, height):\n",
    "    df_residuals = calculate_residuals(Y)\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "    f = plt.figure(figsize=[length,height])\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    ax = f.add_subplot(111)\n",
    "    sns.boxplot(data=df_residuals, ax=ax, showfliers = True, orient=\"h\")\n",
    "    ax.set_xlabel(xlabel=box_plot_title,fontsize=22)\n",
    "    plt.tick_params(axis='x',labeltop='on', labelbottom='on')\n",
    "    ax.xaxis.set_ticks_position('both')\n",
    "    #ax.set_yticks(yticks_new)\n",
    "#     plt.setp(ax.get_yticklabels(), rotation=90)\n",
    "    f.tight_layout()\n",
    "    plt.show()\n",
    "    ax.savefig('job_err.png')\n",
    "def residual_plot(Y):\n",
    "    Y_predicted = Y.iloc[:,1:]\n",
    "    print('Y_predicted ', Y_predicted.shape)\n",
    "    cols = Y_predicted.columns\n",
    "    markers = ['8', 'P', '*', 'h', 'X','+','^','s','o']\n",
    "#     colors = ['steelblue', 'darkorange', 'darkorchid', 'limegreen', 'fuchsia']\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(cols)))\n",
    "    \n",
    "    Y_residuals = calculate_residuals(Y)\n",
    "    print('Y_residuals ', Y_residuals.shape)\n",
    "    \n",
    "    for col in cols:\n",
    "        plot_index = Y_predicted.columns.get_loc(col)\n",
    "        plt.scatter(Y_predicted[col], Y_residuals[col], \n",
    "                   edgecolor='white', c=colors[plot_index],\n",
    "                   marker=markers[plot_index], label=col)\n",
    "    plt.xlabel('Predicted values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.hlines(y=0, xmin=0, xmax=9000, color='black', lw=2)\n",
    "    plt.xlim([0, 9000])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def create_workload(batch_size, data):\n",
    "    # Select relevant columns\n",
    "    df_data = data[['queryId', 'db2', 'actual', 'cluster']]\n",
    "    \n",
    "    labels = df_data['cluster'].unique()\n",
    "    labels = np.sort(labels)\n",
    "    cluster_columns = [f\"cluster_{int(c)}\" for c in labels]\n",
    "    \n",
    "    # Break view link if any filtering\n",
    "    df_data = df_data.copy()\n",
    "    df_data.loc[:, \"cluster\"] = df_data[\"cluster\"].astype(\"int64\")\n",
    "\n",
    "    df_data = pd.get_dummies(df_data, columns=[\"cluster\"], dtype=int)\n",
    "\n",
    "    missing_columns = [col for col in cluster_columns if col not in df_data.columns]\n",
    "    if missing_columns:\n",
    "        df_missing = pd.DataFrame(0, index=df_data.index, columns=missing_columns)\n",
    "        df_data = pd.concat([df_data, df_missing], axis=1)\n",
    "\n",
    "    # Sort columns\n",
    "    df_data = df_data.reindex(columns=['queryId', 'db2', 'actual'] + cluster_columns)\n",
    "    \n",
    "    # Initialize batches\n",
    "    df_batches = []\n",
    "    query_ids_per_batch = []  # üëà list to track query IDs\n",
    "    indices = np.arange(len(df_data))\n",
    "    num_batches = len(df_data) // batch_size\n",
    "    \n",
    "    # Create batches\n",
    "    for ibat in range(num_batches):\n",
    "        batch_indices = indices[ibat * batch_size:(ibat + 1) * batch_size]\n",
    "        ibat_Y = df_data.iloc[batch_indices]\n",
    "\n",
    "        df_batches.append(ibat_Y.drop(columns=['queryId']).sum())  # drop queryId before summing\n",
    "        query_ids_per_batch.append(ibat_Y['queryId'].tolist())     # üëà save queryIds separately\n",
    "\n",
    "    # Combine batches into a DataFrame\n",
    "    batches_df = pd.DataFrame(df_batches)\n",
    "\n",
    "    return batches_df, query_ids_per_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_clusters(k, data, km=None, test_size=0.3, min_rows_per_cluster=10, random_state=42):\n",
    "    # Split data\n",
    "    train_data, test_data = train_test_split(\n",
    "        data,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Features only (drop queryId, db2, actual)\n",
    "    X_train = train_data.drop(columns=['queryId', 'db2', 'actual'])\n",
    "    X_test = test_data.drop(columns=['queryId', 'db2', 'actual'])\n",
    "\n",
    "    # Train KMeans once\n",
    "    if km is not None:\n",
    "        print('Using provided clustering model')\n",
    "    else:\n",
    "        print('Training initial clustering model')\n",
    "        km = KMeans(\n",
    "            n_clusters=k,\n",
    "            init='k-means++',\n",
    "            n_init=10,\n",
    "            max_iter=300,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        km.fit(X_train)\n",
    "\n",
    "    # Predict initial cluster assignments\n",
    "    initial_train_clusters = km.predict(X_train)\n",
    "\n",
    "    # Find which clusters to keep\n",
    "    cluster_counts = pd.Series(initial_train_clusters).value_counts()\n",
    "    keep_clusters = cluster_counts[cluster_counts >= min_rows_per_cluster].index.tolist()\n",
    "    print(f\"Kept {len(keep_clusters)} clusters after pruning (‚â• {min_rows_per_cluster} members each)\")\n",
    "\n",
    "    # Keep only centroids of valid clusters\n",
    "    kept_centroids = km.cluster_centers_[keep_clusters]\n",
    "\n",
    "    # Function to assign rows to nearest kept centroid\n",
    "    def assign_to_kept_centroids(X, kept_centroids):\n",
    "        distances = cdist(X, kept_centroids, metric='euclidean')  # shape: (n_samples, n_kept_clusters)\n",
    "        nearest_cluster_indices = np.argmin(distances, axis=1)    # which centroid is nearest\n",
    "        return nearest_cluster_indices\n",
    "\n",
    "    # Predict final labels for full train and test sets\n",
    "    relabeled_train_clusters = assign_to_kept_centroids(X_train, kept_centroids)\n",
    "    relabeled_test_clusters = assign_to_kept_centroids(X_test, kept_centroids)\n",
    "\n",
    "    # Relabel clusters starting from 1\n",
    "    relabeled_train_clusters += 1\n",
    "    relabeled_test_clusters += 1\n",
    "\n",
    "    # Prepare final DataFrames\n",
    "    train_df = train_data.copy()\n",
    "    train_df['cluster'] = relabeled_train_clusters.astype('int64')\n",
    "\n",
    "    test_df = test_data.copy()\n",
    "    test_df['cluster'] = relabeled_test_clusters.astype('int64')\n",
    "\n",
    "    # Report\n",
    "    print('Distortion (Training Inertia): %.2f' % km.inertia_)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 110\n",
    "df_train, df_test = get_clusters(k, df, min_rows_per_cluster=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"cluster\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of rows in each cluster\n",
    "cluster_counts = df_train[\"cluster\"].value_counts().sort_index()\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(cluster_counts.index, cluster_counts.values)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Number of Rows\")\n",
    "plt.title(\"Distribution of Rows by Cluster\")\n",
    "plt.xticks(cluster_counts.index)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Ensure the folder exists\n",
    "# output_folder = \"cluster_data\"\n",
    "# if not os.path.exists(output_folder):\n",
    "#     os.makedirs(output_folder)\n",
    "    \n",
    "df_train_workloads, train_query_ids_per_batch = create_workload(batch_size, df_train)\n",
    "df_test_workloads, test_query_ids_per_batch = create_workload(batch_size, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_workloads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_query_ids_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query_ids_per_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_workloads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns that are missing in test\n",
    "missing_cols = set(df_train_workloads.columns) - set(df_test_workloads.columns)\n",
    "\n",
    "# Add missing columns with value 0\n",
    "for col in missing_cols:\n",
    "    df_test_workloads[col] = 0\n",
    "\n",
    "# Reorder columns to match train\n",
    "df_test_workloads = df_test_workloads[df_train_workloads.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_workloads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_workloads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_workloads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_workloads.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_workloads.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "align the two frames on their columns and tell pandas to fill anything that‚Äôs missing with¬†0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_train_data():\n",
    "    df = df_train_workloads\n",
    "    \n",
    "    feature_cols = [col for col in df.columns if col.startswith('cluster_')]    \n",
    "    print(feature_cols)\n",
    "\n",
    "    target_col = ['actual']\n",
    "    \n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col].values.ravel()  # Flatten y to 1D\n",
    "    \n",
    "    print('X.shape: ', X.shape)\n",
    "    print('y.shape: ', y.shape)\n",
    "    \n",
    "    return X.values, y\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    df = df_test_workloads\n",
    "  \n",
    "    # Assuming `df` is your DataFrame\n",
    "    feature_cols = [col for col in df.columns if col.startswith('cluster_')]\n",
    "    print(feature_cols)\n",
    "    \n",
    "    target_cols = ['db2', 'actual']\n",
    "    \n",
    "    X = df[feature_cols]\n",
    "    Y = df[target_cols]\n",
    "    \n",
    "    print('X.shape: ', X.shape)\n",
    "    print('y.shape: ', Y.shape)\n",
    "    \n",
    "    return X, Y, test_query_ids_per_batch\n",
    "\n",
    "def my_validation_curve(estimator_name, estimator, param_name, param_range):\n",
    "    train_scores, valid_scores = validation_curve(estimator, X, y, param_name=param_name,\n",
    "        param_range=param_range, cv=10, scoring=\"neg_mean_squared_error\",\n",
    "    )\n",
    "\n",
    "    train_scores = np.sqrt(np.abs(train_scores))\n",
    "    valid_scores = np.sqrt(np.abs(valid_scores))\n",
    "    \n",
    "    print(len(train_scores))\n",
    "    print(len(valid_scores))\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "    \n",
    "    title_str = \"Validation Curve with \" + estimator_name\n",
    "    plt.title(title_str)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.plot(param_range, train_scores_mean, label=\"train rmse\")\n",
    "    plt.plot(param_range, valid_scores_mean, label=\"validation rmse\")\n",
    "\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    train_rmse = [round(elem, 2) for elem in train_scores_mean]\n",
    "    valid_rmse = [round(elem, 2) for elem in valid_scores_mean]\n",
    "    \n",
    "    df_scores = pd.DataFrame({'param': param_range, 'train_rmse': train_rmse, 'valid_rmse': valid_rmse})\n",
    "    print(df_scores)\n",
    "    \n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def cross_validate(model):\n",
    "    # Load data\n",
    "    X, y = load_train_data()\n",
    "    train_data = X.copy()\n",
    "    train_targets = y.copy()\n",
    "\n",
    "    # k = 10\n",
    "    num_val_samples = len(train_data) // k\n",
    "    all_train_scores = []\n",
    "    all_scores = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        print(f\"Processing fold #{i}\")\n",
    "        val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        \n",
    "        partial_train_data = np.concatenate([train_data[:i * num_val_samples],\n",
    "                                             train_data[(i + 1) * num_val_samples:]], axis=0)\n",
    "        \n",
    "        partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],\n",
    "                                                train_targets[(i + 1) * num_val_samples:]], axis=0)\n",
    "\n",
    "        model.fit(partial_train_data, partial_train_targets)\n",
    "    \n",
    "        train_mse = mean_squared_error(partial_train_targets, model.predict(partial_train_data))\n",
    "        val_mse = mean_squared_error(val_targets, model.predict(val_data))\n",
    "    \n",
    "        all_train_scores.append(train_mse)\n",
    "        all_scores.append(val_mse)\n",
    "    \n",
    "    train_rmse = np.sqrt(np.mean(all_train_scores))\n",
    "    val_rmse = np.sqrt(np.mean(all_scores))\n",
    "\n",
    "    print('train rmse:', train_rmse)\n",
    "    print('validation rmse:', val_rmse)\n",
    "\n",
    "    return train_rmse, val_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_scores = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Cross-Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(\n",
    "        fit_intercept=True, \n",
    "        solver='lsqr',\n",
    "        alpha = 1.0,\n",
    "        random_state=42)\n",
    "\n",
    "rmse_scores['Ridge'] = cross_validate(ridge)\n",
    "# ridge.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression - Tuning max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(\n",
    "    max_depth=5,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=0.23,\n",
    "    random_state=33,\n",
    ")\n",
    "\n",
    "rmse_scores['Decision Tree'] = cross_validate(tree)\n",
    "\n",
    "tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest final model - using tuned HP from AutoAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(\n",
    "    max_depth=5,\n",
    "    max_features=0.6109469920813564,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=5,\n",
    "    n_estimators=17,\n",
    "    #n_jobs=CPU_NUMBER,\n",
    "    random_state=33,\n",
    ")\n",
    "\n",
    "rmse_scores['Random Forest'] = cross_validate(forest)\n",
    "\n",
    "forest.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_regressor = XGBRegressor(\n",
    "    # --- general ----------------------------------------------------------\n",
    "    objective=\"reg:squarederror\",        # default for regression\n",
    "    base_score=0.5,\n",
    "    booster=\"gbtree\",\n",
    "    random_state=33,                     # controls all randomness\n",
    "    seed=33,                             # still accepted (alias for random_state)\n",
    "\n",
    "    # --- tree construction ------------------------------------------------\n",
    "    tree_method=\"hist\",                  # faster than \"exact\" on most CPUs\n",
    "    device=\"cpu\",                        # set to \"cuda\" for GPU training ‚ûä\n",
    "    n_jobs=1,                            # threads (was `nthread`) ‚ûã\n",
    "    n_estimators=879,\n",
    "    learning_rate=0.1814227666290778,\n",
    "    max_depth=1,\n",
    "    min_child_weight=2,\n",
    "    max_delta_step=0,\n",
    "\n",
    "    # --- column / row sampling -------------------------------------------\n",
    "    subsample=0.04694370939809412,\n",
    "    colsample_bytree=1,\n",
    "    colsample_bylevel=1,\n",
    "    colsample_bynode=1,\n",
    "\n",
    "    # --- regularisation ---------------------------------------------------\n",
    "    gamma=0.0,                           # alias for `min_split_loss`\n",
    "    reg_alpha=1.0,\n",
    "    reg_lambda=0.40529327440922186,\n",
    "\n",
    "    # --- misc -------------------------------------------------------------\n",
    "    interaction_constraints=\"\",\n",
    "    monotone_constraints=\"()\",\n",
    "    num_parallel_tree=1,\n",
    "    scale_pos_weight=1,\n",
    "    verbosity=0,                         # replaces deprecated `silent` ‚ûå\n",
    "    validate_parameters=True,            # still supported ‚ûç\n",
    ")\n",
    "\n",
    "\n",
    "rmse_scores['XGBoost'] = cross_validate(xgb_regressor)\n",
    "\n",
    "xgb_regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "dnn_model = dnn_model = MLPRegressor(max_iter=500,\n",
    "                     alpha=0.001,\n",
    "                     activation='identity',\n",
    "                     learning_rate= 'constant',\n",
    "                     random_state = 6,\n",
    "                     hidden_layer_sizes = (48, 39, 27, 16, 7, 5),\n",
    "                     solver = 'lbfgs'\n",
    "                    )\n",
    "rmse_scores['DNN'] = cross_validate(dnn_model)\n",
    "\n",
    "dnn_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract model names and RMSE values\n",
    "model_names = list(rmse_scores.keys())\n",
    "train_rmses = [rmse_scores[model][0] for model in model_names]\n",
    "val_rmses = [rmse_scores[model][1] for model in model_names]\n",
    "\n",
    "# Update model labels\n",
    "label_mapping = {\n",
    "    'Ridge': 'Ridge (LearnedWMP)',\n",
    "    'Decision Tree': 'Decision Tree (Baseline)',\n",
    "    'Random Forest': 'Random Forest (Baseline)',\n",
    "    'XGBoost': 'XGBoost (Baseline)',\n",
    "    'DNN': 'DNN (LearnedWMP)'\n",
    "}\n",
    "model_names = [label_mapping.get(name, name) for name in model_names]\n",
    "\n",
    "# Identify the best model (lowest validation RMSE)\n",
    "best_index = val_rmses.index(min(val_rmses))\n",
    "\n",
    "# Plot setup\n",
    "x = range(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Setup consistent colors\n",
    "train_color = '#a6cee3'  # light blue for Train RMSE\n",
    "val_color = '#fdbf6f'    # light orange for Validation RMSE\n",
    "\n",
    "# Draw bars\n",
    "train_bars = ax.bar([i - width/2 for i in x], train_rmses, width, label='Train RMSE', color=train_color)\n",
    "val_bars = ax.bar([i + width/2 for i in x], val_rmses, width, label='Validation RMSE', color=val_color)\n",
    "\n",
    "# Add RMSE value labels on top of bars\n",
    "for bar_group in [train_bars, val_bars]:\n",
    "    for bar in bar_group:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Find minimum of train and validation RMSE for best model\n",
    "best_bar_height = min(train_rmses[best_index], val_rmses[best_index])\n",
    "\n",
    "# Annotate above the bar, move slightly higher\n",
    "ax.annotate(\n",
    "    '‚¨á Best Model',\n",
    "    xy=(best_index, best_bar_height),\n",
    "    xytext=(0, 20),      # üîµ move 20 points above\n",
    "    textcoords='offset points',\n",
    "    ha='center',\n",
    "    va='bottom',\n",
    "    fontsize=10,\n",
    "    color='#1f9e44',\n",
    "    fontweight='bold'\n",
    ")\n",
    "\n",
    "\n",
    "# Formatting and labels\n",
    "ax.set_ylabel('RMSE', fontsize=12)\n",
    "ax.set_title('Train vs Validation RMSE by Model', fontsize=14, weight='bold', pad=20)\n",
    "ax.set_xticks(list(x))\n",
    "ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Remove grid lines\n",
    "ax.grid(False)\n",
    "\n",
    "# Remove top and right spines (borders)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust y-limits\n",
    "ax.set_ylim(\n",
    "    bottom=0,  # start at 0 now (no need for negative values)\n",
    "    top=max(max(train_rmses), max(val_rmses)) + 5\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test, list_query_ids = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_query_ids_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y_test.copy()\n",
    "Y_test['ridge'] = ridge.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'actual' is the first column\n",
    "cols = ['actual'] + [col for col in Y_test.columns if col != 'actual']\n",
    "Y_test = Y_test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    epsilon = 1e-10\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "\n",
    "# Assume Y_test is a DataFrame with 'actual', 'db2', 'dnn'\n",
    "\n",
    "# MAPE for db2\n",
    "mape_db2 = mean_absolute_percentage_error(Y_test['actual'], Y_test['db2'])\n",
    "\n",
    "# MAPE for dnn\n",
    "mape_dnn = mean_absolute_percentage_error(Y_test['actual'], Y_test['ridge'])\n",
    "\n",
    "print(f\"MAPE (db2): {mape_db2:.3f}%\")\n",
    "print(f\"MAPE (ridge): {mape_dnn:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Db2 vs Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume Y_test is your DataFrame\n",
    "\n",
    "# Calculate per-row MAPE for each method\n",
    "Y_test['db2_mape'] = np.abs((Y_test['db2'] - Y_test['actual']) / (Y_test['actual'] + 1e-10)) * 100\n",
    "Y_test['ridge_mape'] = np.abs((Y_test['ridge'] - Y_test['actual']) / (Y_test['actual'] + 1e-10)) * 100\n",
    "\n",
    "# Compare which method is better for each example\n",
    "db2_better = Y_test[Y_test['db2_mape'] < Y_test['ridge_mape']]\n",
    "ridge_better = Y_test[Y_test['ridge_mape'] < Y_test['db2_mape']]\n",
    "\n",
    "# (Optional) Print some results\n",
    "print(f\"Number of examples where DB2 is better (by MAPE): {len(db2_better)}\")\n",
    "print(f\"Number of examples where Ridge is better (by MAPE): {len(ridge_better)}\")\n",
    "\n",
    "# (Optional) View\n",
    "# print(db2_better[['actual', 'db2', 'ridge', 'db2_mape', 'ridge_mape']])\n",
    "# print(ridge_better[['actual', 'db2', 'ridge', 'db2_mape', 'ridge_mape']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count the number of rows where db2_mape is below 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_db2_mape_below_1 = (Y_test['db2_mape'] < 1).sum()\n",
    "print(f\"Number of rows where DB2 MAPE < 1%: {count_db2_mape_below_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Rows with Smallest DB2 MAPE (among examples where DB2 MAPE < 1%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select rows where db2_mape < 1\n",
    "db2_mape_below_1 = Y_test[Y_test['db2_mape'] < 1]\n",
    "\n",
    "# Step 2: Prepare a small DataFrame showing original index, db2_mape, ridge_mape\n",
    "result = db2_mape_below_1[['db2_mape', 'ridge_mape']]\n",
    "\n",
    "# Step 3: Reset index for display if you want, but keep original index shown\n",
    "print(\"Original Row Index | DB2 MAPE (%) | Ridge MAPE (%)\")\n",
    "for idx, row in result.iterrows():\n",
    "    print(f\"{idx:<18} {row['db2_mape']:.4f}%         {row['ridge_mape']:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rows where DB2 MAPE < 1% and Ridge MAPE ‚â• 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select rows where db2_mape < 1\n",
    "db2_mape_below_1 = Y_test[Y_test['db2_mape'] < 1]\n",
    "\n",
    "# Step 2: Further filter to keep rows where ridge_mape >= 5\n",
    "filtered = db2_mape_below_1[db2_mape_below_1['ridge_mape'] >= 5]\n",
    "\n",
    "# Step 3: Sort filtered rows by db2_mape ascending\n",
    "filtered_sorted = filtered.sort_values(by='db2_mape')\n",
    "\n",
    "# Step 4: Keep only the top 10 rows\n",
    "top_10_filtered = filtered_sorted.head(10)\n",
    "\n",
    "# Step 5: Prepare a small DataFrame showing original index, db2_mape, ridge_mape\n",
    "result = top_10_filtered[['db2_mape', 'ridge_mape']]\n",
    "\n",
    "# Step 6: Print\n",
    "print(\"Original Row Index | DB2 MAPE (%) | Ridge MAPE (%)\")\n",
    "for idx, row in result.iterrows():\n",
    "    print(f\"{idx:<18} {row['db2_mape']:.4f}%         {row['ridge_mape']:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Rows Where Ridge MAPE < 1% and DB2 MAPE ‚â• 5% (Sorted by Smallest Ridge MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select rows where ridge_mape < 1\n",
    "ridge_mape_below_1 = Y_test[Y_test['ridge_mape'] < 1]\n",
    "\n",
    "# Step 2: Further filter to keep rows where db2_mape >= 5\n",
    "filtered = ridge_mape_below_1[ridge_mape_below_1['db2_mape'] >= 5]\n",
    "\n",
    "# Step 3: Sort filtered rows by ridge_mape ascending\n",
    "filtered_sorted = filtered.sort_values(by='ridge_mape')\n",
    "\n",
    "# Step 4: Keep only the top 10 rows\n",
    "top_10_filtered = filtered_sorted.head(10)\n",
    "\n",
    "# Step 5: Prepare a small DataFrame showing original index, db2_mape, ridge_mape\n",
    "result = top_10_filtered[['db2_mape', 'ridge_mape']]\n",
    "\n",
    "# Step 6: Print\n",
    "print(\"Original Row Index | DB2 MAPE (%) | Ridge MAPE (%)\")\n",
    "for idx, row in result.iterrows():\n",
    "    print(f\"{idx:<18} {row['db2_mape']:.4f}%         {row['ridge_mape']:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the top 10% where db2 made the smallest error (i.e., best predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort db2_better by db2_mape ascending\n",
    "db2_better_sorted = db2_better.sort_values(by='db2_mape')\n",
    "\n",
    "# Pick the top 10 rows (not top 10%)\n",
    "top_db2_rows = db2_better_sorted.head(10)\n",
    "\n",
    "# Show their original DataFrame index\n",
    "print(\"Top 10 DB2 predictions with smallest MAPE (original index):\")\n",
    "print(top_db2_rows.index.tolist())\n",
    "\n",
    "# Print the highest MAPE among these top 10\n",
    "threshold_mape = top_db2_rows['db2_mape'].min()\n",
    "print(f\"Lowest MAPE in top 10 best DB2 predictions: {threshold_mape:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort db2_better by db2_mape ascending\n",
    "db2_better_sorted = db2_better.sort_values(by='db2_mape')\n",
    "\n",
    "# Pick the top 10 rows\n",
    "top_db2_rows = db2_better_sorted.head(10)\n",
    "\n",
    "# Get their original indices\n",
    "top_indices = top_db2_rows.index.tolist()\n",
    "\n",
    "# Now, look up and collect corresponding items from test_query_ids_per_batch\n",
    "collected_items = []\n",
    "for idx in top_indices:\n",
    "    if idx < len(test_query_ids_per_batch):\n",
    "        item = test_query_ids_per_batch[idx]\n",
    "        # If the item itself is a list, extend; otherwise, append\n",
    "        if isinstance(item, list):\n",
    "            collected_items.extend(item)\n",
    "        else:\n",
    "            collected_items.append(item)\n",
    "    else:\n",
    "        print(f\"Warning: Index {idx} is out of range in test_query_ids_per_batch\")\n",
    "\n",
    "# Now sort the final collected list\n",
    "final_sorted_list = sorted(collected_items)\n",
    "\n",
    "# Print the sorted list\n",
    "print(\"\\nFinal sorted list of query IDs:\")\n",
    "print(final_sorted_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the top 10% where learnedWMP made the smallest error (i.e., best predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort ridge_better by ridge_mape ascending\n",
    "ridge_better_sorted = ridge_better.sort_values(by='ridge_mape')\n",
    "\n",
    "# Pick the top 10 rows\n",
    "top_ridge_rows = ridge_better_sorted.head(10)\n",
    "\n",
    "# Get their original indices\n",
    "top_ridge_indices = top_ridge_rows.index.tolist()\n",
    "\n",
    "# Show the original DataFrame indices\n",
    "print(\"Top 10 Ridge predictions with smallest MAPE (original indices):\")\n",
    "print(top_ridge_indices)\n",
    "\n",
    "# Now, look up and collect corresponding items from test_query_ids_per_batch\n",
    "collected_ridge_items = []\n",
    "for idx in top_ridge_indices:\n",
    "    if idx < len(test_query_ids_per_batch):\n",
    "        item = test_query_ids_per_batch[idx]\n",
    "        # If the item is a list, extend; otherwise, append\n",
    "        if isinstance(item, list):\n",
    "            collected_ridge_items.extend(item)\n",
    "        else:\n",
    "            collected_ridge_items.append(item)\n",
    "    else:\n",
    "        print(f\"Warning: Index {idx} is out of range in test_query_ids_per_batch\")\n",
    "\n",
    "# Now sort the final collected list\n",
    "final_sorted_ridge_list = sorted(collected_ridge_items)\n",
    "\n",
    "# Print the sorted list\n",
    "print(\"\\nFinal sorted list of query IDs for Ridge:\")\n",
    "print(final_sorted_ridge_list)\n",
    "\n",
    "# Print the highest Ridge MAPE within the top 10\n",
    "threshold_ridge_mape = top_ridge_rows['ridge_mape'].min()\n",
    "print(f\"\\nLowest MAPE in top 10 best Ridge predictions: {threshold_ridge_mape:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the original queries that are present in each of the above two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter df_test based on DB2 best query IDs\n",
    "df_test_db2_selected = df_test[df_test['queryId'].isin(final_sorted_list)]\n",
    "\n",
    "# Step 2: Filter df_test based on Ridge best query IDs\n",
    "df_test_ridge_selected = df_test[df_test['queryId'].isin(final_sorted_ridge_list)]\n",
    "\n",
    "# Step 3: Print results\n",
    "print(f\"Number of rows selected for DB2 best predictions: {len(df_test_db2_selected)}\")\n",
    "print(f\"Number of rows selected for Ridge best predictions: {len(df_test_ridge_selected)}\")\n",
    "\n",
    "# Optional: View a few rows\n",
    "# print(df_test_db2_selected.head())\n",
    "# print(df_test_ridge_selected.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_db2_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_ridge_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_ridge_selected.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Features with Statistically Significant Differences Between DB2 and Ridge Query Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Features list\n",
    "features = [\n",
    "    'TQ', 'TQ COUNT', 'TBSCAN', 'TBSCAN COUNT', 'SORT', 'SORT COUNT',\n",
    "    'FILTER', 'FILTER COUNT', 'HS JOIN', 'HS JOIN COUNT',\n",
    "    'TEMP', 'TEMP COUNT', 'GRPBY', 'GRPBY COUNT', 'UNIQUE', 'UNIQUE COUNT'\n",
    "]\n",
    "\n",
    "# Step 1: Summary statistics\n",
    "summary_db2 = df_test_db2_selected[features].describe().T[['mean', '50%', 'std']]\n",
    "summary_ridge = df_test_ridge_selected[features].describe().T[['mean', '50%', 'std']]\n",
    "\n",
    "# Rename median column\n",
    "summary_db2.rename(columns={'50%': 'median'}, inplace=True)\n",
    "summary_ridge.rename(columns={'50%': 'median'}, inplace=True)\n",
    "\n",
    "# Step 2: Combine into one comparison table\n",
    "comparison_summary = summary_db2.join(\n",
    "    summary_ridge,\n",
    "    lsuffix='_db2',\n",
    "    rsuffix='_ridge'\n",
    ")\n",
    "\n",
    "# Step 3: Find features with noticeable mean differences\n",
    "threshold = 0.05  # Tune as needed\n",
    "interesting_features = []\n",
    "\n",
    "for feature in features:\n",
    "    mean_db2 = df_test_db2_selected[feature].mean()\n",
    "    mean_ridge = df_test_ridge_selected[feature].mean()\n",
    "    mean_diff = abs(mean_db2 - mean_ridge)\n",
    "    \n",
    "    if mean_diff > threshold:\n",
    "        interesting_features.append((feature, mean_db2, mean_ridge, mean_diff))\n",
    "\n",
    "# Step 4: Print features with noticeable differences\n",
    "print(\"\\nFeatures with noticeable mean differences (> {:.2f}):\".format(threshold))\n",
    "for feature, db2_mean, ridge_mean, diff in interesting_features:\n",
    "    print(f\"{feature}: DB2 mean = {db2_mean:.4f}, Ridge mean = {ridge_mean:.4f}, Difference = {diff:.4f}\")\n",
    "\n",
    "# Step 5: Automatically Generate Insight Sentences for DB2\n",
    "print(\"\\nGenerated Insight Sentences for DB2:\\n\")\n",
    "\n",
    "for feature, db2_mean, ridge_mean, diff in interesting_features:\n",
    "    if db2_mean < ridge_mean:\n",
    "        print(f\"In queries where DB2 predictions were more accurate, the feature '{feature}' had a lower average value ({db2_mean:.4f}) compared to Ridge ({ridge_mean:.4f}). This suggests DB2 performs better when '{feature}' is relatively lower.\")\n",
    "    else:\n",
    "        print(f\"In queries where DB2 predictions were more accurate, the feature '{feature}' had a higher average value ({db2_mean:.4f}) compared to Ridge ({ridge_mean:.4f}). This suggests DB2 performs better when '{feature}' is relatively higher.\")\n",
    "\n",
    "# Step 6: Automatically Generate Insight Sentences for Ridge\n",
    "print(\"\\nGenerated Insight Sentences for Ridge:\\n\")\n",
    "\n",
    "for feature, db2_mean, ridge_mean, diff in interesting_features:\n",
    "    if ridge_mean < db2_mean:\n",
    "        print(f\"In queries where Ridge predictions were more accurate, the feature '{feature}' had a lower average value ({ridge_mean:.4f}) compared to DB2 ({db2_mean:.4f}). This suggests Ridge performs better when '{feature}' is relatively lower.\")\n",
    "    else:\n",
    "        print(f\"In queries where Ridge predictions were more accurate, the feature '{feature}' had a higher average value ({ridge_mean:.4f}) compared to DB2 ({db2_mean:.4f}). This suggests Ridge performs better when '{feature}' is relatively higher.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
