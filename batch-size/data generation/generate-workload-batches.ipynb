{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "def load_data():\n",
    "    df_train = pd.read_csv('tpcds_train_clean.csv')\n",
    "    df_test = pd.read_csv('tpcds_test_clean.csv')\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "# clustering\n",
    "def get_clusters(k, data, km):\n",
    "    X = data.drop(columns=['db2','actual']);\n",
    "    if km != None:\n",
    "        print('clustering test dataset')\n",
    "        y_kmeans = km.predict(X)\n",
    "        \n",
    "    else:\n",
    "        print('clustering train dataset')\n",
    "        km = KMeans(n_clusters = k, \n",
    "                        init='k-means++', \n",
    "                        n_init=10, \n",
    "                        max_iter=300, \n",
    "                        random_state=0)\n",
    "        km.fit(X)\n",
    "        y_kmeans = km.predict(X)\n",
    "        \n",
    "    print('Distortion: %.2f' % km.inertia_)\n",
    "    df_train = data.copy();\n",
    "    df_train['cluster'] = np.nan\n",
    "    for i,e in enumerate(y_kmeans):\n",
    "        df_train['cluster'].loc[i] = e;\n",
    "    return km, df_train\n",
    "\n",
    "# creating workloads\n",
    "def create_workload(batch_size, data, k):\n",
    "    # keeping only the columns that I need\n",
    "    df_data = data[['db2','actual','cluster']]\n",
    "    df_data = df_data.astype({'cluster': 'int32'})\n",
    "\n",
    "    # shuffle the dataframe\n",
    "    df_data = df_data.sample(frac=1, ignore_index=True)\n",
    "\n",
    "    # 1-hot encode the cluster column\n",
    "    df_data = pd.get_dummies(df_data, columns=['cluster'])\n",
    "    \n",
    "    # create a dataframe for all workloads\n",
    "    df_workloads = pd.DataFrame(columns=df_data.columns)\n",
    "    \n",
    "    num_batches = int(np.floor(df_data.shape[0] / batch_size))\n",
    "    #num_batches = 2\n",
    "\n",
    "    # creating this pair of variables to store the beginning and ending indices for each batch\n",
    "    first_index = 0\n",
    "    last_index = 0 \n",
    "        \n",
    "    for i in range(num_batches):\n",
    "        if i > 0:\n",
    "            first_index = last_index + 1\n",
    "        \n",
    "        last_index = first_index + batch_size - 1\n",
    "\n",
    "        # selecting the rows between first_index and last_index (inclusive) for the present workload\n",
    "        df_workload = df_data.loc[first_index:last_index, :]\n",
    "\n",
    "        #print('workload queries:\\n', df_workload)\n",
    "\n",
    "        # summing the column values will create a series. coverting it to a dataframe using to_frame()\n",
    "        # Transposing the dataframe will return to the orginal dataframe structure, however, all the \n",
    "        # column values will be aggregated\n",
    "        df_workload = df_workload.sum(axis=0).to_frame().T\n",
    "        #print('workload aggegated:\\n', df_workload)\n",
    "\n",
    "        # adding the new workload to the dataframe of all workloads\n",
    "        df_workloads = pd.concat([df_workloads, df_workload], ignore_index=True)\n",
    "\n",
    "    #print(df_workloads)\n",
    "    return df_workloads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST - create_workload function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_train, df_test = load_data()\\nk = 5\\nkm, df_train_clusters = get_clusters(k, df_train.loc[:100, :], None)\\ncreate_workload(3, df_train_clusters, k)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test create_workload() function\n",
    "'''\n",
    "df_train, df_test = load_data()\n",
    "k = 5\n",
    "km, df_train_clusters = get_clusters(k, df_train.loc[:100, :], None)\n",
    "create_workload(3, df_train_clusters, k)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering train dataset\n",
      "Distortion: 10.22\n",
      "clustering test dataset\n",
      "Distortion: 10.22\n",
      "batch = 1 is done\n",
      "batch = 2 is done\n",
      "batch = 3 is done\n",
      "batch = 5 is done\n",
      "batch = 10 is done\n",
      "batch = 15 is done\n",
      "batch = 20 is done\n",
      "batch = 25 is done\n",
      "batch = 30 is done\n",
      "batch = 35 is done\n",
      "batch = 40 is done\n",
      "batch = 45 is done\n",
      "batch = 50 is done\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = load_data()\n",
    "k = 110\n",
    "batch_sizes = [1, 2, 3, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "#batch_sizes = [2, 3, 5]\n",
    "#batch_sizes = [2]\n",
    "\n",
    "km, df_train_clusters = get_clusters(k, df_train, None)\n",
    "km, df_test_clusters  = get_clusters(k, df_test, km)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    workload_train = create_workload(batch_size,df_train_clusters,k)\n",
    "    workload_test = create_workload(batch_size,df_test_clusters,k)\n",
    "    file_name_train = 'train_workloads_final_%s_clusters_%s_batch.csv' % (k, batch_size)\n",
    "    file_name_test  = 'test_workloads_final_%s_clusters_%s_batch.csv' % (k, batch_size)\n",
    "    workload_train.to_csv(file_name_train ,index=False)\n",
    "    workload_test.to_csv(file_name_test ,index=False)\n",
    "    print(\"batch = %s is done\" % (batch_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
