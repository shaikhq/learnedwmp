{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import loguniform\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn import neural_network\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "k = 110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df_train = pd.read_csv('tpcds_train_clean.csv')\n",
    "    df_test = pd.read_csv('tpcds_test_clean.csv')\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(k, data, km):\n",
    "    X = data.drop(columns=['db2','actual']);\n",
    "    if km != None:\n",
    "        print('clustering test dataset')\n",
    "        y_kmeans = km.predict(X)\n",
    "        \n",
    "    else:\n",
    "        print('clustering train dataset')\n",
    "        km = KMeans(n_clusters = k, \n",
    "                        init='k-means++', \n",
    "                        n_init=10, \n",
    "                        max_iter=300, \n",
    "                        random_state=0)\n",
    "        km.fit(X)\n",
    "        y_kmeans = km.predict(X)\n",
    "        \n",
    "    print('Distortion: %.2f' % km.inertia_)\n",
    "    df_train = data.copy();\n",
    "    df_train['cluster'] = np.nan\n",
    "    for i,e in enumerate(y_kmeans):\n",
    "        df_train['cluster'].loc[i] = e;\n",
    "    \n",
    "    return km, df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workload(batch_size, data, \n",
    "                    k):\n",
    "    df_data = data[['db2','actual','cluster']]\n",
    "    df_data = pd.get_dummies(df_data, columns=['cluster'])\n",
    "    for i in range(0,k):\n",
    "        c_name = 'cluster_%d.0' % i\n",
    "        if c_name not in df_data.columns:\n",
    "            df_data[c_name] = 0\n",
    "            \n",
    "    df_batches = pd.DataFrame(columns=df_data.columns)\n",
    "    \n",
    "    indices = np.linspace(0, data.shape[0]-1, data.shape[0], dtype=int)\n",
    "    num_batches = int(np.floor(df_data.shape[0] / batch_size))\n",
    "        \n",
    "    for ibat in range(num_batches):\n",
    "        start = (ibat * batch_size)\n",
    "        end = (ibat * batch_size + batch_size) - 1\n",
    "        \n",
    "        ibat_Y = df_data.loc[indices[start:end], :]\n",
    "        \n",
    "        df_batches = df_batches.append(ibat_Y.sum(), ignore_index=True)\n",
    "        \n",
    "    return df_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering train dataset\n",
      "Distortion: 10.22\n",
      "clustering test dataset\n",
      "Distortion: 10.22\n",
      "batch = 35 is done\n",
      "batch = 45 is done\n"
     ]
    }
   ],
   "source": [
    "#batch_sizes = [2, 3, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "batch_sizes = [2, 3, 5]\n",
    "\n",
    "km, df_train_clusters = get_clusters(k, df_train, None)\n",
    "km, df_test_clusters  = get_clusters(k, df_test, km)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    workload_train = create_workload(batch_size,df_train_clusters,k)\n",
    "    workload_test = create_workload(batch_size,df_test_clusters,k)\n",
    "    file_name_train = 'train_workloads_final_%s_clusters_%s_batch.csv' % (k, batch_size)\n",
    "    file_name_test  = 'test_workloads_final_%s_clusters_%s_batch.csv' % (k, batch_size)\n",
    "    workload_train.to_csv(file_name_train ,index=False)\n",
    "    workload_test.to_csv(file_name_test ,index=False)\n",
    "    print(\"batch = %s is done\" % (batch_size))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
